{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7d82bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import base64\n",
    "import io\n",
    "import pdfplumber\n",
    "from PIL import Image\n",
    "from together import Together\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a1a3f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_caption(base64_image: base64) -> str:\n",
    "    \"\"\"Image captioning by using a Vision Language Model\"\"\"\n",
    "    \n",
    "    client = Together()\n",
    "    prompt = \"Give a suitable caption for the provided image\"\n",
    "    \n",
    "    stream = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-Vision-Free\",\n",
    "    # Other vision model choices\n",
    "    # Meta Llama 3.2 90B Vision Instruct Turbo $ 1.2\n",
    "    # Meta Llama 3.2 11B Vision Instruct Turbo $ 0.18\n",
    "    # Meta Llama Guard 3 11B Vision Turbo $ 0.18\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    "    )\n",
    "    \n",
    "    caption = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices and chunk.choices[0].delta:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            caption += content\n",
    "    \n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8ac6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "        Processes a PDF, extracts text, images (gets captions), and tables,\n",
    "        and returns a Markdown string.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    print(f\"Processing PDF: {pdf_path} with {len(doc)} pages.\")\n",
    "    \n",
    "    final_doc = []\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        \n",
    "        page_content = []\n",
    "        page_content.append(f\"\\n## Page {page_num + 1}\\n\")\n",
    "        \n",
    "        # Extracting Text\n",
    "        text = page.get_text(\"text\")\n",
    "        if text.strip():\n",
    "            page_content.append(\"### Text\\n\")\n",
    "            page_content.append(text.strip())\n",
    "            # page_content.append(\"\\n\")\n",
    "            pass\n",
    "            \n",
    "        \n",
    "        # Extracting Images and getting caption\n",
    "        image_list = page.get_images(full=True)\n",
    "        print(f\"Page: {page_num}\")\n",
    "        if image_list:\n",
    "            # print(f\"YESS: {page_num}\")\n",
    "            page_content.append(\"### Images\\n\")\n",
    "            \n",
    "            for img in image_list:\n",
    "                \n",
    "                # get the XREF of the image\n",
    "                xref = img[0]\n",
    "\n",
    "                base_image = doc.extract_image(xref)\n",
    "                # base_image is a dictionary with lot of info\n",
    "                \n",
    "                # this is the bytes of the image\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                \n",
    "                # converting it to base 64 to make it easy to use with Together AI\n",
    "                base64_image = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "                \n",
    "                # get the image extension(useful for saving the img)\n",
    "                # image_ext = base_image[\"ext\"]\n",
    "                \n",
    "                # Caption the image and add it to our page_content\n",
    "                caption = get_image_caption(base64_image)\n",
    "                page_content.append(caption)\n",
    "\n",
    "        \n",
    "        # Extracting tables\n",
    "        # PyMuPDF's table extraction is heuristic.\n",
    "        # For complex tables, check pdfplumber or camelot-py.\n",
    "        tables = page.find_tables()\n",
    "        if tables.tables:\n",
    "            # page_content.append(\"### Table\\n\")\n",
    "            # Write logic here to convert table into plain text\n",
    "            pass\n",
    "                    \n",
    "        final_doc.extend(page_content)\n",
    "    \n",
    "    return \"\\n\\n\".join(final_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be6e3e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = 'Data/Applications of Transformers.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d973d83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF: Data/Applications of Transformers.pdf with 58 pages.\n",
      "Page: 0\n",
      "Page: 1\n",
      "Page: 2\n",
      "Page: 3\n",
      "Page: 4\n",
      "Page: 5\n",
      "Page: 6\n",
      "Page: 7\n",
      "Page: 8\n",
      "Page: 9\n",
      "Page: 10\n",
      "Page: 11\n",
      "Page: 12\n",
      "Page: 13\n",
      "Page: 14\n",
      "Page: 15\n",
      "Page: 16\n",
      "Page: 17\n",
      "Page: 18\n",
      "Page: 19\n",
      "Page: 20\n",
      "Page: 21\n",
      "Page: 22\n",
      "Page: 23\n",
      "Page: 24\n",
      "Page: 25\n",
      "Page: 26\n",
      "Page: 27\n",
      "Page: 28\n",
      "Page: 29\n",
      "Page: 30\n",
      "Page: 31\n",
      "Page: 32\n",
      "Page: 33\n",
      "Page: 34\n",
      "Page: 35\n",
      "Page: 36\n",
      "Page: 37\n",
      "Page: 38\n",
      "Page: 39\n",
      "Page: 40\n",
      "Page: 41\n",
      "Page: 42\n",
      "Page: 43\n",
      "Page: 44\n",
      "Page: 45\n",
      "Page: 46\n",
      "Page: 47\n",
      "Page: 48\n",
      "Page: 49\n",
      "Page: 50\n",
      "Page: 51\n",
      "Page: 52\n",
      "Page: 53\n",
      "Page: 54\n",
      "Page: 55\n",
      "Page: 56\n",
      "Page: 57\n"
     ]
    }
   ],
   "source": [
    "output = process_pdf(PDF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4929fe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Page 1\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "This work has been submitted to the Expert Systems With Applications journal (Elsevier) for\n",
      "possible publication\n",
      "A COMPREHENSIVE SURVEY ON APPLICATIONS OF\n",
      "TRANSFORMERS FOR DEEP LEARNING TASKS\n",
      "Saidul Islam1, Hanae Elmekki1, Ahmed Elsebai1, Jamal Bentahar1,2,∗, Najat Drawel 1, Gaith Rjoub3,1, Witold Pedrycz4,5,6,7\n",
      "1Concordia Institute for Information Systems Engineering, Concordia University, Montreal, Canada\n",
      "2Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, UAE\n",
      "3King Hussein School of Computing Sciences, Princess Sumaya University for Technology, Jordan\n",
      "4Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Canada\n",
      "5Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n",
      "6Department of Computer Engineering, Istinye University, Sariyer/Istanbul, Turkiye\n",
      "7Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, Saudi Arabia\n",
      "∗Corresponding Author’s Email: jamal.bentahar@concordia.ca\n",
      "Contributing Authors’ Emails: saidul.islam@concordia.ca; hanae.elmekki@mail.concordia.ca;\n",
      "ahmed.elsebai@outlook.com; n drawe@encs.concordia.ca; g.rjoub@psut.edu.jo; wpedrycz@ualberta.ca\n",
      "The authors contributed equally to this work.\n",
      "ABSTRACT\n",
      "Transformer is a deep neural network that employs a self-attention mechanism to comprehend the con-\n",
      "textual relationships within sequential data. Unlike conventional neural networks or updated versions of\n",
      "Recurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM), transformer models excel\n",
      "in handling long dependencies between input sequence elements and enable parallel processing. As a result,\n",
      "transformer-based models have attracted substantial interest among researchers in the field of artificial intel-\n",
      "ligence. This can be attributed to their immense potential and remarkable achievements, not only in Natural\n",
      "Language Processing (NLP) tasks but also in a wide range of domains, including computer vision, audio\n",
      "and speech processing, healthcare, and the Internet of Things (IoT). Although several survey papers have\n",
      "been published highlighting the transformer’s contributions in specific fields, architectural differences, or\n",
      "performance evaluations, there is still a significant absence of a comprehensive survey paper encompassing\n",
      "its major applications across various domains. Therefore, we undertook the task of filling this gap by con-\n",
      "ducting an extensive survey of proposed transformer models from 2017 to 2022. Our survey encompasses\n",
      "the identification of the top five application domains for transformer-based models, namely: NLP, Computer\n",
      "Vision, Multi-Modality, Audio and Speech Processing, and Signal Processing. We analyze the impact of\n",
      "highly influential transformer-based models in these domains and subsequently classify them based on their\n",
      "respective tasks using a proposed taxonomy. Our aim is to shed light on the existing potential and future\n",
      "possibilities of transformers for enthusiastic researchers, thus contributing to the broader understanding of\n",
      "this groundbreaking technology.\n",
      "Keywords: Self-attention; Transformer; Deep learning, Recurrent networks; Long short-term memory-\n",
      "LSTM; Multi-modality.\n",
      "1\n",
      "INTRODUCTION\n",
      "Deep Neural Networks (DNNs) have emerged as the predominant infrastructure and state-of-the-art solution for the majority\n",
      "of learning-based machine intelligence tasks in the field of artificial intelligence. Although various types of DNNs are utilized\n",
      "for specific tasks, the multilayer perceptron (MLP) represents the classic form of neural network which is characterized by\n",
      "multiple linear layers and nonlinear activation functions (Murtagh, 1990). For instance, in computer vision, convolutional\n",
      "neural networks incorporate convolutional layers to process images, while recurrent neural networks employ recurrent cells\n",
      "to process sequential data, particularly in Natural Language Processing (NLP) (O’Shea & Nash, 2015, Mikolov et al., 2010).\n",
      "Despite the wide use of recurrent neural networks, they exhibit certain limitations. One of the major issues with conventional\n",
      "networks is that they have short-term dependencies associated with exploding and vanishing gradients. In contrast, to achieve\n",
      "good results in NLP, long-term dependencies must be captured. Additionally, recurrent neural networks are slow to train\n",
      "due to their sequential data processing and computational approach (Giles et al., 1995). To address these issues, the long-\n",
      "short-term memory (LSTM) version of recurrent networks was developed, which improves the gradient descent problem\n",
      "of recurrent neural networks and increases the memory range of NLP tasks (Hochreiter & Schmidhuber, 1997). However,\n",
      "LSTMs still struggle with the problem of sequential processing, which hinders the extraction of the actual meaning of the\n",
      "context. To tackle this challenge, bidirectional LSTMs were introduced, which process natural language from both directions,\n",
      "arXiv:2306.07303v1  [cs.LG]  11 Jun 2023\n",
      "\n",
      "\n",
      "## Page 2\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "i.e., left to right and right to left, and then concatenate the outcomes to obtain the context’s actual meaning. Nevertheless, this\n",
      "technique still results in a slight loss of the true meaning of the context (Graves & Schmidhuber, 2005, Li et al., 2020b).\n",
      "Transformers are a type of deep neural network (DNNs) that offer a solution to the limitations of sequence-to-sequence (seq-2-\n",
      "seq) architectures, including short-term dependency of sequence inputs and the sequential processing of input, which hinders\n",
      "parallel training of networks. Transformers leverage the multi-head self-attention mechanism to extract features, and they\n",
      "exhibit great potential for application in NLP. Unlike traditional recurrence methods, transformers utilize attention to learn\n",
      "from an entire segment of a sequence, using encoding and decoding blocks. One key advantage of transformers over LSTM\n",
      "and recurrent neural networks is their ability to capture the true meaning of the context, owing to their attention mechanism.\n",
      "Moreover, transformers are faster since they can work in parallel, unlike recurrent networks, and can be calculated using\n",
      "Graphic Processing Units (GPUs), allowing for faster computation of tasks with large inputs (Niu et al., 2021, Vaswani et al.,\n",
      "2017, Zheng et al., 2020). The advantages of the transformer model have inspired deep learning researchers to explore its\n",
      "potential for various tasks in different fields of application (Ren et al., 2023), leading to numerous research papers and the\n",
      "development of transformer-based models for a range of tasks in the field of artificial intelligence (Yeh et al., 2019, Wang\n",
      "et al., 2019, Reza et al., 2022).\n",
      "In the research community, the importance of survey papers in providing a productive analysis, comparison, and contribution\n",
      "of progressive topics is widely recognized. Numerous survey papers on the topic of transformers can be found in the literature.\n",
      "Most of them are addressing specific fields of application (Khan et al., 2022, Wang et al., 2020a, Shamshad et al., 2023),\n",
      "compare the performance of different model(Tay et al., 2023, Fournier et al., 2021, Selva et al., 2023), or conduct architecture-\n",
      "based analysis (Lin et al., 2022). Nevertheless, a well-defined structure that comprehensively focuses on the top application\n",
      "fields and systematically analyzes the contribution of transformer-based models in the execution of various deep learning\n",
      "tasks within those fields is still widely needed.\n",
      "Indeed, conducting a survey on transformer applications would serve as a valuable reference source for enthusiastic deep-\n",
      "learning researchers seeking to gain a better understanding of the contributions of transformer models in diverse fields. Such\n",
      "a survey would enable the identification and discussion of potential models, their characteristics, and working methodology,\n",
      "thus promoting the refinement of existing transformer models and the discovery of novel transformer models or applications.\n",
      "To address the absence of such a survey, this paper presents a comprehensive analysis of all transformer-based models,\n",
      "and identifies the top five application fields, namely NLP, Computer Vision, Multi-Modality, Audio & Speech, and Signal\n",
      "Processing), and proposes a taxonomy of transformer models, with significant models being classified and analyzed based\n",
      "on their task execution within these fields. Furthermore, the top-performing and significant models are analyzed within the\n",
      "application fields, and based on this analysis, we discuss the future prospects and challenges of transformer models.\n",
      "1.1\n",
      "CONTRIBUTIONS AND MOTIVATIONS\n",
      "Although several survey articles on the topic of transformers already exist in the literature, our motivations for conducting\n",
      "this survey stem from two essential observations. First, most of these studies have focused on transformer architecture, model\n",
      "efficiency, and specific artificial intelligence fields, such as NLP, computer vision, multi-modality, audio & speech, and signal\n",
      "processing. They have often neglected other crucial aspects, such as the transformer-based model’s execution in deep learning\n",
      "tasks across multiple application domains. We aim in this survey to cover all major fields of application and present significant\n",
      "models for different task executions. The second motivation is the absence of a comprehensive and methodical analysis\n",
      "encompassing various prevalent application domains, and their corresponding utilization of transformer-based models, in\n",
      "relation to diverse deep learning tasks within distinct fields of application. We propose a high-level classification framework\n",
      "for transformer models, which is based on their most prominent fields of application. The prominent models are categorized\n",
      "and evaluated based on their task performance within the respective fields. In this survey, we highlight the application domains\n",
      "of transformers that have received comparatively greater or lesser attention from researchers. To the best of our knowledge,\n",
      "this is the first review paper that presents a high-level classification scheme for the transformer-based models and provides\n",
      "a collection of criteria that aim to achieve two objectives: (1) assessing the effectiveness of transformer models in various\n",
      "applications; and (2) assisting researchers interested in exploring and extending the capabilities of transformer-based models\n",
      "to new domains. Moreover, the paper provides valuable insights into potential future applications and highlights unresolved\n",
      "challenges within this field.\n",
      "The remainder of the paper is organized as follows. Preliminary concepts important for the rest of the paper are explained in\n",
      "Section 2. A detailed description of the systematic methodology used to search for relevant research articles is provided in\n",
      "Section 3. Section 4 presents related review papers and discusses similarities and differences with the current survey paper,\n",
      "which helps us identify the unique characteristics and the value added of our survey. Section 5 identifies the the transformer\n",
      "models proposed so far across different fields of application. A Classification of the selected scientific articles on section\n",
      "6. Section 7 outlines potential directions for future work. Finally, Section 8 concludes the paper and summarizes the key\n",
      "findings and contributions of the study.\n",
      "2\n",
      "PRELIMINARIES\n",
      "Before delving into the literature of transformers, let us describe some concepts that will be used throughout this article.\n",
      "\n",
      "\n",
      "## Page 3\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Figure 1: Multi-head attention & scaled dot product attention (Vaswani et al., 2017)\n",
      "2.1\n",
      "TRANSFORMER ARCHITECTURE\n",
      "The transformer model was first proposed in 2017 for a machine translation task, and since then, numerous models have\n",
      "been developed based on the inspiration of the original transformer model to address a variety of tasks across different fields.\n",
      "While some models have utilized the vanilla transformer architecture as is, others have leveraged only the encoder or decoder\n",
      "module of the transformer model. As a result, the task and performance of transformer-based models can vary depending on\n",
      "the specific architecture employed. Nonetheless, a key and widely used component of transformer models is self-attention,\n",
      "which is essential to their functionality. All transformer-based models employ the self-attention mechanism and multi-head\n",
      "attention, which typically forms the primary learning layer of the architecture. Given the significance of self-attention, the\n",
      "role of the attention mechanism is crucial in transformer models (Vaswani et al., 2017)\n",
      "2.1.1\n",
      "ATTENTION MECHANISM\n",
      "The attention mechanism has garnered significant recognition since its introduction in the 1990s, owing to its ability to\n",
      "concentrate on critical pieces of information. In image processing, certain regions of images were found to be more pertinent\n",
      "than others. Consequently, the attention mechanism was introduced as a novel approach in computer vision tasks, aiming to\n",
      "emphasize important parts based on their contextual relevance in the application. This technique yielded significant outcomes\n",
      "when implemented in computer vision, thereby promoting its widespread adoption in various other fields such as language\n",
      "processing.\n",
      "In 2017, a novel attention-based neural network, named “Transformer”, was introduced to address the limitations of other\n",
      "neural networks (such as A recurrent neural network (RNN)) in encoding long-range dependencies in sequences, particularly\n",
      "in language translation tasks (Vaswani et al., 2017). The incorporation of a self-attention mechanism in the transformer model\n",
      "improved the performance of the attention mechanism by better capturing local features and reducing the reliance on external\n",
      "information. In the original transformer architecture, the attention technique is implemented through the “Scaled Dot Product\n",
      "Attention”, which is based on three primary parameter matrices: Query (Q), Key (K), and Value (V). Each of these matrices\n",
      "carries an encoded representation of each input in the sequence (Vaswani et al., 2017). The SoftMax function is applied to\n",
      "obtain the final output of the attention process, which is a probability score computed from the combination of the weights of\n",
      "the three matrices (see Figure 1). Mathematically, the scaled dot product attention function is computed as follows:\n",
      "Attention(Q, K, V ) = softmax\n",
      "\u0012QKT\n",
      "√\n",
      "dk\n",
      "\u0013\n",
      "V\n",
      "The matrices Q and K represent the Query and Key vectors respectively, both having a dimension of dk, while the matrix V\n",
      "represents the values vectors.\n",
      "2.1.2\n",
      "MULTI-HEAD ATTENTION\n",
      "The application of the scaled dot-product attention function in parallel within the multi-head Attention module is essential\n",
      "for extracting the maximum dependencies among different segments in the input sequence. Each head denoted by k performs\n",
      "the attention mechanism based on its own learnable weights W kQ, W kK, and W kv. The attention outputs calculated by each\n",
      "\n",
      "### Images\n",
      "\n",
      "\n",
      "The image presents a flowchart illustrating the process of a machine learning model, with various components and processes connected by arrows. The chart is divided into two main sections: the left side, which represents the input data, and the right side, which represents the output.\n",
      "\n",
      "**Main Points:**\n",
      "\n",
      "* **Input Data**\n",
      "\t+ Query\n",
      "\t+ Key\n",
      "\t+ Value\n",
      "* **Model Components**\n",
      "\t+ Scaled-Dot Product Attention\n",
      "\t+ Concatenate\n",
      "\t+ Linear\n",
      "\t+ Matrix Multiplication\n",
      "\t+ Probability Calculation (SoftMax)\n",
      "\t+ Mask (Optional)\n",
      "\t+ Scale\n",
      "* **Output**\n",
      "\t+ Matrix Multiplication\n",
      "\t+ Probability Calculation (SoftMax)\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The flowchart illustrates the process of a machine learning model, from input data to output. The model consists of several components, including Scaled-Dot Product Attention, Concatenate, Linear, Matrix Multiplication, Probability Calculation (SoftMax), Mask (Optional), and Scale. The input data includes query, key, and value, which are processed through the model components to produce the output. The output is represented by Matrix Multiplication and Probability Calculation (SoftMax). Overall, the flowchart provides a clear and concise overview of the machine learning model's architecture and process.\n",
      "\n",
      "\n",
      "## Page 4\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Figure 2: Transformer architecture (Vaswani et al., 2017)\n",
      "head are subsequently concatenated and linearly transformed into a single matrix with the expected dimension (Vaswani et al.,\n",
      "2017).\n",
      "headk = Attention(QW kQ, KW kK, V W kV )\n",
      "MultiHead(Q, K, V ) = Concat(head1, head2, ....headH)W 0\n",
      "The utilization of multi-head attention facilitates the neural network in learning and capturing diverse characteristics of the\n",
      "input sequential data. Consequently, this enhances the representation of the input contexts, as it merges information from\n",
      "distinct features of the attention mechanism within a specific range, which could be either short or long. This approach allows\n",
      "the attention mechanism to jointly function, which results in better network performance (Vaswani et al., 2017).\n",
      "2.2\n",
      "ARCHITECTURE OF THE TRANSFORMER MODEL\n",
      "The transformer model was primarily developed based on the attention mechanism (Vaswani et al., 2017), with the aim\n",
      "of processing sequential data. Its outstanding performance, especially in achieving state-of-the-art benchmarks for NLP\n",
      "translation models, has led to the widespread use of transformers. As depicted in Figure 2, the overall architecture of the\n",
      "transformer model for sentence translation tasks involves the use of attention mechanisms. However, for different applications,\n",
      "the transformer architecture may be subject to variation, depending on specific requirements.\n",
      "The initial transformer architecture was developed based on the auto-regressive sequence transduction model, comprising\n",
      "two primary modules, namely Encoder and Decoder. These modules are executed multiple times, as required by the task at\n",
      "hand. Each module comprises several layers that integrate the attention mechanism. Particularly, the attention mechanism\n",
      "is executed in parallel multiple times within the transformer architecture, which explains the presence of multiple “Attention\n",
      "Heads” (Vaswani et al., 2017).\n",
      "2.2.1\n",
      "ENCODER MODULE\n",
      "The stacked module within the transformer architecture comprises two fundamental layers, namely the Feed-Forward Layer\n",
      "and Multi-Head Attention Layer. In addition, it incorporates Residual connections around both layers, as well as two Add\n",
      "and Norm layers, which play a pivotal role (Vaswani et al., 2017). In the case of text translation, the Encoder module receives\n",
      "an embedding input that is generated based on the input’s meaning and position information via the Embedding and Position\n",
      "\n",
      "### Images\n",
      "\n",
      "\n",
      "The image presents a flowchart illustrating the process of generating outputs from inputs through a neural network. The chart is divided into two main sections: the left side, which represents the encoder, and the right side, which represents the decoder.\n",
      "\n",
      "*   **Encoder**\n",
      "    *   The encoder receives the input and performs the following operations:\n",
      "        *   Positional Encoding: Adds positional information to the input.\n",
      "        *   Input Embedding: Embeds the input into a higher-dimensional space.\n",
      "        *   Add & Norm: Adds the embedded input to a normed value.\n",
      "        *   Feed Forward: Applies a feed-forward neural network to the input.\n",
      "        *   Residual Connection: Adds the input to the output of the feed-forward network.\n",
      "        *   Multi-Head Attention: Applies multi-head attention to the input.\n",
      "        *   Value: Represents the value of the input.\n",
      "        *   Query: Represents the query of the input.\n",
      "        *   Key: Represents the key of the input.\n",
      "    *   The output of the encoder is then passed through another layer of the same operations (Add & Norm, Feed Forward, Residual Connection, Multi-Head Attention) to produce the final encoded output.\n",
      "*   **Decoder**\n",
      "    *   The decoder receives the output from the encoder and performs the following operations:\n",
      "        *   Positional Encoding: Adds positional information to the input.\n",
      "        *   Input Embedding: Embeds the input into a higher-dimensional space.\n",
      "        *   Add & Norm: Adds the embedded input to a normed value.\n",
      "        *   Feed Forward: Applies a feed-forward neural network to the input.\n",
      "        *   Residual Connection: Adds the input to the output of the feed-forward network.\n",
      "        *   Multi-Head Attention: Applies multi-head attention to the input.\n",
      "        *   Masked Multi-Head Attention: Applies masked multi-head attention to the input.\n",
      "        *   Add & Norm: Adds the input to a normed value.\n",
      "        *   Linear: Applies a linear transformation to the input.\n",
      "        *   Softmax: Applies a softmax activation function to the input.\n",
      "        *   Output Embedding: Embeds the output into a higher-dimensional space.\n",
      "\n",
      "In summary, the flowchart illustrates the process of generating outputs from inputs through a neural network, comprising an encoder and a decoder. The encoder processes the input by performing several operations, including positional encoding, input embedding, add & norm, feed forward, residual connection, and multi-head attention, to produce the final encoded output. The decoder then processes the output from the encoder by performing similar operations, including positional encoding, input embedding, add & norm, feed forward, residual connection, masked multi-head attention, add & norm, linear, softmax, and output embedding, to produce the final output.\n",
      "\n",
      "\n",
      "## Page 5\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Encoding layers. From the embedding input, three parameter matrices are created, namely the Query (Q), Key (K), and Value\n",
      "(V ) matrices, along with positional information, which are passed through the “Multi-Head Attention” layer. Following this\n",
      "step, the Feed-Forward layer addresses the issue of rank collapse that can arise during the computation process. Additionally,\n",
      "a normalization layer is applied to each step, which reduces the dependencies between layers by normalizing the weights used\n",
      "in gradient computation within each layer. To address the issue of vanishing gradients, the Residual Connection is applied to\n",
      "every output of both the attention and feed-forward layers, as illustrated in Figure 2.\n",
      "2.2.2\n",
      "DECODER MODULE\n",
      "The Decoder module in the transformer architecture is similar to the Encoder module, with the inclusion of additional layers\n",
      "such as Masked Multi-Head Attention. In addition to the Feed-Forward, Multi-Head Attention, Residual connection, and Add\n",
      "and Norm layers, the Decoder also contains Masked Multi-Head Attention layers. These layers use the scaled dot product\n",
      "and Mask Operations to exclude future predictions and consider only previous outputs. The Attention mechanism is applied\n",
      "twice in the Decoder: one for computing attention between elements of the targeted output and another for finding attention\n",
      "between the encoding inputs and targeted output. Each attention vector is then passed through the feed-forward unit to make\n",
      "the output more comprehensible to the layers. The generated decoding result is then caught by Linear and SoftMax layers\n",
      "at the top of the Decoder to compute the final output of the transformer architecture. This process is repeated multiple times\n",
      "until the last token of a sentence is found (Vaswani et al., 2017), as illustrated in Figure 2.\n",
      "3\n",
      "RESEARCH METHODOLOGY\n",
      "In this survey, we collect and analyze the most recent surveys on transformers that have been published in refereed journals\n",
      "and conferences with the aim of studying their contributions and limitations. To gather the relevant papers, we employed a\n",
      "two-fold strategy: (1) searching using several established search engines and selected papers based on the keywords “survey”,\n",
      "“review”, “Transformer”, “attention”, “self-attention”, “artificial intelligence”, and “deep learning; and (2) evaluating the\n",
      "selected papers and eliminated those that were deemed irrelevant for our study. A detailed organization of our survey is\n",
      "depicted in Figure 3.\n",
      "Figure 3: Methodology of the survey\n",
      "Indeed, by means of a comprehensive examination of survey papers and expert discussions on deep learning, we have iden-\n",
      "tified the top five domains of application for transformer-based models, these are: (i) NLP, (ii) computer vision, (iii) multi-\n",
      "modality, (iv) audio/speech, and (v) signal processing. Subsequently, we performed a systematic search for journal and\n",
      "conference papers that presented transformer-based models in each of the aforementioned fields of application, utilizing the\n",
      "keywords presented in Table 1. Our search yielded a substantial number of papers for each field, which we thoroughly re-\n",
      "viewed and evaluated. We selected papers that proposed novel transformer-based or transformer-inspired models for deep\n",
      "learning tasks, while disregarding others. Through our examination of this extensive collection of models, we have identified\n",
      "prevalent deep-learning tasks associated with each field of application.\n",
      "As we have examined more than 600 transformer models during this process, it has become exceedingly difficult to classify\n",
      "such a large number of models and conduct thorough analyses of each task within every field of application. Therefore, we\n",
      "have opted to perform a more comprehensive analysis of a number of transformer models for each task within every field of\n",
      "\n",
      "### Images\n",
      "\n",
      "\n",
      "The image represents a flowchart illustrating the process of identifying the top 5 fields of applications for digital libraries, specifically focusing on transformer models. This flowchart is divided into ten steps, each with a corresponding icon and text that outlines the task to be performed.\n",
      "\n",
      "The first step involves searching keywords related to the topic of interest. The second step involves searching for available survey and review papers on the topic from popular digital libraries. The third step involves filtering out irrelevant papers. The fourth step involves analyzing the survey papers and discussing them with deep learning experts to identify the top 5 fields of application.\n",
      "\n",
      "The fifth step involves identifying the popular tasks for each of the top 5 fields of applications. The sixth step involves analyzing the models and identifying the relevant models for the top 5 fields of applications. The seventh step involves filtering out irrelevant papers. The eighth step involves searching for transformer models for the top 5 fields of applications from conference and journal using popular digital libraries.\n",
      "\n",
      "The ninth step involves exploring the specific tasks, requirements, model structure, datasets, and characteristics of the selected models. The tenth step involves identifying models for further discussion for each other task under each field and classifying them through taxonomy.\n",
      "\n",
      "Overall, the flowchart provides a structured approach to identifying the top 5 fields of applications for transformer models in digital libraries, taking into account relevant papers, expert discussions, and model analysis.\n",
      "\n",
      "\n",
      "## Page 6\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "application. These models were selected based on specific criteria and an in-depth analysis was carried out accordingly. The\n",
      "selected models are as follows:\n",
      "1. The transformer-based models that have been proposed to execute a deep learning task for the first time and opened\n",
      "up new path for research in the field of transformer applications.\n",
      "2. The models that have proposed alternative or novel approaches to implementing the transformer’s attention mech-\n",
      "anism, as compared to the vanilla architecture, such as introducing a new attention mechanism or enhancing the\n",
      "position encoding module.\n",
      "3. The transformer models have had a significant impact in the field, with higher citation rates, and have been widely\n",
      "accepted by the scientific community. Models that have also contributed to breakthroughs in the advancement of\n",
      "transformer applications.\n",
      "4. The models and their variants have been proposed for the purpose of applying the transformer technology to real-\n",
      "world applications, with the aim of achieving superior performance results in comparison to other deep learning\n",
      "methods.\n",
      "5. The transformer-based models generated a significant buzz within the theoretical and applied artificial intelligence\n",
      "community.\n",
      "Fields of\n",
      "Application\n",
      "Keywords for Paper Search\n",
      "Tasks Of Application\n",
      "Number of papers\n",
      "Relevant models\n",
      "using keywords\n",
      "Selected models\n",
      "for Taxonomy\n",
      "Natural Language\n",
      "Processing\n",
      "“Natural Language Processing”,\n",
      "“NLP”,“Text”,“Text Processing”,\n",
      "“Transformer”, “Attention”,\n",
      "“Self-attention”, “multi-head\n",
      "attention”, “Language model”.\n",
      "Language Translation\n",
      "257\n",
      "25\n",
      "Text Classification & Segmentation\n",
      "Question Answering\n",
      "Text Summarization\n",
      "Text Generation\n",
      "Natural Language Reasoning\n",
      "Automated Symbolic\n",
      "Reasoning\n",
      "Computer Vision\n",
      "“Transformer”,“Attention”,\n",
      "“Self-attention”,“Image”,\n",
      "“Natural image”,“medical\n",
      "image”,“Biomedical”,\n",
      "“health”,“Image processing”,\n",
      "“Computer vision”,“Vision”.\n",
      "Natural Image\n",
      "Processing\n",
      "Image\n",
      "Classification\n",
      "197\n",
      "27\n",
      "Recognition &\n",
      "Object Detection\n",
      "Image\n",
      "Segmentation\n",
      "Image Generation\n",
      "Medical Image\n",
      "Processing\n",
      "Image\n",
      "Segmentation\n",
      "Image\n",
      "Classification\n",
      "Image\n",
      "Translation\n",
      "Multi-modal\n",
      "“Transformer”,“Attention”,\n",
      "“Self-attention”,“multi-head\n",
      "attention”,“multimodal”,\n",
      "“multi-modality”,“text-image”,\n",
      "“image-text”,“ video-audio-\n",
      "text, “text-audio”,“audio-text”,\n",
      "“vision-language”,\n",
      "“language-vision”.\n",
      "Classification &\n",
      "Segmentation\n",
      "94\n",
      "20\n",
      "Continued on next page\n",
      "\n",
      "\n",
      "## Page 7\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Table 1 – continued from previous page\n",
      "Fields of\n",
      "Application\n",
      "Keywords for Paper Search\n",
      "Tasks Of Application\n",
      "Number of papers\n",
      "Relevant models\n",
      "using keywords\n",
      "Selected models\n",
      "for Taxonomy\n",
      "Visual Question Answering\n",
      "Visual Captioning\n",
      "Visual Common-sense\n",
      "Reasoning\n",
      "Text/Image/Video/Speech\n",
      "Generation\n",
      "Cloud Task Computing\n",
      "Audio & Speech\n",
      "“Transformer”,“Attention”,\n",
      "“Self-attention”,“multi-head\n",
      "attention”,“audio”,“Speech”,\n",
      "“audio processing”,“speech\n",
      "processing”,\n",
      "Audio & Speech Recognition\n",
      "70\n",
      "16\n",
      "Audio & Speech Separation\n",
      "Audio & Speech Classification\n",
      "Signal Pro-\n",
      "cessing\n",
      "“Transformer”, “Attention”,\n",
      "“Self-attention”, “multi-head\n",
      "attention”, “signal”, “signal\n",
      "processing” , “wireless”,\n",
      "“wireless signal”, “wireless\n",
      "network”, “biosignal”, “medical\n",
      "signal”.\n",
      "Wireless network Signal\n",
      "processing\n",
      "23\n",
      "11\n",
      "Medical Signal Processing\n",
      "Table 1: Transformer models’ field of application, used keywords for paper search, popular deep learning tasks, number of\n",
      "relevant papers by search, and number of selected models for taxonomy and further discussion.\n",
      "In the field of application, we have classified the selected models based on their task execution and developed a taxonomy of\n",
      "transformer applications. Our analysis involved a comprehensive examination of the models, including their structures, char-\n",
      "acteristics, operational methods, and datasets, among others. Based on this investigation, we provide an in-depth discussion\n",
      "of the potential future applications of transformers. To conduct this research, we consulted various prominent research repos-\n",
      "itories, such as “AAAI”, “ACM”, “ACL”, “CVPR”, “ICML”, “ICLR”, “ICCV”, “NeurlIPS”, “LREC”, “IEEE”, “PMLR”,\n",
      "”National Library of medicine”,“SCOPUS”, “MDPI”, “ScienceDirect”, and “Cornell University-arxiv library”. Table 1 de-\n",
      "picts the category of the selected models.\n",
      "4\n",
      "RELATED WORK\n",
      "Transformers have been subjected to a number of surveys in recent years due to their effectiveness and a broad range of\n",
      "applications. We recorded more than 50 survey papers about transformers in various digital libraries and examined them.\n",
      "After carefully considering these surveys, we selected 17 significant survey papers for further in-depth analysis of their\n",
      "works. During this process, we considered the surveys published in reputed conferences and journals with high number of\n",
      "citations, whereas we discarded the papers that have not been published yet. We extensively analyzed this set of 17 papers,\n",
      "delving into their content and investigating their respective fields of work and applications. We prioritized examining the\n",
      "resemblances and disparities between the existing surveys and our own paper. Our investigation revealed that numerous\n",
      "surveys primarily concentrated on the architecture and efficiency of transformers, while others solely focused on applications\n",
      "in NLP and computer vision. However, only a few explored the utilization of transformers in multi-modal combinations\n",
      "involving text and image data. These findings, along with supporting details, are presented in Table 2.\n",
      "Several review papers centered their attention on conducting architecture and performance-based analyses of transformers.\n",
      "Among them, the survey paper titled “A Survey of Transformers” stands out as it offers a comprehensive examination of\n",
      "different X-formers and introduces a taxonomy based on architecture, pre-training, and application (Lin et al., 2022). Another\n",
      "survey paper on transformers was entitled “Efficient Transformers: A survey” to compare the computational power and\n",
      "memory efficiency of X-formers (Tay et al., 2023). Moreover, another paper focused on light and fast transformers while it\n",
      "explored different efficient alternatives to the standard transformers (Fournier et al., 2021).\n",
      "Within the field of NLP, there exists a survey paper titled “Visualizing Transformers for NLP: A Brief Survey” (Brasoveanu\n",
      "& Andonie, 2020). This particular study centers its attention on exploring the different aspects of transformers that can\n",
      "be effectively examined and understood through the application of visual analytics techniques. On a related note, another\n",
      "survey paper delves into the realm of pre-trained transformer-based models for NLP (Subramanyam et al., 2021b). This\n",
      "\n",
      "\n",
      "## Page 8\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "study extensively discusses pretraining methods and tasks employed in these models. Moreover, it introduces a taxonomy\n",
      "that effectively categorizes the wide range of transformer-based Pre-Trained Language Models (T-PTLMs) available in the\n",
      "literature.\n",
      "Moreover, the paper entitled “Survey on Automatic Text Summarization and Transformer Models Applicability” focused on\n",
      "using transformers for text summarization tasks and proposed a transformer model that solves the issue of the long sequence\n",
      "input (Wang et al., 2020a). On the other hand, another survey worked on applying a bidirectional transformer encoder (BERT)\n",
      "in multi-layer as a word-embedding tool (Kaliyar, 2020). Furthermore, the application of transformers to detect different\n",
      "levels of emotions from text-based data has been explored in (Acheampong et al., 2021) under the tile “Transformer models\n",
      "for text-based emotion detection: a review of BERT-based approaches”. Another paper explored the use of the transformer\n",
      "language model in different information systems (Gruetzemacher & Paradice, 2022). It focused on using transformers as text\n",
      "miners to extract useful data from large organizations’ data.\n",
      "Due to huge improvements in image processing tasks and amazing applications on computer vision with the help of trans-\n",
      "former models in recent years, these models gained popularity among computer vision researchers. For instance, “Trans-\n",
      "formers in Vision: A Survey” provided a comprehensive overview of the existing transformer models in the field of computer\n",
      "vision and classified the models based on popular recognition tasks (Khan et al., 2022). A meticulous survey was undertaken\n",
      "to comprehensively analyze the merits and drawbacks of the leading “Vision Transformers”. This study placed significant\n",
      "emphasis on scrutinizing the training and testing datasets associated with these top-performing models, offering valuable\n",
      "insights into their performance and suitability for various applications (Han et al., 2023).\n",
      "Another survey paper compared transformer models developed for image and video data based on their performance in\n",
      "classification tasks (Selva et al., 2023). Recent advances in computer vision and multi-modality have been emphasized\n",
      "in another survey paper (Xu et al., 2022) comparing the performance of different transformer models and providing some\n",
      "information regarding their pre-training. Furthermore, an existing survey describes in detail several transformer models that\n",
      "have been developed for medical images; however, it does not provide information regarding medical signals (Li et al., 2023).\n",
      "Another paper gives an overview of transformer models developed in the medical field; however, it only concerns medical\n",
      "images, not medical signals (Shamshad et al., 2023).\n",
      "Multi-modality is getting very popular in deep learning tasks that helped to decipher several surveys on transformer focusing\n",
      "multi-modal domain. A paper worked on categorizing transformer vision-language models based on tasks and summarizing\n",
      "their co-responding advantages and disadvantages. Moreover, this survey paper covered video-language pre-trained mod-\n",
      "els and categorized the models into single-stream and multi-stream structures, and the performance of the models is also\n",
      "compared in this survey (Ruan & Jin, 2022). Another survey, “Perspectives and Prospects on Transformer Architecture for\n",
      "Cross-Modal Tasks with Language and Vision”, explored transformers in multi-modal visual-linguistic tasks (Shin et al.,\n",
      "2022). Other than NLP, computer vision and multi-modality, transformers are getting significant attention from researchers\n",
      "to apply to other fields such as time series and reasoning tasks.\n",
      "Approach\n",
      "Fields of\n",
      "Application\n",
      "Similarities\n",
      "Differences\n",
      "Q Fournier\n",
      "et al.\n",
      "(Fournier\n",
      "et al., 2021)\n",
      "Performance\n",
      "/Archi-\n",
      "tecture\n",
      "• A classification of the trans-\n",
      "formers is suggested, and this\n",
      "classification is based on atten-\n",
      "tion mechanism modification or\n",
      "architecture modification\n",
      "• This pare surveyed the different alternatives of the standard\n",
      "transformers that are more efficient in terms of time and mem-\n",
      "ory complexities, and these alternatives are categorized by ei-\n",
      "ther modifying the attention mechanism or the network archi-\n",
      "tecture. Their classification is based on the change in architec-\n",
      "ture or change in attention mechanism, while our classification\n",
      "is driven by application areas.\n",
      "T. Lin et\n",
      "al. (Lin\n",
      "et al., 2022)\n",
      "Performance\n",
      "/Archi-\n",
      "tecture\n",
      "• Proposed taxonomy of X-\n",
      "formers covering several fields\n",
      "• This existing survey compared X-formers from architectural\n",
      "modification, pre-training, and a very small range of application\n",
      "perspectives, while our survey deeply focuses on popular tasks\n",
      "under each field of application.\n",
      "• The wireless/medical signal processing and cloud computing\n",
      "tasks application were missing in this exciting survey, while our\n",
      "survey covers these tasks and applications.\n",
      "Continued on next page\n",
      "\n",
      "\n",
      "## Page 9\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Table 2 – continued from previous page\n",
      "Approach\n",
      "Fields of\n",
      "Application\n",
      "Similarities\n",
      "Differences\n",
      "Y. Tay et\n",
      "al. (Tay\n",
      "et al., 2023)\n",
      "Performance\n",
      "/Archi-\n",
      "tecture\n",
      "• Proposed a taxonomy consid-\n",
      "ering the primary use case of\n",
      "transformer models in language\n",
      "and vision domains .\n",
      "• This existing survey compared the computational power and\n",
      "memory efficiency of transformer models, whereas our survey\n",
      "focuses on deep learning tasks and applications.\n",
      "• This exciting survey focused on language and vision domain\n",
      "only, while we cover other top five fields of transformer appli-\n",
      "cations: NLP, computer vision, multi-modality, audio/speech,\n",
      "and signal processing.\n",
      "A. M. P.\n",
      "Bras¸oveanu\n",
      "et al.\n",
      "(Brasoveanu\n",
      "& Andonie,\n",
      "2020)\n",
      "Natural\n",
      "language\n",
      "Processing-\n",
      "NLP\n",
      "• Explain transformer architec-\n",
      "ture and explain its features.\n",
      "• Our survey describes the transformer model and the significant\n",
      "models’ working processing for a range of tasks. However, this\n",
      "existing paper focused on visualization techniques used to ex-\n",
      "plain the most recent transformer architectures and explored\n",
      "two large tool classes to explain the inner workings of Trans-\n",
      "formers.\n",
      "• we covered five fields of transformer applications: NLP, com-\n",
      "puter vision, multi-modality, audio/speech, and signal process-\n",
      "ing and this exciting survey focused on the models for NLP\n",
      "only.\n",
      "W Guan et\n",
      "al. (Wang\n",
      "et al.,\n",
      "2020a)\n",
      "Natural\n",
      "language\n",
      "Processing-\n",
      "NLP\n",
      "• Survey an application area of\n",
      "transformers, which is text\n",
      "summarization, which is one\n",
      "of the application areas covered\n",
      "in our survey\n",
      "• The authors propose a transformer-based summarizer that\n",
      "solves the issues of standard transformers that cannot take a\n",
      "long text as an input. They survey different use cases of apply-\n",
      "ing transformers to different text summarization tasks and they\n",
      "only cover text summarization. no proposed transformers have\n",
      "been built in our survey.\n",
      "R Kumar\n",
      "(Kaliyar,\n",
      "2020)\n",
      "Natural\n",
      "language\n",
      "Processing-\n",
      "NLP\n",
      "• Discussion of different NLP\n",
      "downstream tasks that BERT\n",
      "performs. BERT is covered\n",
      "in our survey as well as the\n",
      "different NLP tasks\n",
      "• Survey different techniques on using BERT as a word-\n",
      "embedder against traditional word-embedding techniques.\n",
      "Their survey is only focused as using transformers as a tool\n",
      "for embedding text\n",
      "F Acheam-\n",
      "pong et al.\n",
      "(Acheam-\n",
      "pong et al.,\n",
      "2021)\n",
      "Natural\n",
      "language\n",
      "Processing-\n",
      "NLP\n",
      "• Survey different transformer\n",
      "architectures that accomplish\n",
      "the emotion detection task. We\n",
      "do the same, the application of\n",
      "different transformers to the\n",
      "same type if task\n",
      "• Survey the application of transformer architecture to a single\n",
      "application area but in too much detail, which is emotion detec-\n",
      "tion from text-based data, a form of sentiment analysis but the\n",
      "goal is to extract fine-grained emotion from the data. The task\n",
      "of sentiment analysis is covered in our survey, but we didn’t\n",
      "cover especially the task of detecting emotions on different lev-\n",
      "els and not just as a binary classification task as usually done in\n",
      "sentiment analysis\n",
      "Continued on next page\n",
      "\n",
      "\n",
      "## Page 10\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Table 2 – continued from previous page\n",
      "Approach\n",
      "Fields of\n",
      "Application\n",
      "Similarities\n",
      "Differences\n",
      "R Gruet-\n",
      "zemacher et\n",
      "al. (Gruet-\n",
      "zemacher\n",
      "& Paradice,\n",
      "2022)\n",
      "Natural\n",
      "language\n",
      "Processing-\n",
      "NLP\n",
      "• Survey the progress of trans-\n",
      "formers in the text-mining ap-\n",
      "plication area. We do cover\n",
      "in our survey the progress of\n",
      "transformers on a wide variety\n",
      "of tasks\n",
      "• Tackle the different transformers on how they can be used as\n",
      "text miners for organizations that have huge amounts of un-\n",
      "structured data against traditional NLP text-mining techniques\n",
      "J. Selva et\n",
      "al. (Selva\n",
      "et al., 2023)\n",
      "Computer\n",
      "Vision\n",
      "• This paper is an overview of\n",
      "transformers developed for\n",
      "modeling images and video\n",
      "data\n",
      "• This survey focuses solely on image and video data. Models\n",
      "are compared based on their performance in video classifica-\n",
      "tion, it does not cover any other applications. The paper pro-\n",
      "poses a taxonomy of various transformer models based on their\n",
      "recurrence properties, memory capacities, and architectural de-\n",
      "sign\n",
      "K. S.\n",
      "Kalyan et\n",
      "al. (Subra-\n",
      "manyam\n",
      "et al.,\n",
      "2021a)\n",
      "Natural\n",
      "language\n",
      "Processing-\n",
      "Medical\n",
      "• This paper provides an\n",
      "overview of the developed\n",
      "transformer-based BPLMs for\n",
      "a wide range of NLP tasks,\n",
      "including Natural language\n",
      "inference, Entity extraction,\n",
      "Relation extraction, Semantic\n",
      "textual similarity, Text classi-\n",
      "fication, Question answering,\n",
      "and Text summarization\n",
      "• This survey addresses only transformer-based biomedical pre-\n",
      "trained language models, which restricts its scope to the spe-\n",
      "cific field of biomedical natural language processing. The tax-\n",
      "onomy does not distinguish models based on the type of ap-\n",
      "plication they are used for, but rather based on the dataset of\n",
      "pre-training, the embedding type, and other criteria such as the\n",
      "targeted language\n",
      "K. Han et\n",
      "al. (Han\n",
      "et al., 2023)\n",
      "Computer\n",
      "Vision\n",
      "• Categorized vision transformer\n",
      "models based on different tasks\n",
      "• This existing paper analyzed transformer models’ advantages\n",
      "and disadvantages, and efficient transformer methods for the\n",
      "backbone network, while our survey categorizes transformer\n",
      "models based on tasks and summarize downstream tasks and\n",
      "commonly used dataset.\n",
      "• While our survey paper classified computer vision tasks into\n",
      "two segments: natural image processing & medical image pro-\n",
      "cessing and then focused on popular computer vision like vi-\n",
      "sual question answering, classification, segmentation, ques-\n",
      "tion answering, and so on, then this existing paper focused on\n",
      "high/mid-level vision, low-level vision, and video processing\n",
      "computer vision tasks.\n",
      "• This survey focused on computer vision tasks only, while we\n",
      "covered other four fields of applications-NLP, Multi-modal,\n",
      "Audio/speech, and signal processing besides computer vision\n",
      "Continued on next page\n",
      "\n",
      "\n",
      "## Page 11\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Table 2 – continued from previous page\n",
      "Approach\n",
      "Fields of\n",
      "Application\n",
      "Similarities\n",
      "Differences\n",
      "Y. Xu et\n",
      "al. (Xu\n",
      "et al., 2022)\n",
      "Computer\n",
      "Vision\n",
      "• The survey covers the fields\n",
      "of computer vision and multi-\n",
      "modal in a similar fashion to\n",
      "our survey\n",
      "• This survey focuses primarily on recent advancements in com-\n",
      "puter vision by comparing the performance of different trans-\n",
      "former models. Specifically, this study discusses four areas of\n",
      "research: advances in the design of the ViT models for image\n",
      "classification, high-level vision tasks (such as object detection\n",
      "and semantic segmentation), low-level vision tasks (such as\n",
      "super-resolution, and image generation), and multimodal learn-\n",
      "ing (such as visual question answering (VQA), image caption-\n",
      "ing)\n",
      "J Li et al.\n",
      "(Li et al.,\n",
      "2023)\n",
      "Computer\n",
      "Vision\n",
      "• Comparative analysis of trans-\n",
      "former models is presented\n",
      "in this paper for several tasks\n",
      "involved in medical vision.\n",
      "Several criteria are considered\n",
      "when comparing papers, in-\n",
      "cluding the type of dataset, the\n",
      "type of input data, and the ar-\n",
      "chitecture of the model\n",
      "• This paper describes in detail several transformer models that\n",
      "have been developed for medical images; however, it does not\n",
      "provide information regarding medical signals\n",
      "F Shamshad\n",
      "et al.\n",
      "(Shamshad\n",
      "et al., 2023)\n",
      "Computer\n",
      "Vision-\n",
      "medical\n",
      "• A review of a number of trans-\n",
      "former models with a focus\n",
      "on some tasks related to medi-\n",
      "cal images and different image\n",
      "modalities, and a description of\n",
      "the datasets used for these tasks\n",
      "• This paper compares deep learning models starting with CNNs\n",
      "and moving up to vision transformers. In this paper, medical\n",
      "image modalities and several medical computer vision tasks\n",
      "are discussed to compare papers through the specification of\n",
      "datasets used and also provide an overview of models’ perfor-\n",
      "mance. In this paper, the comparison is based solely on medical\n",
      "images; medical signals are not considered\n",
      "Salman\n",
      "Khan et\n",
      "al. (Khan\n",
      "et al., 2022)\n",
      "Computer\n",
      "Vision\n",
      "• A overview of existing trans-\n",
      "former computer vision models\n",
      "and classified the models based\n",
      "on popular tasks\n",
      "• While this existing survey paper compared the popular tech-\n",
      "niques in terms of architectural design and experimental value,\n",
      "while our survey worked based on popular tasks and applica-\n",
      "tions.\n",
      "• In the computer vision section, we put a special focus on Medi-\n",
      "cal image tasks besides natural image processing.\n",
      "• This survey focused on computer vision tasks only, while we\n",
      "covered other four fields of applications, namely NLP, Multi-\n",
      "modal, Audio/speech, and signal processing besides computer\n",
      "vision\n",
      "L. Ruan et\n",
      "al. (Ruan &\n",
      "Jin, 2022)\n",
      "Multi-\n",
      "modal(NLP-\n",
      "CV)\n",
      "• Categorize transformer vision-\n",
      "language models based on tasks\n",
      "and summarize downstream\n",
      "tasks and commonly used video\n",
      "dataset\n",
      "• This existing survey focused on multi-modal(NLP-CV) tasks\n",
      "only, while we covered other four fields of applications-NLP,\n",
      "Computer vision, Audio/speech, and signal processing besides\n",
      "multi-modal\n",
      "Continued on next page\n",
      "\n",
      "\n",
      "## Page 12\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Table 2 – continued from previous page\n",
      "Approach\n",
      "Fields of\n",
      "Application\n",
      "Similarities\n",
      "Differences\n",
      "A Shin et\n",
      "al. (Shin\n",
      "et al., 2022)\n",
      "Multi-modal\n",
      "(Perfor-\n",
      "mance\n",
      "/Archi-\n",
      "tecture)\n",
      "• They survey transformers for\n",
      "multi-modal tasks, which we\n",
      "do also include in our different\n",
      "application tasks\n",
      "• Cover only one application area in detail, which is multimodal\n",
      "visual-linguistic tasks\n",
      "Table 2: Comparative summary between our survey and the existing surveys\n",
      "After having a thorough search and analysis of these survey papers, we realized that still, a survey on transformers is missing\n",
      "which focused on a most common field of application together and discussed the contribution of transformer-based mod-\n",
      "els in the execution of different deep learning tasks in regarding fields of application. In this paper, first, we surveyed all\n",
      "transformer-based models out there based on our best possible search, identified top-5 fields of application and the proportion\n",
      "of transformers models’ contribution in the progression of top fields of application: Natural Language Processing (NLP),\n",
      "Computer Vision (CV), Multi-modal, Audio and Speech, and Signal processing. Moreover, we proposed a taxonomy of\n",
      "transformer models based on these top five fields of application whereas top performed, and significant models are being\n",
      "classified and analyzed based on their task’s execution under the regarding fields. Through this survey, different aspects of\n",
      "Transformer-based models’ tasks and applications become more explicit in different fields, and it also depicted the fields of\n",
      "transformer applications that got higher and less attention by the researchers so far. Based on this analysis, we discussed\n",
      "future prospects and possibilities of transformers application in different fields of application. One of the objectives of this\n",
      "survey is to make a combined source of reference for a better understanding of the contribution of transformer models in\n",
      "different fields and the characteristics and execution methods of the models which kept significant contributions to improving\n",
      "the performance of the different tasks in their fields. Besides, this paper would be a resource to perceive future possibilities\n",
      "and scope of transformer-based models’ application for enthusiastic researchers who wants to extend and work for the new\n",
      "application of transformers.\n",
      "5\n",
      "TRANSFORMER APPLICATIONS\n",
      "Since 2017, the transformer model has emerged as a highly attractive research model in the field of deep learning. Originally\n",
      "developed for processing long-range textual sentences. However, its scope has expanded to a variety of applications beyond\n",
      "NLP tasks. In fact, after a series of successes in NLP, researchers turned their attention to computer vision, exploring the\n",
      "potential of transformer models’ global attention capability, while Convolutional Neural Networks (CNNs) were adept at\n",
      "tracking local features. The transformer model has also been tested and applied in various other fields and for various tasks.\n",
      "To gain a deeper understanding of transformer applications, we conducted a comprehensive search of various research libraries\n",
      "and reviewed the transformer models available from 2017 to the present day. Our search yielded approximately more than\n",
      "650 transformer-based models that are being applied in various fields.\n",
      "We identified the major fields in which transformer models are being used, including NLP, CV, Multi-modal applications,\n",
      "Audio and Speech processing, and Signal Processing. Our analysis provides an overview of the transformer models available\n",
      "in each field, their applications, and their impact on their respective industries.\n",
      "Figure 4 shows the percentage breakdown of the transformer models proposed so far across different application fields. Our\n",
      "analysis revealed approximately 250 transformer-based models for NLP, representing around 40% of the total transformer\n",
      "models collected. Moreover, we accounted for approximately 200 models for computer vision. Due to the different processing\n",
      "of natural and medical images, and the extensive growth of both fields, we segmented computer vision into two categories: (i)\n",
      "Natural Image Processing and (ii) Medical Image Processing. As per this categorization, Natural Image Processing accounted\n",
      "for 20% of transformer-based models, medical image processing accounted for 11%, and combinedly they accounted for\n",
      "31% of transformer-based models. Additionally, our analysis identified approximately 90 transformer models for multi-\n",
      "modal applications, representing 15% of the total, and around 70 models for audio and speech processing, representing 11%\n",
      "of total transformer models. Finally, only 4% of transformer models were recorded for signal processing.\n",
      "Our analysis provides a clear understanding of the proportion of attention received by transformer applications in each field,\n",
      "facilitating the identification of further research areas and tasks where transformer models can be implemented.\n",
      "6\n",
      "APPLICATION-BASED CLASSIFICATION TAXONOMY OF TRANSFORMERS\n",
      "As a result of conducting a thorough comprehensive analysis of all selected articles following the methodology explained\n",
      "in Section 3, we noticed that the existing categorizations did not fully capture the wide range of transformer-based models\n",
      "and their diverse applications across different fields. Hence, in this study, we aimed to propose a more comprehensive\n",
      "taxonomy of transformers that would reflect their practical applications. To achieve this, we carefully reviewed a large\n",
      "number of transformer models and classified them based on their tasks within their respective fields of application. Our\n",
      "\n",
      "\n",
      "## Page 13\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Figure 4: Proportion of transformer application in Top-5 fields\n",
      "analysis identified several highly impactful and significant transformer-based models that have been successfully applied in a\n",
      "variety of fields. We then organized these models into five different application areas: Natural Language Processing (NLP),\n",
      "Computer Vision, Multi-modality, Audio and Speech, and Signal Processing. The proposed taxonomy in Figure 5 provides a\n",
      "more nuanced and comprehensive framework for understanding the diverse applications of transformers. We believe that this\n",
      "taxonomy would be beneficial for researchers and practitioners working on transformer-based models, as it would help them\n",
      "to identify the most relevant models and techniques for their specific applications.\n",
      "6.1\n",
      "NATURAL LANGUAGE PROCESSING (NLP)\n",
      "Transformers have become a vital tool in NLP, and various NLP tasks have largely benefited from these models. Our pro-\n",
      "posed taxonomy focuses on NLP and organizes transformer models into seven popular NLP tasks, including Translation,\n",
      "Summarization, Classification and Segmentation, Question Answering, Text Generation, Natural Language Reasoning, and\n",
      "Automated Symbolic Reasoning. To ensure a comprehensive analysis, we only considered transformer models that have\n",
      "significantly impacted the NLP field and improved its performance. Our analysis included an in-depth discussion of each\n",
      "NLP task, along with essential information about each model presented in Table 3. We also highlighted the significance and\n",
      "working methods of each model. This taxonomy provides a valuable framework for understanding the different transformer\n",
      "models used in NLP and their practical applications. It can help researchers and practitioners select the most appropriate\n",
      "transformer model for their specific NLP task.\n",
      "6.1.1\n",
      "LANGUAGE TRANSLATION\n",
      "Language translation is a fundamental task in NLP, aimed at converting input text from one language to another. Its primary\n",
      "objective is to produce an output text that accurately reflects the meaning of the source text in the desired language. For\n",
      "example, given an English sentence as input text, the task aims to produce its equivalent in French or any other desired\n",
      "language. The original transformer model was developed explicitly for the purpose of language translation, highlighting the\n",
      "significance of this task in the NLP field. Table 3 identifies the transformer-based models that have demonstrated significant\n",
      "performance in the Language Translation task. These models play a vital role in facilitating effective communication and\n",
      "collaboration across different languages, enabling more efficient information exchange and knowledge sharing. Overall, the\n",
      "language translation task represents a crucial area of research in NLP, with significant implications for diverse applications,\n",
      "including business, science, education, and social interactions. The transformer-based models presented in the table offer\n",
      "promising solutions for advancing the state-of-the-art in this field, paving the way for new and innovative approaches to\n",
      "language translation (Chowdhary & Chowdhary, 2020, Monroe, 2017, Hirschberg & Manning, 2015).\n",
      "\n",
      "### Images\n",
      "\n",
      "\n",
      "The image depicts a pie chart that illustrates the distribution of various categories, with each category represented by a distinct color and labeled with its corresponding percentage. The categories are as follows:\n",
      "\n",
      "*   Natural Images: 20%\n",
      "*   Medical Images: 11%\n",
      "*   Computer Vision: 31%\n",
      "*   NLP: 40%\n",
      "*   Multi-Modal: 15%\n",
      "*   Audio/Speech: 11%\n",
      "*   Signal: 4%\n",
      "\n",
      "The pie chart is divided into seven sections, with each section representing a different category. The categories are arranged in a clockwise direction, starting from the top-left section, which represents Natural Images. The remaining sections are arranged in a counterclockwise direction, with the final section, Signal, located at the bottom-right.\n",
      "\n",
      "The pie chart provides a clear visual representation of the distribution of the categories, allowing viewers to easily compare the percentages of each category. The use of distinct colors for each category enhances the visual appeal of the chart and makes it easier to distinguish between the different categories. Overall, the pie chart is an effective tool for communicating the distribution of the categories in a clear and concise manner.\n",
      "\n",
      "\n",
      "## Page 14\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Figure 5: Application-based taxonomy of transformer models\n",
      "\n",
      "### Images\n",
      "\n",
      "\n",
      "This image depicts a flowchart with a brown arrow pointing to the left, labeled \"Transformer,\" which is the central element of the chart. The chart is divided into several sections, each represented by a distinct color and labeled in white text: \"Natural Language Processing,\" \"Computer Vision,\" \"Medical Images,\" \"Multi-Modal,\" \"Audio/Speech,\" and \"Signal Processing.\" These sections are connected by arrows, illustrating the relationships between the various components.\n",
      "\n",
      "The chart also features numerous green boxes, each containing a specific term or concept related to the Transformer model. These terms are arranged in a hierarchical structure, with more general categories branching out into more specific subcategories. The chart's purpose is to provide a comprehensive overview of the Transformer model's capabilities and applications, highlighting its versatility and range of use cases.\n",
      "\n",
      "\n",
      "## Page 15\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "Transformer-\n",
      "2017\n",
      "(Vaswani\n",
      "et al., 2017)\n",
      "Language\n",
      "Translation\n",
      "2017\n",
      "Encoder &\n",
      "Decoder\n",
      "No\n",
      "NA\n",
      "WMT 2014 English-\n",
      "German,WMT 2014\n",
      "English-French\n",
      "XLM\n",
      "(Conneau\n",
      "& Lample,\n",
      "2019)\n",
      "Translation and\n",
      "Classification\n",
      "for multiple\n",
      "language\n",
      "2019\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "WMT’16, WMT’14\n",
      "English-French,\n",
      "WMT’16\n",
      "(English-German,\n",
      "English-Romanian,\n",
      "Romanian-English)\n",
      "Wikipedia of 16 XNLI lan-\n",
      "guages(English, French, Span-\n",
      "ish, Russian, Arabic, Chinese,\n",
      "Hindi, German, Greek, Bul-\n",
      "garian, Turkish, Vietnamese,\n",
      "Thai, Urdu, Swahili, Japanese)\n",
      "BART (Lewis\n",
      "et al., 2020)\n",
      "Language\n",
      "Translation,\n",
      "Sentence\n",
      "Reconstruction,\n",
      "Comprehension,\n",
      "text Generation\n",
      "2019\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "Corrupting docu-\n",
      "ments, 1M steps\n",
      "on a combina-\n",
      "tion of books and\n",
      "Wikipedia data,\n",
      "news, stories, and\n",
      "web text (Training)\n",
      "SQuAD, MNLI, ELI5,\n",
      "XSum, ConvAI2, CNN/DM,\n",
      "CNN/DailyMail, WMT16\n",
      "Romanian-English, augmented\n",
      "with back-translation data\n",
      "from Sennrich et al. (2016).\n",
      "Switch\n",
      "Transformer\n",
      "(Fedus\n",
      "et al., 2021)\n",
      "Language\n",
      "understanding\n",
      "task- Translation,\n",
      "question\n",
      "answering,\n",
      "Classification,\n",
      "and so on.\n",
      "2021\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "C4(Colossal Clean\n",
      "Crawled Corpus)\n",
      "GLUE and SuperGLUE\n",
      "benchmarks, CNNDM, BBC\n",
      "XSum, and SQuAD data sets,\n",
      "ARC Reasoning Challenge,3\n",
      "closed-book question an-\n",
      "swering data sets (Natural\n",
      "Questions, Web Questions,\n",
      "and Trivia QA), Wino-\n",
      "grande Schema Challenge,\n",
      "Adversarial NLI Benchmark\n",
      "Table 3: Transformer models for NLP - language translation task\n",
      "• Transformer:\n",
      "In 2017, Vaswani et al. (Vaswani et al., 2017) introduced the first transformer model, which has since\n",
      "revolutionized the field of NLP. The transformer model was designed specifically for language translation and is known as\n",
      "the Vanilla transformer model. Unlike its predecessors, the transformer model incorporates both an encoder and a decoder\n",
      "module, employing multi-head attention and masked-multi-head attention mechanisms. The encoder module is responsible\n",
      "for analyzing the contextual information of the input language, while the decoder module generates the output in the target\n",
      "language, using the output of the encoder and masked multi-head attention. The transformer model’s success is largely\n",
      "attributed to its ability to perform parallel computations, which allows it to process words simultaneously with positional\n",
      "information. This feature makes it highly efficient in processing large volumes of text and enables it to handle long-range\n",
      "dependencies, which are crucial in language translation.\n",
      "• XLM: It is a cross-lingual language pretraining model developed to support multiple languages. The model is built using\n",
      "two methods: a supervised method and an unsupervised method. The unsupervised method utilizes Masked Language\n",
      "Modeling (MLM) and Casual Language Modeling (CLM) techniques and has shown remarkable effectiveness in translation\n",
      "tasks. On the other hand, the supervised method has further improved the translation tasks (Conneau & Lample, 2019).\n",
      "This combination of supervised and unsupervised learning has made the XLM model a powerful tool for cross-lingual\n",
      "applications, making it possible to perform natural language processing tasks in multiple languages. The effectiveness\n",
      "of the XLM model in translation tasks has made it a popular choice among researchers in the field of natural language\n",
      "processing.\n",
      "• BART: BART (Bidirectional and Auto-Regressive Transformers) is an advanced pre-trained model primarily aimed at\n",
      "cleaning up the corrupt text. It features two pre-training stages: the first stage corrupts the text with noise, while the second\n",
      "stage focuses on recovering the original text from the noisy version. BART employs a transformer translation model that\n",
      "integrates both the encoder and decoder modules, allowing it to perform various tasks such as text generation, translation,\n",
      "and comprehension with impressive accuracy (Lewis et al., 2020). Its bi-directional approach enables it to learn from the\n",
      "past and future tokens, while its auto-regressive properties make it suitable for generating output tokens sequentially. These\n",
      "features make BART an incredibly versatile model for various natural language processing tasks.\n",
      "• Switch Transformer: The Switch transformer model is a recent development in the field of NLP that has gained attention\n",
      "for its ability to perform various tasks with high accuracy. It incorporates two key components: a permutation-based routing\n",
      "mechanism and a gating mechanism. The permutation-based routing mechanism allows the model to learn a routing strategy\n",
      "\n",
      "\n",
      "## Page 16\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "that selects which parts of the input sequence to attend to. This enables the model to handle variable-length inputs, as it can\n",
      "dynamically determine which parts of the sequence to attend to for each input. The gating mechanism allows the model to\n",
      "perform both Classification and Segmentation tasks. The gating mechanism is designed to learn how to combine information\n",
      "from different parts of the input sequence in order to make predictions. This allows the model to perform Classification\n",
      "tasks by predicting a label for the entire input sequence, or Segmentation tasks by predicting labels for each part of the\n",
      "input sequence. The Switch transformer is a highly versatile model that can effectively perform both Classification and\n",
      "Segmentation tasks (Bogatinovski et al., 2022). Its detailed description can be found in the dedicated Classification &\n",
      "Segmentation section below (Fedus et al., 2021).\n",
      "6.1.2\n",
      "CLASSIFICATION & SEGMENTATION\n",
      "Text classification and segmentation are fundamental tasks in natural language processing (NLP) that enable the automatic\n",
      "organization and analysis of large volumes of textual data. Text classification involves assigning tags or labels to text based\n",
      "on its contents, such as sentiment, topic, or intent, among others. This process helps to categorize textual documents from\n",
      "different sources and can be useful in a variety of applications, such as recommendation systems, information retrieval, and\n",
      "content filtering. On the other hand, text segmentation involves dividing the text into meaningful units, such as sentences,\n",
      "words, or topics, to facilitate further analysis or processing. This task is crucial for various NLP applications, including\n",
      "language understanding, summarization, and question answering, among others (Chowdhary & Chowdhary, 2020, Kuhn,\n",
      "2014, Hu et al., 2016).\n",
      "Transformer-based models have been shown to achieve state-of-the-art performance in text classification and segmentation\n",
      "tasks. These models are characterized by their ability to capture long-range dependencies and contextual information in text,\n",
      "making them well-suited for complex NLP tasks. Table 4 highlights some of the most prominent transformer-based models\n",
      "that have demonstrated significant performance in text classification and segmentation tasks.\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "GPT &\n",
      "variants\n",
      "(Radford\n",
      "et al., 2018;\n",
      "2019, Brown\n",
      "et al., 2020)\n",
      "Text classifica-\n",
      "tion, Question\n",
      "answering,\n",
      "textual entail-\n",
      "ment, semantic\n",
      "similarity\n",
      "2018\n",
      "Decoder\n",
      "Yes\n",
      "Book corpus\n",
      "SNLI, MNLI, QNLI, Sc-\n",
      "iTail, RTE, RACE, CNN,\n",
      "SQuaD, MRPC, QQP,\n",
      "STS-B, SST2 & CoLA\n",
      "XLM\n",
      "(Conneau\n",
      "& Lample,\n",
      "2019)\n",
      "Translation and\n",
      "Classification\n",
      "for multiple\n",
      "language\n",
      "2019\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "WMT’16, WMT’14\n",
      "English-French,\n",
      "WMT’16\n",
      "(English-German,\n",
      "English-Romanian,\n",
      "Romanian-English)\n",
      "Wikipedia of 16 XNLI lan-\n",
      "guages(English, French, Span-\n",
      "ish, Russian, Arabic, Chinese,\n",
      "Hindi, German, Greek, Bul-\n",
      "garian, Turkish, Vietnamese,\n",
      "Thai, Urdu, Swahili, Japanese)\n",
      "T5 (Raffel\n",
      "et al., 2020)\n",
      "Text summariza-\n",
      "tion, Question\n",
      "answering, text\n",
      "classification\n",
      "2020\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "C4 (Colossal Clean\n",
      "Crawled Corpus)\n",
      "GLUE and SuperGLUE\n",
      "benchmarks, CNN/Daily\n",
      "Mail abstractive summa-\n",
      "rization, SQuAD question\n",
      "answering, WMT En-\n",
      "glish to German, French,\n",
      "and Romanian translation\n",
      "Charformer\n",
      "(Tay et al.,\n",
      "2022)\n",
      "Classification\n",
      "task, toxicity\n",
      "detection,\n",
      "and so on.\n",
      "2022\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "The same datasets\n",
      "used in T5 model-\n",
      "C4(Colossal Clean\n",
      "Crawled Corpus)\n",
      "GLUE IMDb, AGNews,(Maas\n",
      "et al., 2011), (Zhang et al.,\n",
      "2015), Civil Comments,\n",
      "Wikipedia Comments,\n",
      "TyDiQA-GoldP, XQuAD,\n",
      "MLQA, XNLI, PAWS-X..\n",
      "Continued on next page\n",
      "\n",
      "\n",
      "## Page 17\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Table 4 – continued from previous page\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "Switch\n",
      "Transformer\n",
      "(Fedus\n",
      "et al., 2021)\n",
      "Language\n",
      "understanding\n",
      "task- Translation,\n",
      "question\n",
      "answering,\n",
      "Classification,\n",
      "and so on.\n",
      "2021\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "C4(Colossal Clean\n",
      "Crawled Corpus)\n",
      "GLUE and SuperGLUE\n",
      "benchmarks, CNNDM, BBC\n",
      "XSum and SQuAD data sets,\n",
      "ARC Reasoning Challenge,3\n",
      "closed-book question an-\n",
      "swering data sets (Natural\n",
      "Questions, Web Questions,\n",
      "and Trivia QA), Wino-\n",
      "grande Schema Challenge,\n",
      "Adversarial NLI Benchmark\n",
      "Table 4: Transformer models for NLP - language classification & segmentation tasks\n",
      "• Charformer: It is a transformer-based model that introduces Gradient-based subword tokenization (GBST), a lightweight\n",
      "approach to learning latent subwords directly from characters at the byte level. The model has both English and multi-\n",
      "lingual variants and has demonstrated outstanding performance on language understanding tasks, such as the classification\n",
      "of long text documents (Tay et al., 2022).\n",
      "• Switch Transformer:\n",
      "The use of pre-trained models such as BERT and GPT, trained on large datasets, has gained\n",
      "popularity in the field of natural language processing. However, there are concerns about the economic and environmental\n",
      "costs of training such models. To address these concerns, the Switch transformer was introduced, which offers a larger\n",
      "model size without a significant increase in computational cost. The Switch transformer replaces the feed-forward neural\n",
      "network (FFN) with a switch layer that contains multiple FFNs, resulting in a model with trillions of parameters. Despite\n",
      "the increase in model size, the computational cost of the Switch transformer remains comparable to that of other models. In\n",
      "fact, the Switch transformer has been evaluated on 11 different tasks and has shown significant improvement in tasks such\n",
      "as translation, question-answering, classification, and summarization (Fedus et al., 2021).\n",
      "GPT & Variants, XLM, T5: These models are versatile and capable of performing a range of NLP tasks, including but\n",
      "not limited to classification, segmentation, question answering, and language translation. Section 6.1.3 will provide detailed\n",
      "description.\n",
      "6.1.3\n",
      "QUESTION ANSWERING\n",
      "Question Answering is a classical NLP task. It involves matching a text query to the most relevant answer in the form of text,\n",
      "based on the relevance of the text to the query. This task is challenging, as finding a concise and accurate answer to a given\n",
      "query can be difficult (Chowdhary & Chowdhary, 2020, Hirschman & Gaizauskas, 2001). Recent research has focused on\n",
      "this task, leading to the development of several transformer-based models that leverage deep learning techniques to improve\n",
      "the accuracy and efficiency of this task. A detailed overview of these models is provided in Table 5.\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "BERT\n",
      "(Devlin\n",
      "et al., 2019)\n",
      "Question\n",
      "answering,\n",
      "Sentence\n",
      "Prediction,\n",
      "language\n",
      "understanding\n",
      "2018\n",
      "Encoder\n",
      "Yes\n",
      "Book Corpus,\n",
      "English Wikipedia\n",
      "SQuAD v1.1, SQuAD\n",
      "v2.0, SWAG, QNLI, MNLI\n",
      "ELECTRA\n",
      "(Clark et al.,\n",
      "2020a)\n",
      "Language\n",
      "understanding\n",
      "tasks- Question\n",
      "answering\n",
      "and so on\n",
      "2020\n",
      "Encoder\n",
      "Yes\n",
      "Wikipedia,\n",
      "BooksCorpus,\n",
      "ClueWeb, Common-\n",
      "Crawl, Gigaword\n",
      "SQuAD 1.1,\n",
      "SQuAD 2.0, GLUE\n",
      "Continued on next page\n",
      "\n",
      "\n",
      "## Page 18\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Table 5 – continued from previous page\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset(Fine-tuning,\n",
      "Training, Testing)\n",
      "GPT &\n",
      "variants\n",
      "(Radford\n",
      "et al., 2018;\n",
      "2019, Brown\n",
      "et al., 2020)\n",
      "Text classifica-\n",
      "tion, Question\n",
      "answering,\n",
      "textual entail-\n",
      "ment, semantic\n",
      "similarity\n",
      "2018\n",
      "Decoder\n",
      "Yes\n",
      "Book corpus\n",
      "SNLI, MNLI, QNLI, Sc-\n",
      "iTail, RTE, RACE, CNN,\n",
      "SQuaD, MRPC, QQP,\n",
      "STS-B, SST2 & CoLA\n",
      "Switch\n",
      "Transformer\n",
      "(Fedus\n",
      "et al., 2021)\n",
      "Language\n",
      "understanding\n",
      "task- Translation,\n",
      "question\n",
      "answering,\n",
      "Classification,\n",
      "and so on.\n",
      "2021\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "C4(Colossal Clean\n",
      "Crawled Corpus)\n",
      "GLUE and SuperGLUE\n",
      "benchmarks, CNNDM, BBC\n",
      "XSum, and SQuAD data sets,\n",
      "ARC Reasoning Challenge,3\n",
      "closed-book question an-\n",
      "swering data sets (Natural\n",
      "Questions, Web Questions,\n",
      "and Trivia QA), Wino-\n",
      "grande Schema Challenge,\n",
      "Adversarial NLI Benchmark\n",
      "T5 (Raffel\n",
      "et al., 2020)\n",
      "Text summariza-\n",
      "tion, Question\n",
      "answering, text\n",
      "classification\n",
      "2020\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "C4 (Colossal Clean\n",
      "Crawled Corpus)\n",
      "GLUE and SuperGLUE\n",
      "benchmarks, CNN/Daily\n",
      "Mail abstractive summa-\n",
      "rization, SQuAD question\n",
      "answering, WMT En-\n",
      "glish to German, French,\n",
      "and Romanian translation\n",
      "InstructGPT\n",
      "(Ouyang\n",
      "et al., 2022)\n",
      "Text Generation,\n",
      "Question\n",
      "Answering,\n",
      "summarization,\n",
      "and so on.\n",
      "2022\n",
      "Decoder\n",
      "Yes\n",
      "Based on the\n",
      "pre-training\n",
      "model GPT-3\n",
      "SFT dataset, RM dataset, PPO\n",
      "dataset, a dataset of prompts\n",
      "and completions Winogender,\n",
      "CrowS-Pairs, Real Toxicity\n",
      "Prompts, TruthfulQA, DROP,\n",
      "QuAC, SquadV2, Hellaswag,\n",
      "SST, RTE and WSC, WMT\n",
      "15 Fr ! En, CNN/Daily Mail\n",
      "Summarization, Reddit TLDR\n",
      "Summarization datasets.\n",
      "Table 5: Transformer models for NLP - question answering task\n",
      "• BERT & BERT variants: BERT is an acronym that stands for Bidirectional Encoder Representations of transformers. It\n",
      "was introduced by the Google AI team and is embedded within the encoder module of the transformer. BERT employs a\n",
      "bidirectional approach, allowing it to pre-train a transformer on unannotated text by considering the context of each word.\n",
      "As a result, BERT has achieved remarkable performance on various natural language processing (NLP) tasks (Devlin et al.,\n",
      "2019).\n",
      "A variety of BERT-based models have been developed with different characteristics. For instance, some are optimized for\n",
      "fast computation, while others produce superior results with a small number of parameters. Some are also tailored to specific\n",
      "tasks, such as RoBERTa, which is designed for masked language modeling and next sentence prediction (Liu et al., 2019).\n",
      "FlueBERT is another model that can be used for tasks such as text classification, paraphrasing, natural language inference,\n",
      "parsing, and word sense disambiguation (Le et al., 2020). Additionally, DistilBERT is suitable for question answering and\n",
      "other specific tasks. These models have significantly improved pre-trained transformer models (Sanh et al., 2019).\n",
      "• GPT & GPT variants: Generative Pre-Trained Transformer (GPT) models are built exclusively on the decoder block\n",
      "of transformers, which significantly improves the progress of transformers in natural language processing. GPT adopts a\n",
      "semi-supervised approach to language comprehension, which involves unsupervised pre-training and supervised fine-tuning\n",
      "methods (Radford et al., 2018). In 2019, following the success of the GPT model, a massively pre-trained transformer-\n",
      "based model called GPT-2 with 1.5 billion parameters was introduced, which significantly improved the pre-trained version\n",
      "of transformers (Radford et al., 2019). Subsequently, in 2020, the largest pre-trained version of GPT with 175 billion\n",
      "parameters, called GPT-3, was released. This model is 10 times larger than the previous non-sparse language model. One\n",
      "of the most notable achievements of GPT-3 is that it exhibits strong performance across a range of tasks without the need\n",
      "for gradient updates or fine-tuning, which is a requirement for pre-training models like BERT (Brown et al., 2020).\n",
      "\n",
      "\n",
      "## Page 19\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "• Electra: An acronym for “Efficiently Learning an Encoder that Classifies Token Replacements Accurately”, utilizes a dis-\n",
      "tinct pre-training method compared to other pre-trained models. Electra deploys a ”Masked Language Modeling” approach\n",
      "that masks certain words and trains the model to predict them. Additionally, Electra incorporates a ”Discriminator” network\n",
      "that aids in comprehending language without the need to memorize the training data. This unique approach enables Electra\n",
      "to generate superior text and surpass the performance of BERT (Clark et al., 2020a).\n",
      "InstructGPT, T5 and Switch Transformer:\n",
      "While the InstructGPT model can generate text apart from question-\n",
      "answering tasks, the T5 is significant in test summarization tasks and Switch transformer models can perform classification\n",
      "and segmentation tasks as well. More descriptions of these models have been provided in Sections 6.1.1 and 6.1.2.\n",
      "6.1.4\n",
      "TEXT SUMMARIZATION\n",
      "Text summarization is a natural language processing task that involves breaking down lengthy texts into shorter versions while\n",
      "retaining essential and valuable information and preserving the meaning of the text. Text summarization is particularly useful\n",
      "in comprehending lengthy textual documents, and it also helps to reduce computational resources and time (Chowdhary &\n",
      "Chowdhary, 2020, Tas & Kiyani, 2007). transformer-based models have shown exceptional performance in text summariza-\n",
      "tion tasks. The transformer-based models in text summarization are listed in Table 6.\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "GPT &\n",
      "variants\n",
      "(Radford\n",
      "et al., 2018;\n",
      "2019, Brown\n",
      "et al., 2020)\n",
      "Text classifica-\n",
      "tion, Question\n",
      "answering,\n",
      "textual entail-\n",
      "ment, semantic\n",
      "similarity\n",
      "2018\n",
      "Decoder\n",
      "Yes\n",
      "Book corpus\n",
      "SNLI, MNLI, QNLI, Sc-\n",
      "iTail, RTE, RACE, CNN,\n",
      "SQuaD, MRPC, QQP,\n",
      "STS-B, SST2 & CoLA\n",
      "PEGASUS\n",
      "(Zhang\n",
      "et al., 2020a)\n",
      "Text sum-\n",
      "marization\n",
      "2020\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "C4, HugeNews\n",
      "XSum, CNN/DailyMail,\n",
      "NEWSROOM, Multi-News,\n",
      "Gigaword, arXiv, PubMed,\n",
      "BIGPATENT, WikiHow,\n",
      "Reddit TIFU, AESLC,BillSum\n",
      "Switch\n",
      "Transformer\n",
      "(Fedus\n",
      "et al., 2021)\n",
      "Language\n",
      "understanding\n",
      "task- Translation,\n",
      "question\n",
      "answering,\n",
      "Classification,\n",
      "and so on.\n",
      "2021\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "C4(Colossal Clean\n",
      "Crawled Corpus)\n",
      "GLUE and SuperGLUE\n",
      "benchmarks, CNNDM, BBC\n",
      "XSum, and SQuAD data sets,\n",
      "ARC Reasoning Challenge,3\n",
      "closed-book question an-\n",
      "swering data sets (Natural\n",
      "Questions, Web Questions,\n",
      "and Trivia QA), Wino-\n",
      "grande Schema Challenge,\n",
      "Adversarial NLI Benchmark\n",
      "T5 (Raffel\n",
      "et al., 2020)\n",
      "Text summariza-\n",
      "tion, Question\n",
      "answering, text\n",
      "classification\n",
      "2020\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "C4 (Colossal Clean\n",
      "Crawled Corpus)\n",
      "GLUE and SuperGLUE\n",
      "benchmarks, CNN/Daily\n",
      "Mail abstractive summa-\n",
      "rization, SQuAD question\n",
      "answering, WMT En-\n",
      "glish to German, French,\n",
      "and Romanian translation\n",
      "InstructGPT\n",
      "(Ouyang\n",
      "et al., 2022)\n",
      "Text Generation,\n",
      "Question\n",
      "Answering,\n",
      "summarization,\n",
      "and so on.\n",
      "2022\n",
      "Decoder\n",
      "Yes\n",
      "Based on the\n",
      "pre-training\n",
      "model GPT-3\n",
      "SFT dataset, RM dataset, PPO\n",
      "dataset, a dataset of prompts\n",
      "and completions Winogender,\n",
      "CrowS-Pairs, Real Toxicity\n",
      "Prompts, TruthfulQA, DROP,\n",
      "QuAC, SquadV2, Hellaswag,\n",
      "SST, RTE, and WSC, WMT\n",
      "15 Fr ! En, CNN/Daily Mail\n",
      "Summarization, Reddit TLDR\n",
      "Summarization datasets.\n",
      "Continued on next page\n",
      "\n",
      "\n",
      "## Page 20\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Table 6 – continued from previous page\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "Table 6: Transformer models for NLP - text summarization task\n",
      "• PEGASUS: It is an exemplary model for generative text summarization that employs both the encoder and decoder modules\n",
      "of the transformer. While models based on masked language modeling only mask a small portion of text, PEGASUS masks\n",
      "entire multiple sentences, selecting the masked sentences based on their significance and importance, and generating them\n",
      "as the output. The model has exhibited significant performance on unknown summarization datasets (Zhang et al., 2020a).\n",
      "• T5: The T5 transformer model, which stands for Text-To-Text Transfer Transformer, introduced a dataset named “Colossal\n",
      "Clean Crawled Corpus (C4)” that improved the performance in various downstream NLP tasks. T5 is a multi-task model\n",
      "that can be trained to perform a range of NLP tasks using the same set of parameters. Following pre-training, the model can\n",
      "be fine-tuned for different tasks and achieves comparable performance to several task-specific models (Raffel et al., 2020).\n",
      "GPT & variants, InstructGPT and Switch Transformer: These models have been discussed in earlier sections. More-\n",
      "over, apart from text summarization, certain models such as GPT and its variants can perform question-answering tasks,\n",
      "InstructGPT can generate text, and Charformer models are capable of classification and segmentation tasks as well.\n",
      "6.1.5\n",
      "TEXT GENERATION\n",
      "The task of text generation has gained immense popularity in the field of NLP due to its usefulness in generating long-form\n",
      "documentation, among other applications. Text generation models attempt to derive meaning from trained text data and\n",
      "create a connection between the text that has been previously outputted. These models typically operate on the basis of\n",
      "this connection (Chowdhary & Chowdhary, 2020, Reiter & Dale, 1997). The use of transformer-based models has led to\n",
      "significant advancements in the task of text generation. Please refer to Table 7.\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "CTRL\n",
      "(Keskar\n",
      "et al., 2019)\n",
      "Text Generation\n",
      "2019\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "Project Gutenberg,\n",
      "subreddits, News\n",
      "Data, Amazon\n",
      "Review, open\n",
      "WebText, WMT\n",
      "Translation date,\n",
      "question-answer\n",
      "pairs, MRQA\n",
      "Multilingual Wikipedia\n",
      "and Open WebText.\n",
      "BART (Lewis\n",
      "et al., 2020)\n",
      "Language\n",
      "Translation,\n",
      "Sentence\n",
      "Reconstruction,\n",
      "Comprehension,\n",
      "text Generation\n",
      "2019\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "Corrupting docu-\n",
      "ments, 1M steps\n",
      "on a combina-\n",
      "tion of books and\n",
      "Wikipedia data,\n",
      "news, stories, and\n",
      "web text (Training)\n",
      "SQuAD, MNLI, ELI5,\n",
      "XSum, ConvAI2, CNN/DM,\n",
      "CNN/DailyMail, WMT16\n",
      "Romanian-English, augmented\n",
      "with back-translation data\n",
      "from Sennrich et al. (2016).\n",
      "ProphetNET\n",
      "(Qi et al.,\n",
      "2020)\n",
      "Text Prediction\n",
      "2020\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "Bookcorpus, English\n",
      "Wikipedia news,\n",
      "books, stories,\n",
      "and web text\n",
      "CNN/dailymail, Giga-word\n",
      "Corpus, SQuAD dataset.\n",
      "Continued on next page\n",
      "\n",
      "\n",
      "## Page 21\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Table 7 – continued from previous page\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "InstructGPT\n",
      "(Ouyang\n",
      "et al., 2022)\n",
      "Text Generation,\n",
      "Question\n",
      "Answering,\n",
      "summarization\n",
      "and so on.\n",
      "2022\n",
      "Decoder\n",
      "Yes\n",
      "Based on the\n",
      "pre-training\n",
      "model GPT-3\n",
      "SFT dataset, RM dataset, PPO\n",
      "dataset, dataset of prompts\n",
      "and completions Winogender,\n",
      "CrowS-Pairs, Real Toxicity\n",
      "Prompts , TruthfulQA , DROP\n",
      ", QuAC , SquadV2 , Hellaswag\n",
      ", SST , RTE and WSC, WMT\n",
      "15 Fr ! En, CNN/Daily Mail\n",
      "Summarization, Reddit TLDR\n",
      "Summarization datasets.\n",
      "Table 7: Transformer models for NLP - text generation task\n",
      "• CTRL: The acronym CTRL denotes the Conditional transformer Language model, which excels in generating realistic text\n",
      "resembling human language, contingent on a given condition. In addition, CTRL can produce text in multiple languages.\n",
      "This model is large-scale, boasting 1.63 billion parameters, and can be fine-tuned for various generative tasks, such as\n",
      "question answering and text summarization (Keskar et al., 2019).\n",
      "• ProphetNET: ProphetNET is a sequence-to-sequence model that utilizes future n-gram prediction to facilitate text gener-\n",
      "ation by predicting n-grams ahead. The model adheres to the transformer architecture, comprising encoder and decoder\n",
      "modules. It distinguishes itself by employing an n-stream self-attention mechanism. ProphetNET demonstrates remarkable\n",
      "performance in summarization and is also competent in question generation tasks (Qi et al., 2020).\n",
      "• InstructGPT: It was proposed as a solution to the problem of language generative models failing to produce realistic and\n",
      "truthful results. It achieves this by incorporating human feedback during fine-tuning and reinforcement learning from the\n",
      "feedback. The GPT-3 model was fine-tuned for this purpose. As a result, InstructGPT can generate more realistic and\n",
      "natural output that is useful in real-life applications. ChatGPT, which follows a similar methodology as InstructGPT, has\n",
      "gained significant attention in the field of NLP at the end of 2022 (Ouyang et al., 2022).\n",
      "BART: The BART model’s description is mentioned above in the language translation section. This model can execute the\n",
      "language translation task as well.\n",
      "6.1.6\n",
      "NATURAL LANGUAGE REASONING\n",
      "The pursuit of natural language reasoning is a field of study that is distinct from that of question-answering. Question-\n",
      "answering focuses on finding the answer to a specific query within a given text passage. On the other hand, natural language\n",
      "reasoning involves the application of deductive reasoning to derive a conclusion from the given premises and rules that are\n",
      "represented in natural language. Neural network architectures aim to learn how to utilize these premises and rules to infer\n",
      "new conclusions. Previously, a similar task was traditionally tackled by systems equipped with the knowledge represented\n",
      "in a formal format and rules to be applied for the derivation of new knowledge. However, the use of formal representation\n",
      "has posed a significant challenge to this line of research (Mark A Musen, 1988). With the advent of transformers and\n",
      "their remarkable performance in numerous NLP tasks, it is now possible to circumvent formal representation and allow\n",
      "transformers to engage in reasoning directly using natural language. Table 8 highlights some of the significant transformer\n",
      "models for natural language reasoning tasks.\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-\n",
      "tuning, Train-\n",
      "ing, Testing)\n",
      "(Clark et al., 2020)\n",
      "(Clark et al., 2020b)\n",
      "Binary Clas-\n",
      "sification\n",
      "2020\n",
      "RoBERTa\n",
      "(Encoder)\n",
      "Yes\n",
      "RACE\n",
      "RuleTaker\n",
      "(Richardson et al.,\n",
      "2022) (Richardson\n",
      "& Sabharwal, 2022)\n",
      "Binary Clas-\n",
      "sification\n",
      "2022\n",
      "RoBERTa\n",
      "Large (Encoder)\n",
      "Yes\n",
      "RACE\n",
      "Hard-RuleTaker\n",
      "(Saha et al., 2020)\n",
      "(Saha et al., 2020)\n",
      "Binary Classifi-\n",
      "cation, Sequence\n",
      "Generation\n",
      "2020\n",
      "PRover\n",
      "[RoBERTa-\n",
      "based](Encoder)\n",
      "No\n",
      "NA\n",
      "RuleTaker\n",
      "(Sinha et al., 2019)\n",
      "(Sinha et al., 2019)\n",
      "Sequence\n",
      "Generation\n",
      "2019\n",
      "BERT (Encoder)\n",
      "No\n",
      "NA\n",
      "CLUTRR\n",
      "\n",
      "\n",
      "## Page 22\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Table 8 – continued from previous page\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-\n",
      "tuning, Train-\n",
      "ing, Testing)\n",
      "(Picco et al., 2021)\n",
      "(Picco et al., 2021)\n",
      "Binary Clas-\n",
      "sification\n",
      "2021\n",
      "BERT-Based\n",
      "(Encoder)\n",
      "Yes\n",
      "RACE\n",
      "RuleTaker\n",
      "Table 8: Transformer models for natural language reasoning\n",
      "• RoBERTa:\n",
      "In a 2020 study by Clark et al. (2020b), a binary classification task was assigned to the transformer, which\n",
      "aimed to determine whether a given statement can be inferred from a provided set of premises and rules represented in\n",
      "natural language. The architecture utilized for the transformer was RoBERTa-large, which was pre-trained on a dataset\n",
      "of high school exam questions that required reasoning skills. This pre-training enabled the transformer to achieve a high\n",
      "accuracy of 98% on the test dataset. The dataset contained theories that were randomly sampled and constructed using sets\n",
      "of names and attributes. The task required the transformer to classify whether the given statement (Statement) followed\n",
      "from the provided premises and rules (Context) (Clark et al., 2020b)\n",
      "• RoBERTa-Large:\n",
      "In the work by Richardson & Sabharwal (2022), the authors aimed to address a limitation of the\n",
      "dataset construction approach presented in the work by Clark et al. (2020b). They highlighted that the uniform random\n",
      "sampling of theories, as done in (Clark et al., 2020b), does not always result in challenging instances. To overcome this\n",
      "limitation, they proposed a novel methodology for creating more challenging algorithmic reasoning datasets. The key\n",
      "idea of their methodology is to sample hard instances from ordinary SAT propositional formulas and translate them into\n",
      "natural language using a predefined set of English rule languages. By following this approach, they were able to construct\n",
      "a more challenging dataset that is consequential for training robust models and for reliable evaluation. To demonstrate the\n",
      "effectiveness of their approach, the authors conducted experiments where they tested the models trained using the dataset\n",
      "from (Clark et al., 2020b) on their newly constructed dataset. The results showed that the models achieved an accuracy of\n",
      "57.7% and 59.6% for T5 and RoBERTa, respectively. These findings highlight that models trained on easy datasets may not\n",
      "be capable of solving challenging instances of the problem.\n",
      "• PRover: In a related study, Saha et al. (2020) proposed a model called PRover, which is an interpretable joint transformer\n",
      "capable of generating a corresponding proof with an accuracy of 87%. The task addressed by PRover is the same as that\n",
      "in the study by Clark et al. (2020b) and Richardson & Sabharwal (2022), where the aim is to determine whether a given\n",
      "conclusion follows from the provided premises and rules. The proof generated by PRover is represented as a directed graph,\n",
      "where the nodes represent statements and rules, and the edges indicate which new statements follow from applying rules\n",
      "on the previous statements. Overall, the proposed approach by Saha et al. (2020) provides a promising direction towards\n",
      "achieving interpretable and accurate reasoning models.\n",
      "• BERT-based: In (Picco et al., 2021), a BERT-based architecture called “neural unifier” was proposed to improve the\n",
      "generalization performance of the model on the RuleTaker dataset. The authors aimed to mimic some elements of the\n",
      "backward-chaining reasoning procedure to enhance the model’s ability to handle queries that require multiple steps to\n",
      "answer, even when trained on shallow queries only. The neural unifier consists of two standard BERT transformers, namely\n",
      "the fact-checking unit and the unification unit. The fact-checking unit is trained to classify whether a query of depth 0,\n",
      "represented by the embedding vector q-0, follows from a given knowledge base represented by the embedding vector C.\n",
      "The unification unit takes as input the embedding vector q-n of a depth-n query and the embedding vector of the knowledge\n",
      "base, vector C, and tries to predict an embedding vector q0, thereby performing backward-chaining.\n",
      "• BERT: Sinha et al. (2019) introduced a dataset named CLUTRR, which differs from the previously discussed studies in\n",
      "that the rules are not given in the input to be used to infer conclusions. Instead, the BERT transformer model is tasked with\n",
      "both extracting relationships between entities and inferring the rules governing these relationships. For instance, given a\n",
      "knowledge base consisting of statements such as “Alice is Bob’s mother” and “Jim is Alice’s father”, the network can infer\n",
      "that “Jim is Bob’s grandfather”.\n",
      "6.1.7\n",
      "AUTOMATED SYMBOLIC REASONING\n",
      "Automated symbolic reasoning is a subfield of computer science that deals with solving logical problems such as SAT solving\n",
      "and automated theorem proving. These problems are traditionally addressed using search techniques with heuristics. How-\n",
      "ever, recent studies have explored the use of learning-based techniques to improve the efficiency and effectiveness of these\n",
      "methods. One approach is to learn the selection of efficient heuristics used by traditional algorithms. Alternatively, an end-to-\n",
      "end learning-based solution can be employed for the problem. Both approaches have shown promising results and offer the\n",
      "potential for further advancements in automated symbolic reasoning (Kurin et al., 2020, Selsam et al., 2019). In this regard, a\n",
      "number of transformer based models have shown significant performance in automated symbolic reasoning tasks. For those\n",
      "models, please look at Table 9.\n",
      "\n",
      "\n",
      "## Page 23\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-\n",
      "tuning, Train-\n",
      "ing, Testing)\n",
      "(Shi et al., 2022)\n",
      "(Shi et al., 2022b)\n",
      "Binary Clas-\n",
      "sification\n",
      "2022\n",
      "SATFormer\n",
      "(Encoder /\n",
      "Decoder)\n",
      "No\n",
      "NA\n",
      "Synthetic\n",
      "(Shi et al., 2021)\n",
      "(Shi et al., 2021)\n",
      "Binary Clas-\n",
      "sification\n",
      "2021\n",
      "TRSAT (Encoder\n",
      "/ Decoder)\n",
      "No\n",
      "NA\n",
      "Synthetic, SATLIB\n",
      "(Hahn et al., 2021)\n",
      "(Hahn et al., 2021)\n",
      "Sequence\n",
      "Generation\n",
      "2021\n",
      "Transformer\n",
      "(Encoder /\n",
      "Decoder)\n",
      "No\n",
      "NA\n",
      "Synthetic\n",
      "(Polu et al,\n",
      "2020) (Polu &\n",
      "Sutskever, 2020)\n",
      "Sequence\n",
      "Generation\n",
      "2020\n",
      "GPT-f (Decoder)\n",
      "Yes\n",
      "CommonCrawl,\n",
      "Github,\n",
      "arXiv,\n",
      "WebMath\n",
      "set.mm\n",
      "Table 9: Transformer models for automated symbolic reasoning\n",
      "• SATformer: The SAT-solving problem for boolean formulas was addressed by Shi et al. in 2022 (Shi et al., 2021) through\n",
      "the introduction of SATformer, a hierarchical transformer architecture that offers an end-to-end learning-based solution\n",
      "for solving the problem. Traditionally, in the context of SAT-solving, a boolean formula is transformed into its conjunctive\n",
      "normal form (CNF), which serves as an input for the SAT solver. The CNF formula is a conjunction of boolean variables and\n",
      "their negations, known as literals, organized into clauses where each clause is a disjunction of these literals. For example,\n",
      "a CNF formula utilizing boolean variables would be represented as (A OR B) AND (NOT A OR C), where each clause (A\n",
      "OR B) and (NOT A OR C) is made up of literals.\n",
      "The authors employ a graph neural network (GNN) to obtain the embeddings of the clauses in the CNF formula. SATformer\n",
      "then operates on these clause embeddings to capture the interdependencies among clauses, with the self-attention weight\n",
      "being trained to be high when groups of clauses that could potentially lead to an unsatisfiable formula are attended together,\n",
      "and low otherwise. Through this approach, SATformer efficiently learns the correlations between clauses, resulting in\n",
      "improved SAT prediction capabilities (Shi et al., 2022b).\n",
      "• TRSAT: Another research endeavor conducted by Shi et al. in 2021 investigated a variant of the boolean SAT problem\n",
      "known as MaxSAT and introduced a transformer model named TRSAT, which serves as an end-to-end learning-based SAT\n",
      "solver (Shi et al., 2021). A comparable problem to the boolean SAT is the satisfiability of a linear temporal formula (Pnueli,\n",
      "1977), where a satisfying symbolic trace to the formula is sought after given a linear temporal formula.\n",
      "• Transformer: In a study conducted by Hahn et al. (2021), the authors addressed the boolean SAT problem and the temporal\n",
      "satisfiability problem, both of which are more complex than binary classification tasks that were tackled in previous studies.\n",
      "In these problems, the task is to generate a satisfying sequence assignment for a given formula, rather than simply classify-\n",
      "ing whether the formula is satisfied or not. The authors constructed their datasets by using classical solvers to generate linear\n",
      "temporal formulas with their corresponding satisfying symbolic traces, and boolean formulas with their corresponding sat-\n",
      "isfying partial assignments. The authors employed a standard transformer architecture to solve the sequence-to-sequence\n",
      "task. The Transformer was able to generate satisfying traces, some of which were not observed during training, demon-\n",
      "strating its capability to solve the problem and not merely mimic the behavior of the classical solvers used in the dataset\n",
      "generation process.\n",
      "• GPT-f: In their work, Polu & Sutskever (2020) presented GPT-F, an automated prover and proof assistant that utilizes a\n",
      "decoder-only transformers architecture similar to GPT-2 and GPT-3. GPT-F was trained on a dataset called set.mm, which\n",
      "contains approximately 38,000 proofs. The largest model investigated by the authors consists of 36 layers and 774 million\n",
      "trainable parameters. This deep learning network has generated novel proofs that have been accepted and incorporated into\n",
      "mathematical proof libraries and communities.\n",
      "6.2\n",
      "COMPUTER VISION\n",
      "Motivated by the success of transformers in natural language processing, researchers have explored the application of the\n",
      "transformer concept in computer vision tasks. Traditionally, convolutional neural networks (CNNs) have been considered\n",
      "the fundamental component for processing visual data. However, different types of images require different processing\n",
      "techniques, with natural images and medical images being two prime examples. Furthermore, research in computer vision for\n",
      "natural images and medical images is vast and distinct. As a result, transformer models for computer vision can be broadly\n",
      "classified into two categories: (i) those designed for natural image processing, and (ii) those designed for medical image\n",
      "processing.\n",
      "\n",
      "\n",
      "## Page 24\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "6.2.1\n",
      "NATURAL IMAGE PROCESSING\n",
      "In the domain of computer vision, natural image processing is a primary focus as compared to medical image processing,\n",
      "owing to the greater availability of natural image data. Furthermore, computer vision with natural images has wide-ranging\n",
      "applications in various domains. Among the numerous tasks associated with computer vision and natural images, we have\n",
      "identified four of the most common and popular tasks: (i) classification and segmentation, (ii) recognition and feature extrac-\n",
      "tion, (iii) mask modeling prediction, and (iv) image generation. In this context, we have provided a comprehensive discussion\n",
      "of each of these computer vision tasks with natural images. Additionally, we have presented a table that provides crucial\n",
      "information about each transformer-based model and have highlighted their working methods and significance.\n",
      "6.2.2\n",
      "IMAGE CLASSIFICATION\n",
      "Image classification is a crucial and popular task in the field of computer vision, which aims to analyze and categorize images\n",
      "based on their features, type, genre, or objects. This task is considered as a primary stage for many other image processing\n",
      "tasks. For example, if we have a set of images of different animals, we can classify them into different animal categories such\n",
      "as cat, dog, horse, etc., based on their characteristics and features (Szummer & Picard, 1998, Lu & Weng, 2007). Due to its\n",
      "significance, many transformer-based models have been developed to address image classification tasks. Table 10 highlights\n",
      "some of the significant transformer models for image classification tasks and discusses their important features and working\n",
      "methodologies.\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "VIT (Doso-\n",
      "vitskiy\n",
      "et al., 2021)\n",
      "Image classifi-\n",
      "cation, image\n",
      "recognition\n",
      "2021\n",
      "Encoder\n",
      "Yes\n",
      "JFT-300M,\n",
      "ILSVRC-2012\n",
      "ImageNet,\n",
      "ImageNet-21k\n",
      "ImageNet-RL, CIFAR-\n",
      "10/100, Oxford Flowers-102,\n",
      "Oxford-IIIT Pets, VTAB\n",
      "ViT Variants\n",
      "(d’Ascoli\n",
      "et al., 2021,\n",
      "Ahmed\n",
      "et al., 2021,\n",
      "Touvron\n",
      "et al., 2021,\n",
      "Arnab\n",
      "et al., 2021)\n",
      "Image clas-\n",
      "sification\n",
      "2020-\n",
      "2021\n",
      "Encoder\n",
      "Yes\n",
      "ConViT: ImageNet\n",
      "(Based on DeiT)\n",
      "SiT: STL10,\n",
      "CUB200, CIFAR10,\n",
      "CIFAR100,\n",
      "ImageNet-1K,\n",
      "Pascal VOC,\n",
      "MS-COCO, Visual-\n",
      "Genome DEIT:\n",
      "ImageNet. ViViT:\n",
      "ImageNet, JFT\n",
      "ConViT: ImageNet,CIFAR100\n",
      "SiT: CIFAR-10,CIFAR-100 ,\n",
      "STL-10, CUB200, ImageNet-\n",
      "1K, Pascal VOC, MS-COCO,\n",
      "Visual-Genome. DEIT:\n",
      "ImageNet, iNaturalist 2018,\n",
      "iNaturalist 2019, Flowers-102,\n",
      "Stanford Cars, CIFAR-100,\n",
      "CIFAR-10. ViViT: Larger JFT,\n",
      "Kinetics, Epic Kitchens-100,\n",
      "Moments in Time, SSv2.\n",
      "BEIT (Bao\n",
      "et al., 2022)\n",
      "Image clas-\n",
      "sification &\n",
      "segmentation\n",
      "2021\n",
      "Encoder\n",
      "Yes\n",
      "ImageNet-1K,\n",
      "ImageNet-22k\n",
      "ILSVRC-2012 ImageNet,\n",
      "ADE20K, CIFAR-100\n",
      "IBOT (Zhou\n",
      "et al., 2021b)\n",
      "Image clas-\n",
      "sification,\n",
      "segmentation,\n",
      "object detection\n",
      "& recognition\n",
      "2022\n",
      "Encoder\n",
      "Yes\n",
      "ImageNet-1K, ViT-\n",
      "L/16, ImageNet-22K\n",
      "COCO, ADE20K\n",
      "Conformer\n",
      "(Peng et al.,\n",
      "2021)\n",
      "Image recog-\n",
      "nition & object\n",
      "detection,\n",
      "Classification\n",
      "2021\n",
      "Encoder\n",
      "Not\n",
      "mentioned\n",
      "in paper\n",
      "N/A\n",
      "LibriSpeech\n",
      "Table 10: Transformer models for natural image processing - image classification\n",
      "• ViT Variants: There are several ViT-based models that have been developed for specific tasks. For instance, ConViT is\n",
      "an improved version of ViT that combines CNN and transformer by adding an inductive bias to ViT, resulting in better\n",
      "accuracy for image classification tasks (d’Ascoli et al., 2021). Self-supervised Vision transformer (SiT) allows for the use\n",
      "of the architecture as an autoencoder and seamlessly works with multiple self-supervised tasks (Ahmed et al., 2021). Data\n",
      "Efficient Image Transformer (DeiT) is a type of vision transformer designed for image classification tasks that require less\n",
      "data to be trained (Touvron et al., 2021). There are numerous ViT variants available with certain improvements or designed\n",
      "for specific tasks. For example, Video Vision Transformer (ViViT) is a ViT-based model that classifies videos using both\n",
      "encoder and decoder modules of the transformer, whereas most ViT and ViT-variant models use only the encoder module\n",
      "(Arnab et al., 2021).\n",
      "\n",
      "\n",
      "## Page 25\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "• BEIT: Bidirectional Encoder Representation from Image Transformers (BEIT) (Bao et al., 2022) is a transformer-based\n",
      "model that draws inspiration from BERT and introduces a new pre-training task called Masked Image Modeling (MIM) for\n",
      "vision Transformers. In MIM, a portion of the image is randomly masked, and the corrupted image is passed through the ar-\n",
      "chitecture, which then recovers the original image tokens. BEIT has shown competitive performance on image classification\n",
      "and segmentation tasks, demonstrating its effectiveness for a variety of computer vision applications.\n",
      "• Conformer: In the field of computer vision, Conformer (Peng et al., 2021) is a model that works similarly to the CMT\n",
      "model. While CNN is responsible for capturing the local features of the image, Transformer works for the global context\n",
      "and long-range dependencies of images. However, the Conformer model proposes a new method called cross-attention,\n",
      "which combines both local and global features to focus on various parts of the image based on the task. The model has\n",
      "shown promising results for classification and object detection/recognition tasks.\n",
      "• IBOT: IBOT represents Image BERT Pre-training with Online Tokennizer which is a self-supervised model. This model\n",
      "studied masked image modeling using an online tokenizer and it learns to distill features using a tokenizer. This online\n",
      "tokenizer helps this model to improve the feature representation capability. Besides, the image classification task, this\n",
      "model shows significant performance in object detection and segmentation tasks.\n",
      "ViT The ViT model, which has been discussed in detail in the Recognition and Object Detection section, is also capable of\n",
      "performing recognition and object detection tasks.\n",
      "6.2.3\n",
      "IMAGE RECOGNITION & OBJECT DETECTION\n",
      "Image recognition & Object detection is often considered as nearly similar and related task in computer vision. It is the\n",
      "capability of detecting or recognizing any object, person, or feature in an image or video. An image or video contains a\n",
      "number of objects & features; by extracting the features from the image, a model tries to capture the features of an object\n",
      "through training. By understanding these useful features, a model can recognize the specific object from the other available\n",
      "object in the image or video (Zhao et al., 2019, Jiao et al., 2019, H´enaff, 2020, Chen et al., 2019a). Here we highlight and\n",
      "discuss the significant transformer models for image/object recognition tasks (see Table 11).\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "VIT (Doso-\n",
      "vitskiy\n",
      "et al., 2021)\n",
      "Image classifi-\n",
      "cation, image\n",
      "recognition\n",
      "2021\n",
      "Encoder\n",
      "Yes\n",
      "JFT-300M,\n",
      "ILSVRC-2012\n",
      "ImageNet,\n",
      "ImageNet-21k\n",
      "ImageNet-RL, CIFAR-\n",
      "10/100, Oxford Flowers-102,\n",
      "Oxford-IIIT Pets, VTAB\n",
      "Conformer\n",
      "(Peng et al.,\n",
      "2021)\n",
      "Image recog-\n",
      "nition & object\n",
      "detection,\n",
      "classification\n",
      "2021\n",
      "Encoder\n",
      "Not\n",
      "mentioned\n",
      "in paper\n",
      "N/A\n",
      "LibriSpeech\n",
      "LoFTR (Sun\n",
      "et al., 2021a)\n",
      "Image feature\n",
      "matching\n",
      "& visual\n",
      "localization\n",
      "2021\n",
      "Encoder &\n",
      "Decoder\n",
      "No\n",
      "NA\n",
      "MegaDepth, ScanNet\n",
      "HPatches, ScanNet,\n",
      "MegaDepth, VisLoc\n",
      "benchmark (the Aachen-\n",
      "Day-Night, InLoc)\n",
      "CMT (Guo\n",
      "et al., 2022a)\n",
      "Image recogni-\n",
      "tion, detection\n",
      "& segmentation\n",
      "2022\n",
      "Encoder\n",
      "No\n",
      "NA\n",
      "ImageNet, CIFAR10,\n",
      "CIFAR100, Flowers, Stand-\n",
      "ford cars, Oxford-IIIT\n",
      "pets, COCO val2017\n",
      "Transformer\n",
      "in\n",
      "Transformer-\n",
      "TNT (Han\n",
      "et al., 2021)\n",
      "Image\n",
      "recognition\n",
      "2021\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "ImageNet\n",
      "ILSVRC 2012\n",
      "COCO2017, ADE20K,\n",
      "Oxford 102 Flowers, Oxford-\n",
      "IIIT Pets, iNaturalist 2019,\n",
      "CIFAR-10, CIFAR-100\n",
      "SWIN (Liu\n",
      "et al., 2021)\n",
      "Object detection\n",
      "and segmentation\n",
      "2021\n",
      "Encoder\n",
      "Yes\n",
      "ImageNet-22k\n",
      "ImageNet-1k, COCO\n",
      "2017, ADE20K\n",
      "DETR\n",
      "(Carion\n",
      "et al., 2020)\n",
      "Object detection\n",
      "& prediction\n",
      "2020\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "ImageNet pretrained\n",
      "backbone ResNet-50\n",
      "COCO 2017, panoptic\n",
      "segmentation datasets\n",
      "Continued on next page\n",
      "\n",
      "\n",
      "## Page 26\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Table 11 – continued from previous page\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset(Fine-tuning,\n",
      "Training, Testing)\n",
      "HOTR (Kim\n",
      "et al., 2021a)\n",
      "Human-object\n",
      "interaction\n",
      "detection\n",
      "2021\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "MS-COCO\n",
      "V-COCO HICO-DET\n",
      "Table 11: Transformer models for natural image processing - image recognition & object detection\n",
      "• ViT: The ViT (Vision Transformer) is one of the earliest transformer-based models that has been applied to computer\n",
      "vision. ViT views an image as a sequence of patches and processes it using only the encoder module of the Transformer.\n",
      "ViT performs very well for classification tasks and can also be applied to image recognition tasks. It demonstrates that a\n",
      "transformer-based model can serve as an alternative to convolutional neural networks (Dosovitskiy et al., 2021).\n",
      "• TNT: Transformer in Transformers (TNT) is a transformer-based computer vision model that uses a transformer model\n",
      "inside another transformer model to capture features inside local patches of an image (Han et al., 2021). The image is\n",
      "divided into local patches, which are further divided into smaller patches to capture more detailed information through\n",
      "attention mechanisms. TNT shows promising results in visual recognition tasks and offers an alternative to convolutional\n",
      "neural networks for computer vision tasks.\n",
      "• LoFTR: LoFTR, which stands for Local Feature Matching with Transformer, is a computer vision model that is capable of\n",
      "learning feature representations directly from raw images, as opposed to relying on hand-crafted feature detectors for feature\n",
      "matching. This model employs both the encoder and decoder modules of the transformer. The encoder takes features from\n",
      "the image, while the decoder works to create a feature map. By leveraging the transformer’s ability to capture global context\n",
      "and long-range dependencies, LoFTR can achieve high performance in visual recognition tasks.\n",
      "• DETR: The Detection Transformer (DETR) represents a new approach to object detection or recognition, which performs\n",
      "the object detection task as a direct set of prediction problems (Carion et al., 2020). In contrast, other models accomplish this\n",
      "task in two stages. DETR uses an encoder to generate object queries, a self-attention mechanism to capture the relationship\n",
      "between the queries and objects in the image, and creates an object detection scheme. This model has been shown to be\n",
      "effective for object detection and recognition tasks and represents a significant advancement in the field.\n",
      "• HOTR: The HOTR model, which stands for Human-Object Interaction Transformer, is a Transformer-based model\n",
      "designed for predicting Human-Object Interaction.\n",
      "It is the first Transformer-based Human-Object Interaction (HOI)\n",
      "detection prediction model that employs both the encoder and decoder modules of the Transformer. Unlike conventional\n",
      "hand-crafted post-processing schemes, HOTR uses a prediction set to extract the semantic relationship of the image,\n",
      "making it one of the fastest human-object interaction detection models available (Kim et al., 2021a).\n",
      "CMT, Conformer & SWIN Transformer The CMT and SWIN Transformer model have already been described in the\n",
      "Image Segmentation and Image Classification sections, respectively. Both of these models are also capable of performing\n",
      "the task of Image Segmentation. Additionally, the Conformer model was described in the Image Classification section.\n",
      "6.2.4\n",
      "IMAGE SEGMENTATION\n",
      "Segmentation is the process of partitioning an image based on objects and creating boundaries between them, requiring pixel-\n",
      "level information extraction. There are two popular types of image segmentation tasks in computer vision: (i) Semantic\n",
      "Segmentation, which aims to identify and color similar objects belonging to the same class among all other objects in an\n",
      "image, and (ii) Instance Segmentation, which aims to detect instances of objects and their boundaries (Minaee et al., 2022,\n",
      "Haralick & Shapiro, 1985). In this section, we will discuss some Transformer-based models that have shown exceptional\n",
      "performance in image segmentation tasks (refer to Table 12 for more details).\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "SWIN (Liu\n",
      "et al., 2021)\n",
      "Object detection\n",
      "and segmentation\n",
      "2021\n",
      "Encoder\n",
      "Yes\n",
      "ImageNet-22k\n",
      "ImageNet-1k, COCO\n",
      "2017, ADE20K\n",
      "CMT (Guo\n",
      "et al., 2022a)\n",
      "Image recogni-\n",
      "tion, detection\n",
      "& segmentation\n",
      "2022\n",
      "Encoder\n",
      "No\n",
      "NA\n",
      "ImageNet, CIFAR10,\n",
      "CIFAR100, Flowers, Stand-\n",
      "ford cars, Oxford-IIIT\n",
      "pets, COCO val2017\n",
      "Continued on next page\n",
      "\n",
      "\n",
      "## Page 27\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Table 12 – continued from previous page\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset(Fine-tuning,\n",
      "Training, Testing)\n",
      "SETR (Zheng\n",
      "et al., 2021)\n",
      "Image seg-\n",
      "mentation\n",
      "2020\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "ImageNet-1k,\n",
      "pre-trained weights\n",
      "provided by\n",
      "ViT or DeiT\n",
      "ADE20K, Pascal\n",
      "Context, CityScapes\n",
      "IBOT (Zhou\n",
      "et al., 2021b)\n",
      "Image clas-\n",
      "sification,\n",
      "segmentation,\n",
      "object detection\n",
      "& recognition\n",
      "2022\n",
      "Encoder\n",
      "Yes\n",
      "ImageNet-1K, ViT-\n",
      "L/16 ImageNet-22K\n",
      "COCO, ADE20K\n",
      "Table 12: Transformer models for natural image processing - image segmentation\n",
      "• SWIN Transformer: The SWIN Transformer (Liu et al., 2021), short for Scaled WINdowed Transformer, is a transformer-\n",
      "based model that is capable of handling large images by dividing them into small patches, or windows, and processing\n",
      "them through its architecture. By using shifted windows, the model requires a smaller number of parameters and less\n",
      "computational power, making it useful for real-life image applications. SWIN Transformer can perform image classification,\n",
      "segmentation, and object detection tasks with exceptional accuracy and efficiency (Zidan et al., 2023, Yang & Yang, 2023).\n",
      "• CMT: CNNs Meet Transformer is a model that combines both Convolutional Neural Networks (CNN) and Vision Trans-\n",
      "former (ViT). CNNs are better suited to capturing local features, while Transformers excel at capturing global context.\n",
      "CMT takes advantage of the strengths of both these models and performs well in image classification tasks as well as object\n",
      "detection and recognition tasks. The integration of CNN and Transformer allows CMT to handle both spatial and sequential\n",
      "data effectively, making it a powerful tool for computer vision tasks (Guo et al., 2022a).\n",
      "• SETR: SETR stands for SEgmentation TRansformer, which is a transformer-based model used for image segmentation\n",
      "tasks. It uses sequence-to-sequence prediction methods and removes the dependency of fully convolutional network with\n",
      "vanilla Transformer architecture. Before feeding the image into the Transformer architecture, it divides the image into a\n",
      "sequence of patches and the flattened pixel of each patch. There are three variants of SETR models available with different\n",
      "model sizes and performance levels (Zheng et al., 2021).\n",
      "IBOT The IBOT model, described above in the Image Classification section, is also capable of performing the Image\n",
      "Classification task.\n",
      "6.2.5\n",
      "IMAGE GENERATION\n",
      "Image generation is a challenging task in computer vision, and transformer-based models have shown promising results in\n",
      "this area due to their parallel computational capability. This task involves generating new images using existing image pixels\n",
      "as input. It can be used for object reconstruction and data augmentation (van den Oord et al., 2016, Liu et al., 2017). While\n",
      "several text-to-image generation models exist, we focus on image generation models that use image pixels without any other\n",
      "type of data. In Table 13, we discuss some transformer-based models that have demonstrated exceptional performance in\n",
      "image generation tasks.\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "Image Transformer\n",
      "(Parmar et al., 2018)\n",
      "Image Gen-\n",
      "eration\n",
      "2018\n",
      "Encoder &\n",
      "Decoder\n",
      "Not\n",
      "mentioned\n",
      "N/A\n",
      "ImageNet, CIFAR-10, CelebA\n",
      "I-GPT (Chen\n",
      "et al., 2020b)\n",
      "Image Gen-\n",
      "eration\n",
      "2020\n",
      "Decoder\n",
      "Yes\n",
      "BooksCorpus\n",
      "dataset,\n",
      "1B Word\n",
      "Benchmark\n",
      "SNLI, MultiNLI, Question\n",
      "NLI, RTE, SciTail, RACE,\n",
      "Story Cloze, MSR Paraphrase\n",
      "Corpus, Quora Question Pairs,\n",
      "STS Benchmark, Stanford\n",
      "Sentiment Treebank-2, CoLA\n",
      "VideoGPT (Yan\n",
      "et al., 2021)\n",
      "Video Generation\n",
      "2021\n",
      "Decoder\n",
      "No\n",
      "NA\n",
      "BAIR RobotNet, Mov-\n",
      "ing MNIST, ViZDoom,\n",
      "UCF-101, Tumblr GIF\n",
      "Table 13: Transformer models for natural image processing - image generation\n",
      "\n",
      "\n",
      "## Page 28\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "• Image Transformer: Image Transformer is an autoregressive sequence generative model that uses the self-attention mech-\n",
      "anism for image generation. This model generates new pixels and increases the size of the image by utilizing the attention\n",
      "mechanism on local pixels. It uses both the encoder and decoder module of the transformer, but does not use masking in the\n",
      "encoder. The encoder layer is used less than the decoder for better performance on image generation. Image Transformer is\n",
      "a remarkable model in the field of image generation (Parmar et al., 2018).\n",
      "• I-GPT: I-GPT or Image GPT is an image generative model that utilizes the GPT-2 model for training to auto-regressively\n",
      "predict pixels by learning image representation, without using the 2D image. BERT motifs can also be used during pre-\n",
      "training. I-GPT has four variants based on the number of parameters: IGPT-S (76M parameters), IGPT-M (455M pa-\n",
      "rameters), IGPT-L (1.4B parameters), and IGPT-XL (6.8M parameters), where models with higher parameters have more\n",
      "validation losses (Chen et al., 2020b).\n",
      "• VideoGPT: VideoGPT is a generative model that combines two classes of architecture: likelihood-based models and VAE\n",
      "(Vector Quantized Variational Autoencoder). The aim of this combination is to create a model that is easy to maintain and\n",
      "use, as well as resource-efficient, while also being able to encode spatio-temporal correlations in video frames. VideoGPT\n",
      "has shown remarkable results compared to other models, particularly in tests conducted on the “BAIR Robot Pushing”\n",
      "dataset (Yan et al., 2021).\n",
      "6.3\n",
      "MEDICAL IMAGE PROCESSING\n",
      "The diagnosis of pathologies based on medical images is often criticized as complicated, time-consuming, error-prone, and\n",
      "subjective (L´opez-Linares et al., 2020). To overcome these challenges, alternative solutions such as deep learning approaches\n",
      "have been explored. Deep learning has made great progress in many other applications, such as Natural Language Processing\n",
      "and Computer Vision. Although transformers have been successfully applied in various domains, their application to medical\n",
      "images is still relatively new. Other deep learning approaches such as Convolutional Neural Networks (CNN), Recurrent\n",
      "Neural Networks (RNN), and Generative Adversarial Networks (GAN) are commonly used. This survey aims to provide a\n",
      "comprehensive overview of the various transformer models developed for processing medical images.\n",
      "6.3.1\n",
      "MEDICAL IMAGE SEGMENTATION\n",
      "Image segmentation refers to the task of grouping parts of the image that belong to the same category. In general, encoder-\n",
      "decoder architectures are commonly used for image segmentation (L´opez-Linares et al., 2020). In some cases, image seg-\n",
      "mentation is performed upstream of the classification task to improve the accuracy of the classification results (Wang et al.,\n",
      "2022c). The most frequently used loss functions in image segmentation are Pixel-wise cross-entropy loss and Dice Loss\n",
      "(L´opez-Linares et al., 2020). Common applications of medical image segmentation include detecting lesions, identifying\n",
      "cancer as benign or malignant, and predicting disease risk. This paper presents a comprehensive overview of relevant models\n",
      "used in medical image segmentation. Table 14 provides a summary of these models.\n",
      "Transformer\n",
      "Name\n",
      "Field of\n",
      "application\n",
      "Year\n",
      "Fully\n",
      "Transformer\n",
      "Architecture\n",
      "Image\n",
      "type\n",
      "Transformer\n",
      "Task\n",
      "Dataset\n",
      "FTN (He\n",
      "et al., 2022)\n",
      "Skin lesion\n",
      "2022\n",
      "YES\n",
      "2D\n",
      "Image seg-\n",
      "mentation /\n",
      "classification\n",
      "ISIC 2018 dataset\n",
      "RAT-Net (Zhu\n",
      "et al., 2022)\n",
      "Oncology\n",
      "(breast cancer)\n",
      "2022\n",
      "NO\n",
      "3D\n",
      "ultrasound\n",
      "Image seg-\n",
      "mentation\n",
      "a dataset of 256 subjects(330\n",
      "Automatic Breast Ultrasound\n",
      "images for each patient)\n",
      "nnFormer\n",
      "(Zhou et al.,\n",
      "2021a)\n",
      "Brain tumor\n",
      "multi-organ\n",
      "cardiac diagnosis\n",
      "2022\n",
      "YES\n",
      "3D\n",
      "Image seg-\n",
      "mentation\n",
      "Medical Segmentation De-\n",
      "cathlon (MSD), Synapse\n",
      "multiorgan segmentation,\n",
      "Automatic Cardiac Diag-\n",
      "nosis Challenge (ACDC)\n",
      "TransConver\n",
      "(Liang\n",
      "et al., 2022)\n",
      "Brain tumor\n",
      "2022\n",
      "NO\n",
      "2D/3D\n",
      "Image Seg-\n",
      "mentation\n",
      "MICCAI BraTS2019,\n",
      "MICCAI BraTS2018\n",
      "SwinBTS\n",
      "(Jiang et al.,\n",
      "2022b)\n",
      "Brain tumor\n",
      "2022\n",
      "NO\n",
      "3D\n",
      "Image Seg-\n",
      "mentation\n",
      "BraTS 2019, BraTS\n",
      "2020, BraTS 2021\n",
      "MTPA Unet\n",
      "(Jiang et al.,\n",
      "2022a)\n",
      "Retinal vessel\n",
      "2022\n",
      "NO\n",
      "2D\n",
      "Image seg-\n",
      "mentation\n",
      "DRIVE, CHASE DB1,\n",
      "and STARE Datasets\n",
      "Continued on next page\n",
      "\n",
      "\n",
      "## Page 29\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Table 14 – continued from previous page\n",
      "Transformer\n",
      "Name\n",
      "Field of\n",
      "application\n",
      "Year\n",
      "Fully\n",
      "Transformer\n",
      "Architecture\n",
      "Image\n",
      "type\n",
      "Transformer\n",
      "Task\n",
      "Dataset\n",
      "Dilated\n",
      "Transformer\n",
      "(Shen et al.,\n",
      "2022b)\n",
      "Oncology\n",
      "(Breast Cancer)\n",
      "2022\n",
      "NO\n",
      "2D\n",
      "ultrasound\n",
      "Image seg-\n",
      "mentation\n",
      "2 small breast ultra-\n",
      "sound image datasets\n",
      "TFNet (Wang\n",
      "et al., 2021b)\n",
      "Oncology\n",
      "(Breast lesion)\n",
      "2022\n",
      "NO\n",
      "2D\n",
      "ultrasound\n",
      "Image Seg-\n",
      "mentation\n",
      "BUSI Dataset DDTI Dataset\n",
      "Chest L-\n",
      "Transformer\n",
      "(Gu et al.,\n",
      "2022)\n",
      "Chest radiograph\n",
      "/ Thoracic\n",
      "diseases\n",
      "2022\n",
      "NO\n",
      "2D\n",
      "Image Clas-\n",
      "sification /\n",
      "Segmentation\n",
      "SIIM-ACR Pneumoth-\n",
      "orax Segmentation\n",
      "dataset contains 12,047\n",
      "Table 14: Transformer models for medical image segmentation\n",
      "• FTN: FTN is a transformer-based architecture developed specifically for segmenting and classifying 2D images of skin le-\n",
      "sions. It comprises of 5 layers, where each layer has a tokenization module known as SWT ”Sliding Window Tokenization”\n",
      "and a transformer module. The model is segregated into encoders and decoders for the segmentation task, while only an en-\n",
      "coder is needed for classification tasks. To improve computational efficiency and storage optimization, MSPA ”Multi-head\n",
      "Spatial Pyramid Attention” is utilized in the ”transformer” module instead of the traditional multi-head attention (MHA). In\n",
      "comparison to CNN, FTN has demonstrated superior performance on 10,025 images extracted from the publicly available\n",
      "ISIC 2018 dataset (He et al., 2022).\n",
      "• RAT-Net: The primary objective of RAT-Net (Region Aware Transformer Network) is to replace the laborious and time-\n",
      "consuming manual task of detecting lesion contours in 3D ABUS (Automatic Breast Ultrasound) images. Compared to\n",
      "other state-of-the-art models proposed for medical image segmentation, RAT-Net has shown excellent performance. It is\n",
      "based on the SegFormer Transformer model, which is used to encode input images and determine the regions that are more\n",
      "relevant for lesion segmentation (Zhu et al., 2022).\n",
      "• nnFormer:\n",
      "is a model that uses Transformer architecture for segmentation of 3D medical images. The experiments\n",
      "were performed on 484 brain tumor images, 30 multi-organ scans, and 100 cardiac diagnosis images. Instead of using\n",
      "the conventional attention mechanism, nnFormer introduced LV-MSA “Volume-based Multi-head Self-attention” and GV-\n",
      "MSA ”Global Volume-based Multi-head Self-attention” to reduce the computational complexity. Additionally, nnFormer\n",
      "employs multiple convolution layers with small kernels in the encoder instead of large convolution kernels as in other visual\n",
      "transformers (Zhou et al., 2021a).\n",
      "• TransConver:\n",
      "It combines CNN and SWIN transformers in parallel to extract global and local features simultaneously\n",
      "is proposed. The transformer block employs a cross-attention mechanism to merge semantically different global and local\n",
      "features. The network is designed to process both 2D and 3D brain tumor images and is trained on 335 cases from the\n",
      "training dataset of MICCAI BraTS2019. It is evaluated on 66 cases from MICCAI BraTS2018 and 125 cases from MICCAI\n",
      "BraTS2019 (Liang et al., 2022).\n",
      "• SwinBTS: is a recently developed model that addresses the segmentation of 3D medical images by combining the Swin\n",
      "Transformer with CNN. It adopts an encoder-decoder architecture that applies the Swin Transformer to both the encoder and\n",
      "decoder. In addition, SwinBTS incorporates an advanced feature extraction module called ETrans (Enhanced Transformer)\n",
      "that follows the transformer approach and leverages convolution techniques (Jiang et al., 2022b).\n",
      "• MTPA Unet:\n",
      "(Multi-scale Transformer-Position Attention Unet) is a model that has been evaluated on several publicly\n",
      "recognized retinal datasets to enhance the performance of retinal image segmentation tasks. This model combines CNN\n",
      "and transformer architectures sequentially to accurately capture local and global image information. To capture long-term\n",
      "dependencies between pixels as well as contextual information about each pixel location, this model employs TPA (Trans-\n",
      "former Position Attention), which is a combination of MSA (Multi-headed Self-Attention) and ”Position Attention Module”.\n",
      "Additionally, to optimize the model’s extraction ability, feature map inputs of different resolutions are implemented due to\n",
      "the detailed information contained in retinal images (Jiang et al., 2022a).\n",
      "• TFNet: TFNet, aims to segment 2D ultrasound images of breast lesions by combining CNN with a transformer architecture.\n",
      "To address the challenge of lesions with different scales and variable intensities, CNN is employed as a backbone to extract\n",
      "features from the images, resulting in 3 high-level features containing semantic information and 1 low-level feature. These\n",
      "\n",
      "\n",
      "## Page 30\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "high-level features are fused through a Transformer Fuse Module (TFM), while the low-level features are fused via skip\n",
      "connection. The transformer module includes two main parts: Vanilla Multi-Head Self-Attention to capture the long-range\n",
      "dependency between sequences and MultiHead Channel-Attention (MCA) to detect dependencies between channels. To\n",
      "enhance the model’s performance, novel loss functions are introduced, resulting in superior segmentation performance\n",
      "compared to other models. This approach is evaluated on a range of ultrasound image datasets, demonstrating excellent\n",
      "segmentation results (Wang et al., 2021b).\n",
      "• Dilated Transformer:\n",
      "The (DT) model has been developed for the segmentation of 2D ultrasound images from small\n",
      "datasets of breast cancer using transformer architecture. Standard transformer models require large pre-training datasets\n",
      "to generate high-quality segmentation results, but DT overcomes this challenge by implementing the ”Residual Axial At-\n",
      "tention” mechanism for segmenting images from small breast ultrasound datasets. This approach applies attention to a\n",
      "single axis, namely the height axis and the width axis, instead of the whole feature map, which saves time and enhances\n",
      "computation efficiency (Shen et al., 2022b).\n",
      "• Chest L-Transformer: This model is developed for the segmentation and classification of chest radiograph images. It uses\n",
      "a combination of CNN and transformer architecture, where the CNN is used as a backbone to extract local features from\n",
      "the 2D images and the transformer block is applied to detect the location of lesions using attention mechanisms. By using\n",
      "transformers in chest radiograph images, the model focuses more on areas where the disease may be more likely to occur,\n",
      "as opposed to treating them similarly using CNN alone (Gu et al., 2022).\n",
      "6.3.2\n",
      "MEDICAL IMAGE CLASSIFICATION\n",
      "Image classification refers to the process of recognizing, extracting, and selecting different types of features from an image for\n",
      "classification using labels (Wang et al., 2020b). Features in an image can be categorized into three types: low-level features,\n",
      "mid-level features, and high-level features (Wang et al., 2020b). Deep learning networks are designed to extract high-level\n",
      "features. Common applications of medical image classification include the detection of lesions, the identification of cancers\n",
      "as benign or malignant, and the prediction of disease risk (Khan & Lee, 2023, Jungiewicz et al., 2023). Table 15, provides an\n",
      "overview of several relevant examples of Transformers used in medical image classification.\n",
      "Transformer\n",
      "Name\n",
      "Field of\n",
      "application\n",
      "Year\n",
      "Fully\n",
      "Transformer\n",
      "Architecture\n",
      "Image\n",
      "type\n",
      "Transformer\n",
      "Task\n",
      "Dataset\n",
      "CCT-based\n",
      "Model (Islam\n",
      "et al., 2022)\n",
      "Malaria Disease\n",
      "2022\n",
      "NO\n",
      "2D\n",
      "images\n",
      "Image Clas-\n",
      "sification\n",
      "National Library of\n",
      "Medicine malaria dataset\n",
      "Chest L-\n",
      "Transformer\n",
      "(Gu et al.,\n",
      "2022)\n",
      "Chest radiograph\n",
      "/ Thoracic\n",
      "diseases\n",
      "2022\n",
      "NO\n",
      "2D\n",
      "Image Clas-\n",
      "sification /\n",
      "Segmentation\n",
      "SIIM-ACR Pneumoth-\n",
      "orax Segmentation\n",
      "dataset contains 12,047\n",
      "Table 15: Transformer models for medical image classification\n",
      "• CCT-based Model (Islam et al., 2022): The model presented in this work is designed for classifying red blood cell (RBC)\n",
      "images as containing malaria parasites or not, by using Compact Convolutional Transformers (CCTs). The model input\n",
      "consists of image patches generated through convolutional operations and preprocessed by reshaping them to a fixed size.\n",
      "Unlike other vision transformer models, this model performs classification using sequence pooling instead of class tokens.\n",
      "Compared to other deep learning models such as CNN, this model shows good performance in classifying RBC images.\n",
      "This satisfactory result was achieved by implementing a transformer architecture, using GRAD-CAM techniques to validate\n",
      "the learning process, and fine-tuning hyperparameters.\n",
      "• Chest L-Transformer (Gu et al., 2022): is a model designed for segmenting and classifying chest radiograph images (Gu\n",
      "et al., 2022). The model utilizes a CNN backbone to extract local features from 2D images and a transformer block to\n",
      "apply attention mechanisms for detecting lesion locations. By incorporating transformers into the chest radiograph image\n",
      "analysis, the model is better able to attend to areas where disease may be more likely to occur, as opposed to traditional\n",
      "CNNs which treat all areas similarly.\n",
      "6.3.3\n",
      "MEDICAL IMAGE TRANSLATION\n",
      "The field of research that involves altering the context (or domain) of an image without changing its original content is\n",
      "gaining traction. One example of this involves applying cartoon-style effects to images to change their appearance (Pang\n",
      "et al., 2022). Image-to-image translation is a promising technique that can be utilized to synthesize medical images from non-\n",
      "corrupted sources with less cost and time, and it is also helpful for preparing medical images for registration or segmentation.\n",
      "\n",
      "\n",
      "## Page 31\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Some of the most popular deep learning models developed for this area include “Pix2Pix” and “cyclic-consistency generative\n",
      "adversarial network” (GAN) (Yan et al., 2022b). Table 16 provides an overview of some relevant examples of ”Transformers”\n",
      "designed for medical image-to-image translation.\n",
      "Transformer\n",
      "Name\n",
      "Field of\n",
      "application\n",
      "Year\n",
      "Fully\n",
      "Transformer\n",
      "Architecture\n",
      "Image\n",
      "type\n",
      "Transformer\n",
      "Task\n",
      "Dataset\n",
      "MMTrans\n",
      "(Yan et al.,\n",
      "2022b)\n",
      "Magnetic\n",
      "resonance\n",
      "imaging (MRI)\n",
      "2022\n",
      "NO\n",
      "2D\n",
      "Medical image-\n",
      "to-image\n",
      "translation\n",
      "BraTs2018, fastMRI, The\n",
      "clinical brain MRI dataset\n",
      "TransCBCT\n",
      "(Chen et al.,\n",
      "2022c)\n",
      "Oncology\n",
      "(prostate Cancer)\n",
      "2022\n",
      "NO\n",
      "2D\n",
      "Image\n",
      "Translation\n",
      "91 patients with\n",
      "prostate cancer\n",
      "Table 16: Transformer models for medical image translation\n",
      "• MMTrans (Yan et al., 2022b): The MMtrans (Multi-Modal Medical Image Translation) model is proposed based on the\n",
      "GAN architecture and Swin transformer structure for performing medical image-to-image translation on Magnetic Reso-\n",
      "nance Imaging (MRI). Unlike other image-to-image translation frameworks, MMtrans utilizes the transformer to model\n",
      "long global dependencies to ensure accurate translation results. Moreover, MMtrans does not require images to be paired\n",
      "and pixel-aligned since it employs SWIN as a registration module adapted for paired and unpaired images, which makes\n",
      "it different from other architectures like Pix2Pix. The remaining modules of GAN use SwinIR as a generator module, and\n",
      "CNN as a discriminator module.\n",
      "• TransCBCT (Chen et al., 2022c): A new architecture called TransCBT is proposed for the purpose of performing ac-\n",
      "curate radiotherapy by improving the quality of 2D images, specifically cone-beam computed tomography (CBCT), and\n",
      "generating synthetic 2D images (sCT) without damaging their structures. TransCBT integrates pure-transformer modeling\n",
      "and convolution approaches to facilitate the extraction of global information and enhances performance by introducing the\n",
      "multi-head self-attention method (SW-MSA). Another model that can improve the quality of CT images reconstructed via\n",
      "sinograms is the CCTR (Shi et al., 2022a). In comparison to TransCBCT, CCTR experiments utilized a lung image database\n",
      "with 1010 patients, rather than the 91 patients used in TransCBCT.\n",
      "6.4\n",
      "MULTI-MODALITY\n",
      "The transformer has demonstrated its potential in multi-modality, which stems from the human ability to perceive and process\n",
      "information from various senses such as vision, hearing, and language. Multi-modality machine learning models are capable\n",
      "of processing and combining different types of data simultaneously. Natural language, vision, and speech are among the\n",
      "most common types of data handled by multi-modal models. Several popular tasks in multi-modality include visual question\n",
      "answering, classification and segmentation, visual captioning, commonsense reasoning, and text/image/video/speech gener-\n",
      "ation. In this section, we present a selection of transformer-based multi-modal models for each of these tasks providing an\n",
      "overview of their key features and working methods.\n",
      "6.4.1\n",
      "VISUAL QUESTION ANSWERING\n",
      "Visual question answering is a popular task that can be accomplished using multi-modal models. It involves combining NLP\n",
      "and computer vision to answer questions about an image or video. The goal is to understand the features of both textual and\n",
      "visual information and provide the correct answer. Typically, the models take an image or video and text as input and deliver\n",
      "text as output answers (Antol et al., 2015, Shih et al., 2016). In this context, we have identified and discussed the significant\n",
      "transformer models for visual question-answering tasks in Table 17.\n",
      "\n",
      "\n",
      "## Page 32\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Transformer\n",
      "Models\n",
      "Processed\n",
      "Data\n",
      "type (i/o)\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "BERT-\n",
      "Verients\n",
      "(Huang et al.,\n",
      "2020, Tan\n",
      "& Bansal,\n",
      "2019, Lu\n",
      "et al., 2019,\n",
      "Su et al.,\n",
      "2020, Chen\n",
      "et al., 2020c)\n",
      "Text and\n",
      "Image\n",
      "Question\n",
      "Answering,\n",
      "Common sense\n",
      "reasoning\n",
      "2019-\n",
      "2020\n",
      "Encoder\n",
      "Yes\n",
      "Pixel-BERT:\n",
      "MS-COCO,\n",
      "Visual Genome\n",
      "LX-MERT: MS\n",
      "COCO,Visual\n",
      "Genome,VQA\n",
      "v2.0,GQA,VG-\n",
      "QA ViLBERT:\n",
      "Visual Genome,\n",
      "COCO VL-\n",
      "BERT: Concep-\n",
      "tual Captions,\n",
      "BooksCor-\n",
      "pus, English\n",
      "Wikipedia\n",
      "Uniter: COCO,\n",
      "VG, CC, SBU\n",
      "Pixel-BERT: VQA 2.0\n",
      "NLVR2, Flickr30K MS-\n",
      "COCO LX-MERT:\n",
      "VQA,GQA,NLVR\n",
      "ViLBERT: Con-\n",
      "ceptual Captions,\n",
      "Flickr30k VL-BERT:\n",
      "VCR dataset, Re-\n",
      "fCOCO Uniter:\n",
      "COCO, Flickr30K,\n",
      "VG, CC, SBU\n",
      "VIOLET (Fu\n",
      "et al., 2021)\n",
      "Video\n",
      "and Text\n",
      "Video Question\n",
      "Answering,\n",
      "Text-to-video\n",
      "retrieval,\n",
      "Visual-Text\n",
      "Matching\n",
      "2022\n",
      "Encoder\n",
      "Yes\n",
      "Conceptual\n",
      "Captions-3M,\n",
      "WebVid-\n",
      "2.5M, YT-\n",
      "Temporal-180M\n",
      "MSRVTT, DiDeMo,\n",
      "YouCook2, LSMDC,\n",
      "TGIF-Action, TGI-\n",
      "Transition, TGIF-\n",
      "Frame, MSRVTT-\n",
      "MC, MSRVTT-QA,\n",
      "MSVD-QA, LSMDC-\n",
      "MC, LSMDC-FiB\n",
      "GIT (Wang\n",
      "et al., 2022a)\n",
      "Image\n",
      "and Text\n",
      "Image\n",
      "Classification,\n",
      "Image/video\n",
      "captioning,\n",
      "Question\n",
      "answering\n",
      "2022\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "combination of\n",
      "COCO, SBU,\n",
      "CC3M, VG,\n",
      "GITL, ALT200M\n",
      "and CC12M\n",
      "Karpathy split-\n",
      "COCO, Flickr30K,\n",
      "no caps, TextCaps,\n",
      "VizWiz-Captions,\n",
      "CUTE, TextOCR\n",
      "SIMVLM\n",
      "(Wang et al.,\n",
      "2022d)\n",
      "Image\n",
      "and Text\n",
      "Visual\n",
      "Question\n",
      "answering,\n",
      "image\n",
      "captioning\n",
      "2022\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "ALIGN &\n",
      "Colossal Clean\n",
      "Crawled Corpus\n",
      "(C4) datasets\n",
      "SNLI-VE, SNLI,\n",
      "MNLI, Multi30k,\n",
      "10% ALIGN , CC-3M\n",
      "BLIP (Li\n",
      "et al., 2022)\n",
      "Image,\n",
      "Video\n",
      "and Text\n",
      "Question\n",
      "Answering,\n",
      "Image\n",
      "Captioning,\n",
      "image-text\n",
      "retrieval\n",
      "2022\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "Bootstrapped\n",
      "dataset-\n",
      "COCO, VG,\n",
      "SBU, CC3M,\n",
      "CC12M, LAION\n",
      "COCO, Flickr30K,\n",
      "NoCaps, MSRVTT\n",
      "Table 17: Transformer models for multi-modality - visual question answering task\n",
      "• BERT-Variants: Following the successful application of BERT-based models in NLP and computer vision tasks, several\n",
      "BERT-based models have demonstrated significant improvements in multi-modal tasks, particularly in question answering\n",
      "and commonsense reasoning. Currently, there are two distinct types of BERT-based models available in the literature: (i)\n",
      "Single-Stream Models and (ii) Two-Stream Models.\n",
      "Single-Stream Models, such as VL-BERT, Uniter, etc., encode both modalities (text and image) within the same module.\n",
      "In contrast, Two-Stream Models, such as VilBERT, LXMERT, etc., process text and image through separate modules. Both\n",
      "types of models have been shown to yield promising results in various multi-modal tasks.\n",
      "• ViLBERT: ViLBERT is a two-stream model that is trained on text-image pairs and then passed both of the modules\n",
      "through co-attention, which helps to detect the important features of both text and images (Lu et al., 2019). VLBERT, on\n",
      "the other hand, is a single-stream model that is pre-trained and takes both the image and text embedding features as input,\n",
      "making this model simple yet powerful (Su et al., 2020). Uniter represents Universal Image-Text Representation, which is\n",
      "a large-scale pre-trained model completed through masking (Chen et al., 2020c). Pixel-BERT is built using a combination\n",
      "\n",
      "\n",
      "## Page 33\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "of convolutional neural network (CNN) to extract image pixels and an encoder to extract text tokens, while the BERT-based\n",
      "transformer works as the cross-modality module. To capture all the spatial information from the image, Pixel-BERT takes\n",
      "the whole image as input, whereas other models extract image features from the regions (Huang et al., 2020). Finally,\n",
      "LXMERT stands for Learning Cross-Modality Encoder Representations from Transformers. It processes the image and\n",
      "text through two different modules and is built with three encoders. This pre-trained model follows masked modeling and\n",
      "cross-modality for pre-training, which captures better relationships between text and images (Tan & Bansal, 2019).\n",
      "• VIOLET: VIOLET is a neural network based on the transformer architecture that is designed to associate video with text.\n",
      "It consists of three modules: (1) Video Swin Transformer (VT), (2) Language Embedder (LE), and (3) Cross-modal Trans-\n",
      "former (CT). VIOLET differs from other models in that it processes the temporal-spatial information from the video, rather\n",
      "than just extracting static images. This model has been evaluated on 12 datasets and has shown outstanding performance\n",
      "in many downstream tasks, such as Text-To-Video Retrieval and Video Question Answering. Furthermore, the authors pro-\n",
      "pose a new pre-training task called ”Masked Visual Token Modeling,” which is used to pre-train VIOLET. This approach\n",
      "is combined with two other pre-training approaches, Masked Language Modeling and Visual-Text Matching, to achieve\n",
      "state-of-the-art performance on various benchmarks (Fu et al., 2021).\n",
      "• GIT: The Generative Image-to-Text Transformer, or GIT, is a multi-modal model designed to generate textual descriptions\n",
      "from visual images. This model employs both the encoder and decoder modules of the transformer architecture, using the\n",
      "image for encoding and decoding the text. To train the model, a large dataset of images paired with textual descriptions\n",
      "is used, allowing GIT to generate textual descriptions for previously unseen images. This approach has shown promising\n",
      "results in generating high-quality textual descriptions of images (Wang et al., 2022a).\n",
      "SimVLM & BLIP: Both SimVLM and BLIP are models that can perform the task of visual captioning, which involves\n",
      "generating textual descriptions of visual images. The highlights of these models can be found in that section.\n",
      "6.4.2\n",
      "CLASSIFICATION & SEGMENTATION\n",
      "Multi-modal classification and segmentation are often considered related tasks that involve classifying or segmenting data\n",
      "based on multiple modalities, such as text, image/video, and speech. As segmentation often helps to classify the image,\n",
      "text, or speech. In multi-modal classification, the task is to classify data based on its similarity and features using multiple\n",
      "modalities. This can involve taking text, image/video, or speech as input and using all of these modalities to classify the data\n",
      "more accurately. Similarly, in multi-modal segmentation, the task is to segment data based on its features and use multiple\n",
      "modalities to achieve a more accurate segmentation. Both of these tasks require a deep understanding of the different forms\n",
      "of data and how they can be used together to achieve better classification or segmentation performance (Liu et al., 2022c,\n",
      "Mahesh & Renjit, 2020, Wu et al., 2016, Menze et al., 2015). In recent years, transformer models have shown promising\n",
      "results in multi-modal classification and segmentation tasks. In Table 18, we highlight some of the significant transformer\n",
      "models that have been developed for these tasks.\n",
      "Transformer\n",
      "Models\n",
      "Processed\n",
      "Data\n",
      "type (i/o)\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "CLIP\n",
      "(Radford\n",
      "et al., 2021)\n",
      "Image\n",
      "and Text\n",
      "image\n",
      "Classification\n",
      "2021\n",
      "Encoder\n",
      "Yes\n",
      "Pre-training\n",
      "dataset from\n",
      "internet for CLIP\n",
      "ImageNet, ImageNet\n",
      "V2, ImageNet Rendi-\n",
      "tion, ObjectNet, Ima-\n",
      "geNet Sketch, ImageNet\n",
      "Adversarial 30 datasets\n",
      "VATT\n",
      "(Akbari\n",
      "et al., 2021)\n",
      "Video,\n",
      "Audio\n",
      "and Text\n",
      "Audio event\n",
      "classification,\n",
      "Image\n",
      "classification,\n",
      "Video action\n",
      "recognition,\n",
      "Text-To-Video\n",
      "retrieval\n",
      "2021\n",
      "Encoder\n",
      "Yes\n",
      "AudioSet,\n",
      "HowTo100M\n",
      "UCF10,HMDB5,\n",
      "Kinetics-400, Kinetics-\n",
      "600,Moments in Time,\n",
      "ESC50, AudioSet,\n",
      "YouCook2, MSR-\n",
      "VTT, ImageNet\n",
      "Continued on next page\n",
      "\n",
      "\n",
      "## Page 34\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Table 18 – continued from previous page\n",
      "Transformer\n",
      "Models\n",
      "Processed\n",
      "Data\n",
      "type (i/o)\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "Unicoder-VL\n",
      "(Li et al.,\n",
      "2020a)\n",
      "Image\n",
      "and Text\n",
      "Object\n",
      "Classification,\n",
      "Visual-\n",
      "linguistic\n",
      "Matching,\n",
      "visual\n",
      "commonsense\n",
      "reasoning,\n",
      "image-text\n",
      "retrieval\n",
      "2020\n",
      "Encoder\n",
      "Yes\n",
      "Conceptual\n",
      "Captions-3M,\n",
      "SBU Captions\n",
      "MSCOCO, Flickr30K\n",
      "ViLT (Kim\n",
      "et al., 2021b)\n",
      "Image\n",
      "and Text\n",
      "Visual\n",
      "Question\n",
      "Answering,\n",
      "Image text\n",
      "matching,\n",
      "Natural\n",
      "Language\n",
      "for Visual\n",
      "Reasoning\n",
      "2021\n",
      "Encoder\n",
      "Yes\n",
      "MS-\n",
      "COCO,Visual\n",
      "Genome, SBU\n",
      "Captions, Google\n",
      "Conceptual\n",
      "Captions\n",
      "VQA 2.0, NLVR2,\n",
      "MSCOCO, Flickr30K\n",
      "MBT\n",
      "(Nagrani\n",
      "et al., 2021)\n",
      "Audio and\n",
      "Visual\n",
      "Audio-visual\n",
      "classification\n",
      "2022\n",
      "Encoder\n",
      "Yes\n",
      "VGGSoun,\n",
      "Kinetics400\n",
      "and AS-500K,\n",
      "VGGSound\n",
      "Audioset-mini and\n",
      "VGGSound, Moments\n",
      "In Time, Kinetics\n",
      "ALIGN (Jia\n",
      "et al., 2021)\n",
      "Image\n",
      "and Text\n",
      "Visual\n",
      "Classification\n",
      "2021\n",
      "Encoder\n",
      "Yes\n",
      "ALIGN training\n",
      "data, 0%\n",
      "randomly\n",
      "sampled ALIGN\n",
      "training data,\n",
      "and CC-3M\n",
      "Conceptual Captions-\n",
      "CC, Flickr30K,\n",
      "MSCOCO,\n",
      "ILSVRC-2012\n",
      "Florence\n",
      "(Yuan\n",
      "et al., 2021)\n",
      "Image\n",
      "and Text\n",
      "Classification,\n",
      "image caption,\n",
      "visual action\n",
      "recognition,\n",
      "Text-visual\n",
      "& visual-text\n",
      "retrieval\n",
      "2021\n",
      "Encoder\n",
      "Yes\n",
      "FLD-900M,\n",
      "ImageNet (Swin\n",
      "transformer\n",
      "& CLIP)\n",
      "Web-scale by data\n",
      "curation, UniCL,\n",
      "ImageNet, COCO,\n",
      "Kinetics-600, Flickr30k,\n",
      "MSCOCO, SR-VTT\n",
      "GIT (Wang\n",
      "et al., 2022a)\n",
      "Image\n",
      "and Text\n",
      "Image\n",
      "Classification,\n",
      "Image/video\n",
      "captioning,\n",
      "Question\n",
      "answering\n",
      "2022\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "combination of\n",
      "COCO, SBU,\n",
      "CC3M, VG,\n",
      "GITL, ALT200M\n",
      "and CC12M\n",
      "Karpathy split-\n",
      "COCO, Flickr30K,\n",
      "no caps, TextCaps,\n",
      "VizWiz-Captions,\n",
      "CUTE, TextOCR\n",
      "Table 18: Multi-modal Transformer models - classification & segmentation tasks\n",
      "• CLIP: CLIP (Constructive Language-Image Pre-Training) is a multi-modal model that is trained in a supervised way with\n",
      "text and image data. This model simultaneously trains both the text and image through an encoder and predicts proper\n",
      "batches of text and image. CLIP is capable of understanding the relationship between text and images, and can generate\n",
      "images based on input text as well as generate text based on input images. CLIP has been shown to perform well on several\n",
      "benchmark datasets and is considered a state-of-the-art model for multi-modal tasks involving text and images (Radford\n",
      "et al., 2021).\n",
      "• VATT: which stands for Video, Audio, Text Transformer, is a multi-modal model based on the traditional transformer\n",
      "architecture without convolution layers. It is inspired by BERT and ViT and is pre-trained on two datasets using the Drop\n",
      "Token approach to optimize the training process. VATT is evaluated on 10 datasets containing videos, audio data, and text\n",
      "speech for 4 downstream tasks: Video action recognition, audio event classification, image classification, and text-to-video\n",
      "retrieval (Akbari et al., 2021).\n",
      "\n",
      "\n",
      "## Page 35\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "• MBT: Multimodal Bottleneck Transformer (MBT) is a transformer-based model designed for processing both audio and\n",
      "visual data. The MBT model utilizes a bottleneck structure to focus on essential information for processing through the\n",
      "transformer architecture, thereby reducing the amount of data processed and minimizing the risk of overfitting. The bottle-\n",
      "neck structure reduces the size of the model, training time, and computational cost. MBT has shown promising results in\n",
      "various multimodal tasks, such as audio-visual speech recognition and audio-visual event detection (Nagrani et al., 2021).\n",
      "• ALIGN: it stands for Large-scale ImaGe and Noise-text embedding. This model is a large-scale model that uses vision-\n",
      "language representational learning with noisy text annotations. ALIGN is a pre-trained model which uses a dual-encoder\n",
      "and is trained on huge-sized noisy image-text pair datasets. The dataset scale is able to adjust for noise, eliminating the need\n",
      "for pre-processing. ALIGN uses the contrastive loss to train the model, considering both image-to-text and text-to-image\n",
      "classification losses (Jia et al., 2021).\n",
      "• Florence: Florence is a visual-language representation model that is capable of handling multiple tasks. It is an encoder-\n",
      "based pre-trained model trained on web-scale image-text data, and it can handle high-resolution images. This model shows\n",
      "strong performance on classification tasks, as well as other tasks like object/action detection and question answering (Yuan\n",
      "et al., 2021).\n",
      "Unicoder-VL, GIT & ViLT: Unicoder-VL and ViLT models have been described in the Visual Commonsense Reasoning\n",
      "section. Both models can perform the Commonsense Reasoning task in addition to other tasks. However, the characteristics\n",
      "of GIT model can be found on the visual question-answering section.\n",
      "6.4.3\n",
      "VISUAL CAPTIONING\n",
      "Visual captioning is a multi-modal task that involves both computer vision and NLP. The task aims to generate a textual\n",
      "description of an image, which requires a deep understanding of the relationship between image features and text. The visual\n",
      "captioning process usually involves several steps, starting with image processing, followed by encoding the features into\n",
      "vectors that can be used by the NLP model. These encoded vectors are then decoded into text, typically through generative\n",
      "NLP models. Although it is a complex process, visual captioning has a wide range of applications (Yu et al., 2020, Hossain\n",
      "et al., 2019). In this section, we discuss significant transformer models for visual captioning tasks (see Table 19).\n",
      "Transformer\n",
      "Models\n",
      "Processed\n",
      "Data\n",
      "type (i/o)\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "BLIP (Li\n",
      "et al., 2022)\n",
      "Image,\n",
      "Video\n",
      "and Text\n",
      "Image\n",
      "Captioning,\n",
      "Question\n",
      "Answering,\n",
      "image-text\n",
      "retrieval\n",
      "2022\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "Bootstrapped\n",
      "dataset-\n",
      "COCO, VG,\n",
      "SBU, CC3M,\n",
      "CC12M, LAION\n",
      "COCO, Flickr30K,\n",
      "NoCaps, MSRVTT\n",
      "SIMVLM\n",
      "(Wang et al.,\n",
      "2022d)\n",
      "Image\n",
      "and Text\n",
      "Image\n",
      "captioning,\n",
      "Visual\n",
      "Question\n",
      "answering\n",
      "2022\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "ALIGN &\n",
      "Colossal Clean\n",
      "Crawled Corpus\n",
      "(C4) datasets\n",
      "SNLI-VE, SNLI,\n",
      "MNLI, Multi30k,\n",
      "10% ALIGN , CC-3M\n",
      "Florence\n",
      "(Yuan\n",
      "et al., 2021)\n",
      "Image\n",
      "and Text\n",
      "Classification,\n",
      "image caption,\n",
      "visual action\n",
      "recognition,\n",
      "Text-visual\n",
      "& visual-text\n",
      "retrieval\n",
      "2021\n",
      "Encoder\n",
      "Yes\n",
      "FLD-900M,\n",
      "ImageNet (Swin\n",
      "transformer\n",
      "& CLIP)\n",
      "Web-scale by data\n",
      "curation, UniCL,\n",
      "ImageNet, COCO,\n",
      "Kinetics-600, Flickr30k,\n",
      "MSCOCO, SR-VTT\n",
      "GIT (Wang\n",
      "et al., 2022a)\n",
      "Image\n",
      "and Text\n",
      "Image\n",
      "Classification,\n",
      "Image/video\n",
      "captioning,\n",
      "Question\n",
      "answering\n",
      "2022\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "combination of\n",
      "COCO, SBU,\n",
      "CC3M, VG,\n",
      "GITL, ALT200M\n",
      "and CC12M\n",
      "Karpathy split-\n",
      "COCO, Flickr30K,\n",
      "no caps, TextCaps,\n",
      "VizWiz-Captions,\n",
      "CUTE, TextOCR\n",
      "Table 19: Multi-modal Transformer models - visual captioning task\n",
      "• BLIP: Bootstrapping Language-Image Pre-training (BLIP) is a pre-trained model designed to enhance performance on var-\n",
      "ious tasks through fine-tuning for specific tasks. This model utilizes a VLP (Vision and Language Pre-training) framework\n",
      "with an encoder-decoder module of the Transformer architecture, which uses noisy data with captions and is trained to\n",
      "\n",
      "\n",
      "## Page 36\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "remove noisy captions. BLIP is capable of performing a range of downstream tasks, including image captioning, question\n",
      "answering, image-text retrieval, and more (Li et al., 2022).\n",
      "• SimVLM: short for SIMple Visual Language Model, SimVLM is a pre-trained model that uses weak supervision methods\n",
      "for training. This approach provides the model with greater flexibility and scalability. Instead of using pixel patch projection,\n",
      "this model uses the full image as patches and is trained with a language model. As a result of these methods, SimVLM is\n",
      "capable of performing various tasks, with question answering being one of its significant strengths (Wang et al., 2022d).\n",
      "• Florence:\n",
      "Florence is a visual-language representation model that can perform multiple tasks. It is an encoder-based\n",
      "pre-trained model trained on web-scale image-text data, which enables it to handle high-resolution images. In addition to\n",
      "tasks such as object/action detection and question answering, Florence also shows strong performance in classification tasks\n",
      "(Yuan et al., 2021).\n",
      "GIT: The description of the model has already been provided in the Question Answering section above. These models can\n",
      "also perform the Question Answering task with high performance.\n",
      "6.4.4\n",
      "VISUAL COMMONSENSE REASONING\n",
      "Visual commonsense reasoning is a challenging task that requires a model with a deep understanding of visualization and\n",
      "different images or videos containing objects and scenes, inspired by how humans see and visualize things. These models\n",
      "capture information from different sub-tasks like object recognition and feature extraction. This information is then trans-\n",
      "formed into a vector to be used for reasoning. The reasoning module understands the relationship between the objects in the\n",
      "image and the output of the inferencing step provides a prediction about the interaction and relationship between the objects.\n",
      "Visual commonsense reasoning helps to improve the performance of various tasks like classification, image captioning, and\n",
      "other deep understanding-related tasks (Zellers et al., 2019, Xing et al., 2021). In this section, we highlight and discuss\n",
      "significant transformer models for visual commonsense reasoning tasks that are summarized in Table 20.\n",
      "Transformer\n",
      "Models\n",
      "Processed\n",
      "Data\n",
      "type(i/o)\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "BERT-\n",
      "Verients\n",
      "(Huang et al.,\n",
      "2020, Tan\n",
      "& Bansal,\n",
      "2019, Lu\n",
      "et al., 2019,\n",
      "Su et al.,\n",
      "2020, Chen\n",
      "et al., 2020c)\n",
      "Text and\n",
      "Image\n",
      "Question\n",
      "Answering,\n",
      "Common\n",
      "sense\n",
      "reasoning\n",
      "2019-\n",
      "2020\n",
      "Encoder\n",
      "Yes\n",
      "Pixel-BERT: MS-\n",
      "COCO, Visual Genome\n",
      "LX-MERT: MS\n",
      "COCO,Visual\n",
      "Genome,VQA\n",
      "v2.0,GQA,VG-QA\n",
      "ViLBERT: Visual\n",
      "Genome, COCO\n",
      "VL-BERT: Con-\n",
      "ceptual Captions,\n",
      "BooksCorpus, English\n",
      "Wikipedia Uniter:\n",
      "COCO, VG, CC, SBU\n",
      "Pixel-BERT:\n",
      "VQA 2.0 NLVR2,\n",
      "Flickr30K MS-\n",
      "COCO LX-MERT:\n",
      "VQA,GQA,NLVR\n",
      "ViLBERT: Con-\n",
      "ceptual Captions,\n",
      "Flickr30k VL-BERT:\n",
      "VCR dataset, Re-\n",
      "fCOCO Uniter:\n",
      "COCO, Flickr30K,\n",
      "VG, CC, SBU\n",
      "ViLT (Kim\n",
      "et al., 2021b)\n",
      "Image\n",
      "and Text\n",
      "Visual\n",
      "Question\n",
      "Answering,\n",
      "Image text\n",
      "matching,\n",
      "Natural\n",
      "Language\n",
      "for Visual\n",
      "Reasoning\n",
      "2021\n",
      "Encoder\n",
      "Yes\n",
      "MS-COCO,Visual\n",
      "Genome, SBU\n",
      "Captions, Google\n",
      "Conceptual Captions\n",
      "VQA 2.0, NLVR2,\n",
      "MSCOCO, Flickr30K\n",
      "Unicode-VL\n",
      "(Li et al.,\n",
      "2020a)\n",
      "Image\n",
      "and Text\n",
      "Object Clas-\n",
      "sification,\n",
      "Visual-\n",
      "linguistic\n",
      "Matching,\n",
      "visual com-\n",
      "monsense\n",
      "reasoning,\n",
      "image-text\n",
      "retrieval\n",
      "2020\n",
      "Encoder\n",
      "Yes\n",
      "Conceptual Captions-\n",
      "3M, SBU Captions\n",
      "MSCOCO, Flickr30K\n",
      "Continued on next page\n",
      "\n",
      "\n",
      "## Page 37\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Table 20 – continued from previous page\n",
      "Transformer\n",
      "Models\n",
      "Processed\n",
      "Data\n",
      "type\n",
      "(i/o)\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training Dataset\n",
      "Dataset(Fine-tuning,\n",
      "Training, Testing)\n",
      "Table 20: Multi-modal Transformer models - visual commonsense reasoning task\n",
      "• Unicoder-VL: Unicoder-VL is a large-scale pre-trained encoder-based model that utilizes cross-modeling to build a strong\n",
      "understanding of the relationship between image and language. The model employs a masking scheme for pre-training on a\n",
      "large corpus of data. These methods enhance the model’s performance on visual commonsense reasoning tasks in addition\n",
      "to visual classification tasks (Li et al., 2020a)\n",
      "• ViLT (Vision-and-Language Transformer): is a multi-modal architecture based on the ViT (Vision Transformer) model,\n",
      "utilizing a free-convolution approach. Unlike other VLP (Vision-and-Language Pre-training) models, ViLT performs data\n",
      "augmentation during the execution of downstream tasks of classification and retrievals, which improves the model’s perfor-\n",
      "mance. Inspired by Pixel-BERT, ViLT takes the entire image as input instead of just using selected regions. By omitting\n",
      "convolutional visual embedders, ViLT reduces the model size and achieves remarkable performance compared to other VLP\n",
      "models (Kim et al., 2021b).\n",
      "BERT-Variants: The BERT-Variants models have been previously described in the Classification & segmentation section.\n",
      "It should be noted that these models are also capable of performing the Classification & segmentation task.\n",
      "6.4.5\n",
      "IMAGE/VIDEO/SPEECH GENERATION\n",
      "Multi-modal generation tasks have gained a lot of attention in the field of artificial intelligence. These tasks involve gener-\n",
      "ating images, text, or speech from inputs of different modalities of input. In recent times, several generative models have\n",
      "demonstrated outstanding performance, making this field of research even more attractive (Suzuki & Matsuo, 2022). In this\n",
      "section, we discuss some significant transformer models that have been used for multi-modal generation tasks. These models\n",
      "are summarized in Table 21.\n",
      "Transformer\n",
      "Models\n",
      "Processed\n",
      "Data\n",
      "type(i/o)\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "DALL-E\n",
      "(Ramesh\n",
      "et al., 2021)\n",
      "Image\n",
      "and Text\n",
      "Image\n",
      "Generation\n",
      "from text\n",
      "2021\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "Not disclosed\n",
      "Conceptual Captions (MS-\n",
      "COCO extension), Wikipedia\n",
      "(text-image pairs), and\n",
      "YFCC100M(filtered subset)\n",
      "GLIDE\n",
      "(Nichol\n",
      "et al., 2022)\n",
      "Image\n",
      "and Text\n",
      "Image\n",
      "generation &\n",
      "edit from text\n",
      "2021\n",
      "Encoder &\n",
      "Decoder\n",
      "No\n",
      "NA\n",
      "MS-COCO, ViT-B CLIP\n",
      "(noised), CLIP and\n",
      "DALL-E filtered datasets\n",
      "Chimera (Li\n",
      "& Hoefler,\n",
      "2021)\n",
      "Audio\n",
      "and Text\n",
      "Text\n",
      "generation\n",
      "from speech\n",
      "2021\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "ALIGN &\n",
      "MT-Dataset\n",
      "MuST-C, Augmented\n",
      "LibriSpeech Dataset (En-Fr),\n",
      "Machine Translation Datasets\n",
      "(WMT 2016, WMT 2014,\n",
      "OPUS100, OpenSubtitles)\n",
      "CogView\n",
      "(Ding et al.,\n",
      "2021)\n",
      "Image\n",
      "and Text\n",
      "Classification,\n",
      "Image\n",
      "generation\n",
      "from text\n",
      "2021\n",
      "Decoder\n",
      "Yes\n",
      "VQ-VAE\n",
      "MS COCO, Wudao\n",
      "Corpora-extension dataset.\n",
      "Table 21: Multi-modal Transformer models - image/video/speech generation task\n",
      "• DALL-E: DALL-E (Ramesh et al., 2021) is a popular transformer-based model for generating images from text. It is\n",
      "trained on a large and diverse dataset of both text and images, utilizing 12 billion parameters from the GPT-3 architecture.\n",
      "To reduce memory consumption, DALL-E compresses images without compromising their visual quality. An updated\n",
      "version of DALL-E, known as DALL-E 2, has been introduced with a higher number of parameters (175 billion) which\n",
      "allows for the generation of higher resolution images. Additionally, DALL-E 2 is capable of generating a wider range of\n",
      "images.\n",
      "• Chimera: The Chimera end-to-end architecture is designed for Speech-And-Text-Translation (ST). This architecture draws\n",
      "inspiration from Text-Machine-Translation and proposes a new module called the Shared Semantic Projection Module based\n",
      "on attention mechanisms. The objective of this module is to reduce latency and errors during the speech translation process\n",
      "by removing dissimilarities between speech and language modalities.\n",
      "\n",
      "\n",
      "## Page 38\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "• CogView: CogView (Ding et al., 2021) is an image generation model that generates images based on input text descriptions,\n",
      "making it a challenging task that requires a deep understanding of the contextual relationship between text and image. It\n",
      "utilizes a transformer-GPT-based architecture to encode the text into a vector and decode it into an image. This model\n",
      "outperforms DALL-E in some cases, which also generates images from text descriptions, but uses text-image pairs for\n",
      "training the model.\n",
      "• GLIDE: short for Guided Language to Image Diffusion for Generation and Editing, GLIDE is a diffusion model that is\n",
      "distinct from conventional models in that it can both generate and edit images. Unlike other models, diffusion models\n",
      "are sequentially injected with random noise and trained to remove that noise to construct the original data. GLIDE takes\n",
      "textual information as input and generates an output image conditioned on that information. In some instances, the images\n",
      "generated by GLIDE are more impressive than those generated by DALL-E (Nichol et al., 2022).\n",
      "6.4.6\n",
      "CLOUD COMPUTING\n",
      "Cloud computing is a crucial element of modern technology, particularly with regard to the Internet of Things (IoT). It\n",
      "encompasses a wide variety of cloud-based tasks, including server computing, task scheduling, storage, networking, and\n",
      "more. In wireless networks, cloud computing aims to improve scalability, flexibility, and adaptability, thereby providing\n",
      "seamless connectivity. To achieve this, data or information is retrieved from the network for computation, with various types\n",
      "of data being processed, including text, images, speech, and digits. Due to this multi-modal approach, cloud computing is\n",
      "classified in this category (Li et al., 2017, Jauro et al., 2020, Yu et al., 2017). In this article, we focus on the significant\n",
      "transformer models used in cloud computing tasks, which are presented in Table 22.\n",
      "Transformer\n",
      "Models\n",
      "Processed\n",
      "Data\n",
      "type(i/o)\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "VMD & R-\n",
      "Transformer\n",
      "(Zhou\n",
      "et al., 2020)\n",
      "workload\n",
      "sequence\n",
      "cloud\n",
      "workload\n",
      "forecasting\n",
      "2020\n",
      "Encoder &\n",
      "Decoder\n",
      "No\n",
      "NA\n",
      "Google cluster trace,\n",
      "Alibaba cluster trace\n",
      "ACT4JS\n",
      "(Xu &\n",
      "Zhao, 2022)\n",
      "Cloud\n",
      "jobs/task\n",
      "Cloud\n",
      "computing\n",
      "resource job\n",
      "scheduling.\n",
      "2022\n",
      "Encoder\n",
      "No\n",
      "NA\n",
      "Alibaba Cluster-V2018\n",
      "TEDGE-\n",
      "Catching\n",
      "(Hajiakhondi-\n",
      "Meybodi\n",
      "et al., 2022)\n",
      "Sequential\n",
      "request\n",
      "pattern\n",
      "of the\n",
      "contents\n",
      "(Ex:\n",
      "video,\n",
      "image,\n",
      "websites,\n",
      "etc)\n",
      "Predict the\n",
      "content\n",
      "popularity\n",
      "in proactive\n",
      "caching\n",
      "schemes.\n",
      "2021\n",
      "Encoder\n",
      "No\n",
      "NA\n",
      "MovieLens\n",
      "SACCT\n",
      "(Wang et al.,\n",
      "2022b)\n",
      "Network\n",
      "status\n",
      "(Band-\n",
      "width,\n",
      "storage,\n",
      "and etc)\n",
      "Optimize\n",
      "network\n",
      "based on\n",
      "network\n",
      "status.\n",
      "2021\n",
      "Encoder\n",
      "No\n",
      "NA\n",
      "Not mentioned clearly\n",
      "Table 22: Multi-modal Transformer models - cloud computing task\n",
      "• VMD & R-TRANSFORMER: Workload forecasting is a critical task for the cloud, and previous research has focused\n",
      "on using recurrent neural networks (RNNs) for this purpose. However, due to the highly complex and dynamic nature of\n",
      "workloads, RNN-based models struggle to provide accurate forecasts because of the problem of vanishing gradients. In this\n",
      "context, the proposed Variational Mode Decomposition-VMD and R-Transformer model offers a more accurate solution\n",
      "by capturing long-term dependencies using multi-head attention and local non-linear relationships of workload sequences\n",
      "with local techniques (Zhou et al., 2020). Therefore, this model is capable of executing the workload forecasting task with\n",
      "greater precision than existing RNN-based models.\n",
      "• TEDGE-Caching: TEDGE is an acronym for Transformer-based Edge Caching, which is a critical component of the\n",
      "6G wireless network as it provides a high-bandwidth, low-latency connection. Edge caching stores multimedia content\n",
      "\n",
      "\n",
      "## Page 39\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "to deliver it to users with low latency. To achieve this, it is essential to proactively predict popular content. However,\n",
      "conventional models are limited by long-term dependencies, computational complexity, and the inability to compute in\n",
      "parallel. In this context, the Transformer-based Edge (TEDGE) caching framework incorporates a vision transformer (ViT)\n",
      "to overcome these limitations, without requiring data pre-processing or additional contextual information to predict popular\n",
      "content at the Mobile Edge. This is the first model to apply a transformer-based approach to execute this task, resulting in\n",
      "superior performance (Hajiakhondi-Meybodi et al., 2022).\n",
      "• ACT4JS: The Actor-Critic Transformer for Job Scheduling (ACT4JS) is a transformer-based model designed to allocate\n",
      "cloud computing resources to different tasks in cloud computing. The model consists of an Actor and Critic network, where\n",
      "the Actor-network selects the best action to take at each step, and the Critic network evaluates the action taken by the Actor-\n",
      "network and provides feedback to improve future steps. This approach allows for a better understanding of the complex\n",
      "relationship between cloud jobs and enables prediction or scheduling of jobs based on different features such as job priority,\n",
      "network conditions, resource availability, and more (Xu & Zhao, 2022).\n",
      "• SACCT: SACCT refers to the Soft Actor-Critic framework with Communication Transformer, which combines the trans-\n",
      "former, reinforcement learning, and convex optimization techniques. This model introduces the Communication Trans-\n",
      "former (CT), which works with reinforcement learning to adapt to different challenges, such as bandwidth limitations,\n",
      "storage constraints, and more, in the wireless edge network during live streaming. Adapting to changing network conditions\n",
      "is critical during live streaming, and SACCT provides a system that adjusts resources to improve the quality of service\n",
      "based on user demand and network conditions. The SACCT model’s ability to adapt to changing conditions and optimize\n",
      "resources makes it an important contribution to the field of wireless edge network technology (Wang et al., 2022b).\n",
      "6.5\n",
      "AUDIO & SPEECH\n",
      "Audio and speech processing is one of the most essential tasks in the field of deep learning. Along with NLP, speech process-\n",
      "ing has also gained attention from researchers, leading to the application of deep neural network methods. As transformers\n",
      "have achieved great success in the field of NLP, researchers have also had significant success when applying transformer-based\n",
      "models to speech processing.\n",
      "6.5.1\n",
      "SPEECH RECOGNITION\n",
      "Speech Recognition is one of the most popular tasks in the field of artificial intelligence. It is the ability of a model to identify\n",
      "human speech and convert it into a textual or written format. This process is also known as speech-to-text, automatic speech\n",
      "recognition, or computer-assisted transcription. Speech recognition technology has advanced significantly in the last few\n",
      "years. It involves two types of models, namely the acoustic model and the language model. Several features contribute to the\n",
      "effectiveness of speech recognition models, including language weighting, speaker labeling, acoustics training, and profanity\n",
      "filtering. Despite significant advancements in speech recognition technology, there is still room for further improvement (Yu\n",
      "& Deng, 2016, Nassif et al., 2019, Deng et al., 2013). One promising development in this area is the use of transformer-\n",
      "based models, which have shown significant improvement in various stages of the speech recognition task. The majority\n",
      "of transformer-based audio/speech processing models have focused on speech recognition tasks, and among these, several\n",
      "models have made exceptional contributions, exhibited high levels of accuracy, introduced new effective ideas, or created\n",
      "buzz in the AI field. In Table 23, we highlight the most significant transformer models for speech recognition tasks.\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "Conformer (Peng\n",
      "et al., 2021)\n",
      "Speech\n",
      "Recognition\n",
      "2020\n",
      "Encoder &\n",
      "Decoder\n",
      "No\n",
      "NA\n",
      "Librispeech, test/testother.\n",
      "Speech Trans-\n",
      "former (Dong\n",
      "et al., 2018)\n",
      "Speech\n",
      "Recognition\n",
      "2018\n",
      "Encoder &\n",
      "Decoder\n",
      "No\n",
      "NA\n",
      "Wall StreetJournal (WSJ),\n",
      "NVIDIA K80 G-PU\n",
      "VQ-Wav2vec\n",
      "(Baevski\n",
      "et al., 2020a)\n",
      "Speech\n",
      "Recognition\n",
      "2020\n",
      "Encoder\n",
      "Yes\n",
      "Librispeech\n",
      "TIMIT, WSJ-\n",
      "Wall StreetJournal\n",
      "Wav2vec\n",
      "2.0 (Baevski\n",
      "et al., 2020b)\n",
      "Speech\n",
      "Recognition\n",
      "2020\n",
      "Encoder\n",
      "Yes\n",
      "Librispeech,\n",
      "LibriVox\n",
      "Librispeech, Lib-\n",
      "riVox, TIMIT\n",
      "HuBERT (Hsu\n",
      "et al., 2021)\n",
      "Speech\n",
      "Recognition\n",
      "2021\n",
      "Encoder\n",
      "Yes\n",
      "Librispeech,\n",
      "Libri-light\n",
      "Librispeech(train-clean),\n",
      "Libri-light(train-clean)\n",
      "BigSSL (Zhang\n",
      "et al., 2022)\n",
      "Speech\n",
      "Recognition\n",
      "2022\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "wav2vec 2.0,\n",
      "YT-U, Libri-Light\n",
      "YouTube, English(US)\n",
      "Voice search(VS), Speech-\n",
      "Stew, LibriSpeech,\n",
      "CHiME6, Telephony\n",
      "\n",
      "\n",
      "## Page 40\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Whisper (Radford\n",
      "et al., 2022)\n",
      "Speech recogni-\n",
      "tion, Translation,\n",
      "Language\n",
      "Identification\n",
      "2022\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "Not Mentioned\n",
      "VoxLingua107, Lib-\n",
      "riSpeeech, CoVoST2,\n",
      "Fleurs, Kincaid46\n",
      "Transformer\n",
      "Transducer\n",
      "(Zhang\n",
      "et al., 2020b)\n",
      "Speech\n",
      "recognition\n",
      "2020\n",
      "Encoder\n",
      "No\n",
      "NA\n",
      "LibriSpeeech\n",
      "XLSR-Wav2Vec2\n",
      "(Conneau\n",
      "et al., 2021)\n",
      "Speech\n",
      "recognition\n",
      "2020\n",
      "Encoder\n",
      "Yes\n",
      "53 languages\n",
      "datasets\n",
      "CommonVoice, BA-\n",
      "BEL,Multilingual\n",
      "LibriSpeech(MLS)\n",
      "Table 23: Transformer models for audio & speech recognition task\n",
      "• Conformer: The Conformer architecture is a model that combines the advantages of both the Transformer and convolutional\n",
      "neural network (CNN) for automatic speech recognition tasks. While the Transformer is proficient at capturing global\n",
      "features, and CNN excels at capturing local features, the Conformer architecture leverages the strengths of both to achieve\n",
      "superior performance. Recent studies have shown that the Conformer architecture outperforms both CNN and Transformer\n",
      "models individually, thereby setting a new state-of-the-art in automatic speech recognition performance (Gulati et al., 2020).\n",
      "• Speech Transformer: It is a speech recognition model published in 2018 which is one of the earliest Transformer inspired\n",
      "speech models. It eliminates the conventional Recurrent Neural Network (RNN)-based approach in speech processing and\n",
      "instead applies an attention mechanism. The introduction of the Transformer model into speech recognition has led to a\n",
      "number of benefits. For instance, the training time and memory usage become lower, allowing for better scalability. This is\n",
      "especially helpful for tasks that require a long-term dependency due to the elimination of recurrence sequence-to-sequence\n",
      "processing (Dong et al., 2018).\n",
      "• Wav2vec 2.0: Wav2Vec 2.0 is a self-supervised model which uses discretization algorithms to capture the vocabulary from\n",
      "raw speech representation. The learned vocabulary is passed through an architecture consisting of multi-layer convolutional\n",
      "feature encoder. This encoder has multiple convolution layers, layer normalization and an activation function, with the\n",
      "audio representations being masked during the training process. Wav2Vec 2.0 offers the advantage of performing well on\n",
      "speech recognition tasks with a small amount of supervised data (Baevski et al., 2020b).\n",
      "• VQ-Wav2vec: VQ-Wav2Vec is a Transformer based model that enables Vector Quantization-VQ tasks to be executed in a\n",
      "self-supervised way. It is built on the Wav2Vec model, which is an effective way of compressing continuous signals into\n",
      "discrete symbols. Unlike other models, VQ-Wav2Vec trains without the need of unlabeled data. Instead, corrupted speech\n",
      "is used, and the model learns by predicting the missing parts of the speech. This process of training has been proven to be\n",
      "highly effective, and the model is capable of achieving a higher accuracy when compared to other models (Baevski et al.,\n",
      "2020a).\n",
      "• BigSSL: BigSSL is a large-scale semi-supervised Transformer-based model designed specifically for speech recognition\n",
      "tasks. Obtaining labeled speech data is a challenging and time-consuming process, while the availability of unlabeled data\n",
      "is considerably vast. In this context, the BigSSL model proposes a novel approach to enhance the performance of speech\n",
      "recognition tasks. By leveraging a smaller portion of labeled data in conjunction with a substantial amount of unlabeled\n",
      "data, this model achieves improved performance. Furthermore, the utilization of a larger quantity of unlabeled data helps\n",
      "alleviate the overfitting issue, thereby further enhancing the overall performance of the BigSSL model (Zhang et al., 2022).\n",
      "• HuBERT: HuBERT, short for Hidden-Unit BERT, is a self-supervised speech representation model. Its approach involves\n",
      "offline clustering for feature representation, with the loss calculation restricted to the masked regions. This emphasis allows\n",
      "the model to effectively learn a combination of acoustic and language models over the input data. HuBERT consists of a\n",
      "convolutional waveform encoder, a projection layer, a BERT encoder, and a code embedding layer. The CNN component\n",
      "generates feature representations, which are then subjected to random masking. These masked representations are subse-\n",
      "quently passed through the BERT encoder, yielding another set of feature representations. HuBERT’s functioning resembles\n",
      "that of a mask language model and has demonstrated notable performance in speech representation tasks, particularly speech\n",
      "recognition (Hsu et al., 2021).\n",
      "• Transformer Transducer:\n",
      "It is a speech recognition model that capitalizes on the strengths of both the self-attention\n",
      "mechanism of the Transformer and the recurrent neural network (RNN). This model is constructed by integrating the en-\n",
      "coder module of the transformer with the RNN-T loss function. The encoder module is responsible for extracting speech\n",
      "representations, while the RNN-T component utilizes this information to make real-time predictions of the transcript, fa-\n",
      "cilitating a swift response—an essential requirement for speech recognition tasks (Transformer-Transducer (Zhang et al.,\n",
      "2020b).\n",
      "• Whisper: It is a noteworthy speech recognition model that emerged in late 2022, specifically designed to address the\n",
      "challenging task of recognizing speech with low volume. The uniqueness of this model lies in its dedicated efforts to\n",
      "\n",
      "\n",
      "## Page 41\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "improve low-volume speech recognition. Whisper adopts a training approach that incorporates a lower level of speech\n",
      "data and leverages weak supervision methods, enabling training on a larger corpus of data. This strategic approach has\n",
      "proven instrumental in enhancing the performance of the Whisper model, enabling it to effectively capture and comprehend\n",
      "low-level speech phenomena (Radford et al., 2022).\n",
      "• XLSR-Wav2Vec2: This model demonstrates the capability to recognize speech across multiple languages, eliminating\n",
      "the need for extensive labeled data in each language for training. By learning the relationships and shared characteristics\n",
      "among different languages, this model surpasses the requirement of training on specific language-labeled speech data.\n",
      "Consequently, the XLSR-Wav2Vec2 model offers an efficient solution for multiple-language speech recognition, requiring\n",
      "significantly less data for training while adhering to the architectural principles of Wav2Vec2 (Conneau et al., 2021).\n",
      "6.5.2\n",
      "SPEECH SEPARATION\n",
      "It poses a considerable challenge within the field of audio signal processing. It involves the task of separating the desired\n",
      "speech signal, which may include various sources such as different speakers or human voices, from additional sounds such\n",
      "as background noise or interfering sources. In the domain of speech separation, three commonly employed methods are\n",
      "followed: (i) Blind source separation, (ii) Beamforming, and (iii) Single-channel speech separation. The significance of\n",
      "speech separation has grown with the increasing popularity of automatic speech recognition (ASR) systems. It is often\n",
      "employed as a preprocessing step for speech recognition tasks. The accurate distinction between the desired speech signal\n",
      "and unwanted noise is crucial to ensure precise speech recognition results. Failure to properly segregate the desired speech\n",
      "from interfering noise can lead to erroneous speech recognition outcomes (Wang & Chen, 2018, Huang et al., 2014). In this\n",
      "context, we present several Transformer-based models that have showcased noteworthy advancements in audio and speech\n",
      "separation tasks. The details of these models are presented in Table 24.\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "DPTNeT (Chen\n",
      "et al., 2020a)\n",
      "Speech Sep-\n",
      "aration\n",
      "2020\n",
      "Encoder &\n",
      "Decoder\n",
      "No\n",
      "NA\n",
      "WSJ0-2mix, LS-2mix\n",
      "Sepformer\n",
      "(Subakan\n",
      "et al., 2021)\n",
      "Speech Sep-\n",
      "aration\n",
      "2021\n",
      "Encoder &\n",
      "Decoder\n",
      "No\n",
      "NA\n",
      "WSJ0-2mix, WSJ0-3mix\n",
      "WavLM (Chen\n",
      "et al., 2022a)\n",
      "Speech sepa-\n",
      "ration, speech\n",
      "denoising, speech\n",
      "prediction,\n",
      "Speaker Verifi-\n",
      "cation, Speech\n",
      "recognition\n",
      "2022\n",
      "Encoder\n",
      "Yes\n",
      "GigaSpeech,\n",
      "VoxPopuli\n",
      "VoxCeleb1, Vox-\n",
      "Celeb2, Switchboard-2\n",
      "CALLHOME, LIB-\n",
      "RICSS, LibriSpeech\n",
      "Table 24: Transformer models for audio & speech separation task\n",
      "• Sepformer: The sepformer model was published in a paper titled “Attention is all you need in speech separation” which\n",
      "uses an attention mechanism to separate speeches that are overlapped. This model does not contain any kind of recurrence\n",
      "scheme and it follows the self-attention mechanisms. Sepformer uses a binary mask prediction scheme for training while\n",
      "this masking network captures both short and long-term dependencies and provide higher accuracy in performance (Subakan\n",
      "et al., 2021).\n",
      "• DPTNeT: DPTNeT stands for Dual-Path Transformer Network for monaural speech separation tasks. This model trained\n",
      "directly minimizes the error between estimated and target value which is called end-to-end processing. This model uses\n",
      "dual-path architecture, replacing positional encoding with the RNN in the transformer architecture which helps to capture\n",
      "complex features of the signal and improves the performance of the speech separation from the overlapped speech (Chen\n",
      "et al., 2020a).\n",
      "• WavLM: WavLM is a large-scale pre-trained model that can execute a range of tasks for speech. WavLM follows BERT\n",
      "inspired speech processing model-HuBERT, whereas, with the help of mask speech prediction, the model predicts the actual\n",
      "speech by removing the noise from the corrupted speech. By this way, this model is trained for a variety of tasks besides\n",
      "automatic speech recognition-ASR task (Chen et al., 2022a).\n",
      "6.5.3\n",
      "SPEECH CLASSIFICATION\n",
      "The speech classification task refers to the ability to categorize input speech or audio into distinct categories based on various\n",
      "features, including speaker, words, phrases, language, and more. There exist several speech classifiers, such as voice activ-\n",
      "ity detection (binary/multi-class), speech detection (multi-class), language identification, speech enhancement, and speaker\n",
      "\n",
      "\n",
      "## Page 42\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "identification. Speech classification plays a crucial role in identifying important speech signals, enabling the extraction of\n",
      "relevant information from large speech datasets (Livezey et al., 2019, Gu et al., 2017). In this context, we present a compi-\n",
      "lation of transformer-based models, which have demonstrated superior accuracy in speech classification tasks compared to\n",
      "conventional models. The details of these models are depicted in Table 25.\n",
      "Transformer\n",
      "Models\n",
      "Task Ac-\n",
      "complished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "AST (Gong\n",
      "et al., 2021)\n",
      "Audio Clas-\n",
      "sification\n",
      "2021\n",
      "Encoder\n",
      "Yes\n",
      "ImageNeT\n",
      "AudioSet, ESC-50,\n",
      "Speech Commands\n",
      "Mockingjay\n",
      "(Liu et al., 2020)\n",
      "Speech Clas-\n",
      "sification &\n",
      "recognition\n",
      "2020\n",
      "Encoder\n",
      "Yes\n",
      "LibriSpeech\n",
      "LibriSpeech test\n",
      "clean, MOSEI\n",
      "XLS-R (Babu\n",
      "et al., 2022)\n",
      "Speech Clas-\n",
      "sification,\n",
      "Speech Trans-\n",
      "lation, Speech\n",
      "Recognition\n",
      "2021\n",
      "Encoder &\n",
      "Decoder\n",
      "Yes\n",
      "VoxPopuli (VP-\n",
      "400K), Multilingual\n",
      "Librispeech\n",
      "(MLS), Common-\n",
      "Voice, VoxLin-\n",
      "gua107, BABEL\n",
      "VoxPopuli, Multilingual\n",
      "Librispeech (MLS),\n",
      "CommonVoice, BABEL\n",
      "UniSpeech-\n",
      "SAT (Chen\n",
      "et al., 2022b)\n",
      "Speech Clas-\n",
      "sification &\n",
      "Recognition\n",
      "2022\n",
      "Encoder\n",
      "Yes\n",
      "LibriVox, Lib-\n",
      "rispeech Gi-\n",
      "gaSpeech, VoxPopuli\n",
      "SUPERB\n",
      "Table 25: Transformer models for audio & speech classification task\n",
      "• AST: AST - Audio Spectrogram Transformer is a transformer-based model which is applied to an audio spectrogram. AST\n",
      "is the first audio classification model where the convolution was not used and it is capable of capturing long-range frames\n",
      "context. It used a transformer encoder to capture the features in the audio spectrogram, a linear projection layer and sigmoid\n",
      "activation function to capture the audio spectrogram representation for audio classification. As the attention mechanism is\n",
      "renowned for capturing global features so it shows significant performance in audio/speech classification tasks (Gong et al.,\n",
      "2021).\n",
      "• Mockingjay: Mockingjay is an unsupervised speech representation model that uses multiple layers of bidirectional Trans-\n",
      "former pre-trained encoders. It uses both past and future features for speech representation rather than only past information,\n",
      "which helps it to gather more information about the speech context. Mockingjay also can improve the performance of the\n",
      "supervised learning tasks as well where the amount of labeled data is low. Capturing more information helped to improve\n",
      "several speech representational tasks like speech classification and recognition (Liu et al., 2020).\n",
      "• XLS-R: XLS-R is a transformer-based self-supervised large-scale speech representation model which is trained with a large\n",
      "amount of data. It is built on the wav2vec discretization algorithm, whereas it uses Wav2Vec 2.0 model that is pretrained\n",
      "with multiple languages. The architecture contains multiple convolution encoders to map raw speech and the output from\n",
      "this stage is transferred to the transformer model(encoder module) as input which provides better audio representation\n",
      "finally. A large amount of training data is crucial for this model where a range of public speech is used and it performed\n",
      "well for multiple downstream multilingual speech tasks (Babu et al., 2022).\n",
      "• UniSpeech: UniSpeech is a semi-supervised unified pre-trained model for speech representation. This model follows\n",
      "the Wav2Vec 2.0 architecture where it contains convolutional feature encoders that converts the raw audio to a higher-\n",
      "dimensional representation and this output is fed into the Transformer. This model is capable of learning to multitask while\n",
      "a quantizer is used in its architecture which helps to capture specific speech recognition information (Chen et al., 2022b).\n",
      "6.6\n",
      "SIGNAL PROCESSING\n",
      "With the growing recognition of the usability of Transformer-based models across various sectors, researchers have started\n",
      "exploring their application in signal processing. This recent development of utilizing Transformer-based models in signal\n",
      "processing represents a novel approach that outperforms conventional methods in terms of performance. Signal processing\n",
      "involves the manipulation and analysis of various types of data, including signal status, information, frequency, amplitude,\n",
      "and more. While audio and speech are considered forms of signals, we have segregated the audio and speech sections to\n",
      "highlight the specific applications of Transformer-based models in those domains. Within the signal processing domain, we\n",
      "have focused on two distinct areas: wireless network signal processing and medical signal processing. These two fields\n",
      "exhibit distinct processing methods and functionalities due to their inherent differences. Here, we delve into both of these\n",
      "tasks and provide an overview of significant Transformer-based models that have demonstrated effectiveness in these specific\n",
      "domains.\n",
      "\n",
      "\n",
      "## Page 43\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "6.6.1\n",
      "WIRELESS NETWORK & SIGNAL PROCESSING\n",
      "In the current era of the 21st century, wireless network communication has emerged as a prominent technology. However, the\n",
      "application of transformers in wireless network signal processing has not received substantial attention thus far. Consequently,\n",
      "the number of Transformer-inspired models developed for this field remains limited. Wireless network signal processing\n",
      "encompasses various tasks, including signal denoising, signal interface detection, wireless signal channel estimation, interface\n",
      "identification, signal classification, and more. Deep neural networks offer great potential for tackling these tasks effectively,\n",
      "and Transformer-based models have introduced significant advancements in this domain (Sun et al., 2017, Clerckx et al.,\n",
      "2021, Zhang et al., 2019, Chen et al., 2019b). In this section, we present several models that have made notable contributions\n",
      "to the enhancements in wireless communication networks and signal processing. The details of these models are provided in\n",
      "Table 26.\n",
      "Transformer\n",
      "Models\n",
      "Task Accomplished\n",
      "Year\n",
      "Architecture\n",
      "(Encoder/\n",
      "Decoder)\n",
      "Pre-\n",
      "trained\n",
      "(Yes/NO)\n",
      "Pre-training\n",
      "Dataset\n",
      "Dataset (Fine-tuning,\n",
      "Training, Testing)\n",
      "SigT (Ren\n",
      "et al., 2022)\n",
      "Signal detection,\n",
      "channel estimation,\n",
      "interference\n",
      "suppression, and\n",
      "data decoding in\n",
      "MIMO-OFDM\n",
      "2022\n",
      "Encoder\n",
      "No\n",
      "NA\n",
      "Peng Cheng labora-\n",
      "tory(PCL), local area data\n",
      "TSDN (Liu\n",
      "et al., 2022b)\n",
      "Remove Interference\n",
      "and nose from\n",
      "wireless signal\n",
      "2022\n",
      "Encoder &\n",
      "Decoder\n",
      "No\n",
      "NA\n",
      "Wall NLoS, Foil\n",
      "NLOS, UWB dataset\n",
      "ACNNT\n",
      "(Wang et al.,\n",
      "2021a)\n",
      "Wireless interface\n",
      "identification\n",
      "2021\n",
      "Encoder\n",
      "No\n",
      "NA\n",
      "ST, BPSK, AM, NAM,\n",
      "SFM, LFM, 4FSK,\n",
      "2FSK signal dataset\n",
      "MCformer\n",
      "(Hamidi-Rad\n",
      "& Jain, 2021)\n",
      "Automatic modula-\n",
      "tion classification\n",
      "complex raw\n",
      "radio signals\n",
      "2021\n",
      "Encoder\n",
      "No\n",
      "NA\n",
      "RadioML2016.10b\n",
      "Quan-\n",
      "Transformer\n",
      "(Xie et al.,\n",
      "2022)\n",
      "Compress &\n",
      "recover channel\n",
      "state information\n",
      "2022\n",
      "Encoder &\n",
      "Decoder\n",
      "No\n",
      "NA\n",
      "CsiNet, CLNet and CRNet\n",
      "Table 26: Transformer models for wireless network & signal processing\n",
      "• SigT: SigT is a wireless communication network signal receiver designed with a transformer architecture, capable of han-\n",
      "dling Multiple-input Multiple-output (MIMO-OFDM) signals. Leveraging the transformer’s encoder module, this innova-\n",
      "tive framework enables parallel data processing and performs essential tasks such as signal detection, channel estimation,\n",
      "and data decoding. Unlike traditional receivers that rely on distinct modules for each task, SigT seamlessly integrates these\n",
      "functions, providing a unified solution (Ren et al., 2022).\n",
      "• TSDN: TSDN is an abbreviation for Transformer-based Signal Denoising Network. It refers to a signal denoising model\n",
      "based on transformers that aims to estimate the Angle-of-Arrival (AoA) of signals transmitted by users within a wireless\n",
      "communication network. This transformer-based model significantly enhances the accuracy of AoA estimation, especially\n",
      "in challenging non-line-of-sight (NLoS) environments where conventional methods often fall short in delivering the desired\n",
      "precision (Liu et al., 2022b).\n",
      "• ACNNT: The Augmented Convolution Neural Network with Transformer (ACNNT) is an architectural framework specif-\n",
      "ically designed for identifying interference within wireless networks. This model combines the power of Convolutional\n",
      "Neural Networks (CNN) and transformer architectures. The multiple CNN layers in ACNNT extract localized features\n",
      "from the input signal, while the transformer component captures global relationships between various elements of the input\n",
      "sequence. By exploiting the strengths of both CNN and Transformer, this model has demonstrated superior accuracy in the\n",
      "identification of wireless interference compared to conventional approaches (Wang et al., 2021a).\n",
      "• MCformer: MCformer, short for Modulation Classification Transformer, refers to a model architecture based on transform-\n",
      "ers that performs feature extraction from input signals and subsequently classifies them based on modulation. This architec-\n",
      "tural design combines Convolutional Neural Network (CNN) and self-attention layers, enabling the processing of intricate\n",
      "features within the signal and achieving superior accuracy in comparison to conventional approaches. The introduction\n",
      "of this model has brought about noteworthy advancements in a wireless network and communication signals, particularly\n",
      "\n",
      "\n",
      "## Page 44\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "in the realm of Automatic Modulation Classification, thereby enhancing system security and performance (Hamidi-Rad &\n",
      "Jain, 2021).\n",
      "• Quan-Transformer: It refers to a Transformer-based model specifically designed to perform quantization in wireless\n",
      "communication systems. Quantization is the vital process of converting a continuous signal into a discrete signal, and it\n",
      "plays a crucial role in network channel feedback processing. Channel feedback processing is essential for estimating channel\n",
      "state information, which in turn aids in adjusting signal transmission parameters. This feedback mechanism holds particular\n",
      "significance in the context of Reconfigurable Intelligent Surface (RIS)-aided wireless networks, a critical component of the\n",
      "6th-generation communication system (Xie et al., 2022).\n",
      "6.6.2\n",
      "MEDICAL SIGNAL PROCESSING\n",
      "The rise of healthcare data has resulted in the rapid growth of deep learning applications, enabling the automatic detection of\n",
      "pathologies, enhanced medical diagnosis, and improved healthcare services. These data can be categorized into three distinct\n",
      "forms: relational data ( symptoms, examinations, and laboratory tests), medical images, and biomedical signals (consisting of\n",
      "raw electronic and sound signals). While the application of deep learning models, particularly transformers, in the context of\n",
      "medical images has gained considerable attention and yielded promising results, the application of transformers to biomedical\n",
      "signals is still in its early stages. The majority of relevant studies have been published between the years 2021 and 2022,\n",
      "with a particular focus on the task of signal classification. We have summarized our findings regarding the application of\n",
      "transformers to biomedical signals in Table 27.\n",
      "• Epilepsy disease case:\n",
      "Epilepsy is a serious debilitating condition for those it affects. Typically, its symptoms are detected through the use\n",
      "of electrical signals, such as electroencephalograms (EEGs) and magnetoencephalograms (MEGs). With the rise of\n",
      "deep learning, it is now possible to detect and predict epilepsy cases using its architecture. Utilizing transformers as\n",
      "their underlying framework, the following models have been developed to analyze electric signals for predicting and\n",
      "classifying epilepsy.\n",
      "– Three-tower transformer network (Yan et al., 2022a):\n",
      "The purpose of this model is to predict epileptic\n",
      "seizures from EEG signals. A transformer-based model is used to perform binary classification of EEG signals\n",
      "based on three EEG features: time, frequency, and channel. This model processes EEG signals as a whole using\n",
      "a model that is based on the classic transformer, which contains three encoders: a time encoder, a frequency\n",
      "encoder, and a channel encoder. Compared to other models, such as CNN, the model shows better performance\n",
      "results in predicting epilepsy attacks.\n",
      "– TransHFO (Guo et al., 2022b): (Transformer-based HFO) is a deep learning model based on BERT for clas-\n",
      "sifying High-Frequency Oscillation (HFO) from normal control (NC). A transformer is used to detect the pres-\n",
      "ence of HFO with high accuracy in one-dimensional magnetoencephalography (MEG) data in order to identify\n",
      "epileptic areas more precisely. Signal classification is performed under k-fold cross-validation and through\n",
      "various algorithms, including logistic regression, SMO, and the ResDen model. Due to the small dataset avail-\n",
      "able, the authors propose to use the data augmentation technique “ADASYN”. Nevertheless, even with the\n",
      "addition of data, the dataset remains small, which makes the shallow transformer more efficient than the deep\n",
      "transformer with more layers.\n",
      "\n",
      "\n",
      "## Page 45\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Transformer Name\n",
      "Field of\n",
      "application\n",
      "Year\n",
      "Fully Trans-\n",
      "former\n",
      "Architecture\n",
      "Signal\n",
      "type\n",
      "Transformer\n",
      "Task\n",
      "Dataset\n",
      "Three-tower trans-\n",
      "former network\n",
      "(Yan et al., 2022a)\n",
      "Epilepsy\n",
      "2022\n",
      "YES\n",
      "EEG\n",
      "Classification\n",
      "of EEG signals\n",
      "CHB-MIT datasets\n",
      "TransHFO (Guo\n",
      "et al., 2022b)\n",
      "Epilepsy\n",
      "2022\n",
      "YES\n",
      "MEG\n",
      "Classification\n",
      "of MEG\n",
      "signals\n",
      "20 clinical patients\n",
      "TCN and Transformer-\n",
      "based model\n",
      "(Casal et al., 2022)\n",
      "Sleep\n",
      "pathologies\n",
      "2022\n",
      "No (using\n",
      "TCN which is\n",
      "based on CNN)\n",
      "Cardiac\n",
      "signals\n",
      "( Heart\n",
      "Rate)\n",
      "Classification\n",
      "of sleep stages\n",
      "Sleep Heart Health\n",
      "Study dataset\n",
      "Constrained trans-\n",
      "former network\n",
      "(Che et al., 2021)\n",
      "Heart disease\n",
      "2021\n",
      "No (Using CNN)\n",
      "ECG\n",
      "Classification\n",
      "of ECG signals\n",
      "6877 patients\n",
      "CRT-NET (Liu\n",
      "et al., 2022a)\n",
      "Heart Disease\n",
      "2022\n",
      "No (Using\n",
      "CNN and Bi-\n",
      "directional GRU)\n",
      "ECG\n",
      "Classification\n",
      "and recognition\n",
      "of ECG signals\n",
      "MIT-BIH, CPSC\n",
      "arrhythmia clin-\n",
      "ical private data\n",
      "CAT (Yang\n",
      "et al., 2022)\n",
      "Atrial\n",
      "Fibrillation\n",
      "2022\n",
      "No (Using MLP)\n",
      "ECG\n",
      "Classification\n",
      "of ECG signals\n",
      "Shaoxing database\n",
      "(more than\n",
      "10000 patients)\n",
      "Table 27: Transformer models for medical signal processing\n",
      "• Cardiac diseases cases: heart diseases are among the areas in which researchers are interested in applying trans-\n",
      "formers. Using ECG signals, a transformer model can detect long-range dependencies and identify heart disease\n",
      "types based on their characteristics.\n",
      "– Constrained transformer network (Che et al., 2021): Using CNN and transformer architecture, this model\n",
      "classifies heart arrhythmia disease based on temporal information in ECG (electrocardiogram) signals. There\n",
      "are also other models that use transformer encoders for classifying heart diseases (such as atrial fibrillation)\n",
      "using ECG signals, such as CRT-NET (Liu et al., 2022a) and CAT (Yang et al., 2022) “Component-Aware\n",
      "Transformer”.\n",
      "A major advantage of CAT’s model (Yang et al., 2022) is the use of a large database containing data from over\n",
      "ten thousand patients for experiments. In contrast, the strength of CRT-NET (Liu et al., 2022a) is the ability to\n",
      "extract different ECG features like waveforms, morphological characteristics, and time domain data, in order to\n",
      "identify many cardiovascular diseases.\n",
      "– TCN and Transformer-based model (Casal et al., 2022): An automatic sleep stage classification system\n",
      "based on 1-dimensional cardiac signals (Heart Rate). The classification is conducted in two steps: extracting\n",
      "features from signals using TCN ”Temporal Convolution Network”[], and modeling signal sequence dependen-\n",
      "cies using the standard Transformer architecture consisting of two stacks of encoders and a simplified decoder\n",
      "module. Based on a dataset of 5000 different participants, this study demonstrated that this new model outper-\n",
      "forms other networks, such as CNN and RNN, which consume more memory and reduce process efficiency.\n",
      "7\n",
      "FUTURE PROSPECTS AND CHALLENGES\n",
      "One of the primary objectives of this survey is to identify and highlight potential research directions for transformer applica-\n",
      "tions, with the goal of expanding their range of applications beyond the currently popular fields of NLP and computer vision.\n",
      "Despite considerable research attention in these areas, there are still a number of areas that remain relatively unexplored with\n",
      "the potential for significant improvements in the future. In order to expand the application areas of Transformers, we have\n",
      "identified several potential directions for future research. These directions include but are not limited to the exploration of\n",
      "transformer-based models for speech recognition, recommendation systems, and natural language generation. In addition,\n",
      "further exploration of transformer-based approaches for multimodal tasks, such as combining audio and visual inputs, would\n",
      "be an interesting direction for future research. By pursuing these research directions, we hope to continue the advancement\n",
      "of transformer-based models and their utility in a broader range of applications.\n",
      "7.1\n",
      "TRANSFORMERS IN WIRELESS NETWORK AND CLOUD COMPUTING\n",
      "While the majority of transformer applications have been in NLP and Computer Vision, there is an exciting potential for\n",
      "transformers in the wireless communication and cloud computing domains. Although there have been relatively fewer studies\n",
      "\n",
      "\n",
      "## Page 46\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "in this area, the ones that exist have demonstrated the enormous potential of transformers for improving various aspects of\n",
      "wireless signal communication and cloud workload computing. In this section, we discuss some of the transformer models\n",
      "that have been developed for wireless signal communication and cloud computing. These models have shown promising\n",
      "results in areas such as wireless interference recognition, wireless signal communication mitigation, and cloud workload\n",
      "forecasting. Moving forward, there are several potential directions for future research in both the wireless network and cloud\n",
      "domains. Some of the possible areas of focus for wireless communication include improving network security, enhancing the\n",
      "efficiency of wireless communication, and developing more accurate interference recognition models. On the other hand, for\n",
      "cloud computing, future work could focus on improving resource allocation and workload management, optimizing cloud\n",
      "performance, and enhancing data privacy and security.\n",
      "Scope of future work for the wireless Signal communication:\n",
      "• Detection of Wireless Interference: The global attention capabilities of Transformers present an exciting avenue for\n",
      "future research in the field of wireless signal communication. By leveraging the power of Transformers, researchers\n",
      "can explore and develop more widely used applications for detecting wireless interference in communication sys-\n",
      "tems. This can involve experimenting with various successive Transformer models to reduce complexity and enhance\n",
      "the efficiency of wireless interference recognition. This research can lead to improved communication systems that\n",
      "are more resilient to interference and provide better overall performance.\n",
      "• Enhancing 5G & 6G Networks: As 5G and 6G networks gain popularity, there is significant potential for Transform-\n",
      "ers to contribute to this field. Advanced networking architectures, such as Reconfigurable Intelligent Surfaces (RIS)\n",
      "and Multiple-Output and Orthogonal Frequency-Division Multiplexing (MIMO-OFDM), play a crucial role in these\n",
      "networks. Transformers have shown promise in improving performance in these areas. Additionally, signal state\n",
      "feedback, which is essential for adjusting and updating networks based on signal state changes, can benefit from\n",
      "the parallel computational capability of Transformers. The ability of Transformers to handle multiple tasks simul-\n",
      "taneously, including signal detection, channel estimation, and data decoding, makes them an effective alternative to\n",
      "conventional methods.\n",
      "• Integration of Transformers with Advanced Communication Technologies: Transformers can be integrated with\n",
      "other advanced communication technologies to further improve wireless signal communication.\n",
      "For example,\n",
      "combining Transformers with technologies like Massive MIMO, millimeter-wave communication, and cognitive\n",
      "radio can enhance the performance, capacity, and spectrum efficiency of wireless networks. Future research can\n",
      "focus on exploring these synergies and developing innovative solutions that leverage the unique capabilities of\n",
      "Transformers in conjunction with other cutting-edge communication technologies.\n",
      "Future possibilities for the Cloud:\n",
      "• Advancements in Cloud Computing: With the increasing application of the Internet of Things (IoT), the cloud plays\n",
      "a crucial role in supporting and managing IoT devices. Transformers offer exciting possibilities for advancing cloud\n",
      "capabilities in various tasks, such as early attack and anomaly detection. By leveraging different Transformer ap-\n",
      "proaches, the cloud can learn and adapt to its behavior, bringing more stability and security. Additionally, Transform-\n",
      "ers can be applied to cloud computing tasks like task scheduling and memory allocation. The multi-head attention\n",
      "and long-range attention features of the Transformers model make it well-suited for optimizing resource allocation\n",
      "and improving overall performance in cloud environments.\n",
      "• Transformation in Mobile Edge Computing (MEC) and Mobile Edge Caching (MEC): In the context of advanced\n",
      "6G networking systems, Mobile Edge Computing (MEC) and Mobile Edge Caching (MEC) play vital roles in\n",
      "reducing communication latency. Transformers have demonstrated significant potential in enhancing MEC and MEC\n",
      "through their parallel computational capabilities. Transformers can be applied to predict popular content, improve\n",
      "content management, optimize resource allocation, and enhance data transmission in MEC systems. By leveraging\n",
      "Transformers, the mobile cloud can respond and process user requests faster, resulting in reduced network response\n",
      "times and faster data transmission.\n",
      "• Intelligent Resource Management in the Cloud: Transformers offer opportunities for intelligent resource manage-\n",
      "ment in cloud environments. By applying Transformers to tasks like workload prediction, resource allocation, and\n",
      "load balancing, cloud systems can optimize resource utilization and enhance performance. Transformers’ ability\n",
      "to capture long-range dependencies and handle complex patterns makes them well-suited for efficiently managing\n",
      "cloud resources and improving overall system efficiency.\n",
      "• Security and Privacy in the Cloud: Transformers can contribute to enhancing security and privacy in the cloud by\n",
      "enabling advanced threat detection, anomaly detection, and data privacy protection mechanisms. Transformers can\n",
      "analyze large volumes of data, identify patterns, and detect potential security breaches or anomalies in real-time.\n",
      "Additionally, Transformers can be utilized for data anonymization and privacy-preserving computations, ensuring\n",
      "that sensitive information remains protected in cloud-based systems.\n",
      "\n",
      "\n",
      "## Page 47\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "7.2\n",
      "MEDICAL IMAGE & SIGNAL PROCESSING\n",
      "Two types of medical data are discussed in this paper: images and signals. According to our literature review, segmentation,\n",
      "and classification are the most transformer-based medical applications, followed by image translation (Yan et al., 2022b, Chen\n",
      "et al., 2022c). In the context of medical images, we commonly see the reuse of existing transformers, such as BERT, ViT, and\n",
      "SWIN, regardless of how the original model was modified. Further, we observe that various types of medical images are used\n",
      "to conduct transformer-based medical applications, such as 2D images (He et al., 2022, Gu et al., 2022), 3D images (Jiang\n",
      "et al., 2022b, Liang et al., 2022, Zhou et al., 2021a, Zhu et al., 2022) and multi-mode images (Sun et al., 2021b).\n",
      "The selected research papers for this survey span the years 2021 to 2022, indicating that the use of transformer architecture\n",
      "in the medical field is still in its nascent stages. Despite its early adoption, there has been a remarkable influx of excellent\n",
      "publications exploring transformer applications in the analysis of medical images within this relatively short period. However,\n",
      "several challenges persist in applying transformers to medical images that need to be addressed and overcome:\n",
      "• Limited focus on 3D images: There is a scarcity of studies that specifically address the application of transformers\n",
      "to 3D medical images. Most research has been concentrated on 2D images, indicating the need for further exploration\n",
      "and development in this area.\n",
      "• Small and private medical image databases: Medical image databases are often small and privately owned due to\n",
      "legal and ethical concerns regarding patient data privacy (L´opez-Linares et al., 2020). This limits the availability of\n",
      "large-scale datasets necessary for training transformer models effectively.\n",
      "• Computational complexity in high-resolution imaging: Transformer-based architectures encounter computational\n",
      "challenges when dealing with high-resolution medical images. The self-attention mechanism, which is integral to\n",
      "transformers, becomes computationally demanding for large images. However, some models, like DI-UNET (Wu\n",
      "et al., 2022), have introduced enhanced self-attention mechanisms to handle higher resolution images effectively.\n",
      "• Limited number of fully developed transformer-based models: The development of transformer-based models\n",
      "for processing medical images is still relatively nascent. Due to the computational complexity and parameter re-\n",
      "quirements of transformers, existing architectures often combine deep learning techniques like CNNs and GANs\n",
      "with transformers (Ma et al., 2022). Knowledge distillation techniques may offer a viable solution for training\n",
      "transformer models with limited computational and storage resources (Leng et al., 2022).\n",
      "Moreover, the application of transformers to bio-signals is relatively limited compared to medical images. There are two main\n",
      "challenges that transformers face in the domain of biomedical signals:\n",
      "• Small bio-signal databases: Bio-signal databases often have limited sizes, which poses challenges for training and\n",
      "validating transformer models effectively. For instance, in a study mentioned by (Guo et al., 2022b), only 20 patients\n",
      "were included, which is considered insufficient to establish the effectiveness of a model. To mitigate the limitations\n",
      "of small databases, some studies have proposed the use of virtual sample generation techniques like ADASYN (He\n",
      "et al., 2008) to augment the dataset.\n",
      "• Limited availability of transformer-based models: Currently, there is a scarcity of models that are exclusively based\n",
      "on transformers for processing biomedical signals. The application of transformers in this context is still relatively\n",
      "unexplored, and more research is needed to develop dedicated transformer architectures for bio-signal analysis and\n",
      "processing.\n",
      "7.3\n",
      "REINFORCEMENT LEARNING\n",
      "The integration of transformers with deep reinforcement learning (RL) methods has emerged as a promising approach for\n",
      "enhancing sequential decision-making processes. Within this domain, two main research categories can be identified: archi-\n",
      "tecture enhancement and trajectory optimization.\n",
      "In the ”architecture enhancement” category, transformers are applied to RL problems based on traditional RL paradigms. This\n",
      "involves leveraging the capabilities of transformers to improve the representation and processing of RL states and actions. On\n",
      "the other hand, the ”trajectory optimization” approach treats RL problems as sequence modeling tasks. It involves training\n",
      "a joint state-action model over entire trajectories, utilizing transformers to learn policies from static datasets, and leveraging\n",
      "the transformers’ ability to model long sequences.\n",
      "Deep RL heavily relies on interactions with the environment to collect data dynamically Rjoub et al. (2019; 2021). However,\n",
      "in certain scenarios such as expensive environments like robotic applications or autonomous vehicles, collecting sufficient\n",
      "training data through real-time interaction may be challenging. To address this, offline RL techniques have been developed,\n",
      "which leverage deep networks to learn optimal policies from static datasets without direct environment interaction. In deep\n",
      "RL settings, transformers are often used to replace traditional components like convolutional neural networks (CNNs) or long\n",
      "short-term memory (LSTM) networks Rjoub et al. (2022), providing memory-awareness and improved modeling capabilities\n",
      "to the agent network. However, standard transformer structures applied directly to decision-making tasks may suffer from\n",
      "stability issues. To overcome this limitation, researchers have proposed modified transformer architectures, such as GtrX\n",
      "(Parisotto et al., 2020), as an alternative solution.\n",
      "In summary, approaches like Decision Transformer and Trajectory Transformer have addressed RL problems as sequence\n",
      "modeling tasks, harnessing the power of transformer architectures to model sequential trajectories (Chen et al., 2021, Janner\n",
      "\n",
      "\n",
      "## Page 48\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "et al., 2021). While these methods show promise in RL tasks, there is still significant room for improvement. Treating RL as\n",
      "sequence modeling simplifies certain limitations of traditional RL algorithms but may also overlook their advantages. There-\n",
      "fore, an interesting direction for further exploration is the integration of traditional RL algorithms with sequence modeling\n",
      "using transformers, combining the strengths of both approaches.\n",
      "7.4\n",
      "OTHER PROSPECTS\n",
      "The successful application of transformers in the field of NLP has sparked interest and exploration in various other do-\n",
      "mains. Researchers have been inspired to apply transformer models to diverse areas, leading to promising developments. For\n",
      "instance, the transformer model BERT has been utilized to model proteins, which, similar to natural language, can be consid-\n",
      "ered as sequential data (Vig et al., 2021). Additionally, the transformer model GPT-2 has been employed to automatically fix\n",
      "JavaScript software bugs and generate patches without human intervention (Lajk´o et al., 2022).\n",
      "Beyond its impact in traditional machine learning and deep learning domains, transformers have found applications in indus-\n",
      "trial studies as well. They have demonstrated impressive performance in various tasks, ranging from predicting the state-of-\n",
      "charge of lithium batteries (Shen et al., 2022a) to classifying vibration signals in mechanical structures (Jin & Chen, 2021).\n",
      "Notably, transformers have showcased superior capabilities compared to Graph Neural Networks (GNNs) in constructing\n",
      "meta-paths from different types of edges in heterogeneous graphs (Yun et al., 2019). This highlights the potential of trans-\n",
      "formers in handling complex and diverse data structures.\n",
      "Another intriguing future application of transformers lies in the field of ”Generative Art,” where intelligent systems are\n",
      "leveraged for automated artistic creation, including images, music, and poetry. While image generation is a well-explored\n",
      "application area for transformers, often focused on natural or medical images, the domain of artistic image generation remains\n",
      "relatively unexplored. However, there have been some initial models based on transformers, such as ”AffectGAN,” which\n",
      "generates images based on semantic text and emotional expressions using transformer models (Galanos et al., 2021). The\n",
      "exploration of transformers in generative art has significant untapped potential for further advancements and creative outputs.\n",
      "Overall, the application of transformers extends beyond NLP and showcases immense potential in various domains, ranging\n",
      "from scientific research to industrial applications and artistic creativity. Continued exploration and innovation in these areas\n",
      "will further expand the possibilities and impact of transformers in the future.\n",
      "8\n",
      "CONCLUSION\n",
      "The transformer, as a deep neural network, has demonstrated superior performance compared to traditional recurrence-based\n",
      "models in processing sequential data. Its ability to capture long-term dependencies and leverage parallel computation has\n",
      "made it a dominant force in various fields such as NLP, computer vision, and more. In this survey, we conducted a compre-\n",
      "hensive overview of transformer models’ applications in different deep learning tasks and proposed a new taxonomy based on\n",
      "the top five fields and respective tasks: NLP, Computer Vision, Multi-Modality, Audio & Speech, and Signal Processing. By\n",
      "examining the advancements in each field, we provided insights into the current research focus and progress of transformer\n",
      "models. This survey serves as a valuable reference for researchers seeking a deeper understanding of transformer applications\n",
      "and aims to inspire further exploration of transformers across various tasks. Additionally, we plan to extend our investigation\n",
      "to emerging fields like wireless networks, cloud computing, reinforcement learning, and others, to uncover new possibilities\n",
      "for transformer utilization. The rapid expansion of transformer applications in diverse domains showcases its versatility and\n",
      "potential for continued growth. With ongoing advancements and novel use cases, transformers are poised to shape the future\n",
      "of deep learning and contribute to advancements in fields beyond the traditional realms of NLP and computer vision.\n",
      "REFERENCES\n",
      "Acheampong, F. A., Nunoo-Mensah, H., & Chen, W. (2021). Transformer models for text-based emotion detection: a review of bert-based\n",
      "approaches. Artif. Intell. Rev., 54, 5789–5829.\n",
      "Ahmed, S. A. A., Awais, M., & Kittler, J. (2021).\n",
      "Sit: Self-supervised vision transformer.\n",
      "CoRR, abs/2104.03602. URL: https:\n",
      "//arxiv.org/abs/2104.03602. arXiv:2104.03602.\n",
      "Akbari, H., Yuan, L., Qian, R., Chuang, W., Chang, S., Cui, Y., & Gong, B. (2021). VATT: transformers for multimodal self-supervised\n",
      "learning from raw video, audio and text. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, & J. W. Vaughan (Eds.), Advances in\n",
      "Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems, NeurIPS, December 6-14,\n",
      "2021, virtual (pp. 24206–24221).\n",
      "Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., & Parikh, D. (2015). VQA: visual question answering. In IEEE\n",
      "International Conference on Computer Vision, ICCV, Santiago, Chile, December 7-13 (pp. 2425–2433). IEEE Computer Society.\n",
      "Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lucic, M., & Schmid, C. (2021). Vivit: A video vision transformer. In 2021 IEEE/CVF\n",
      "International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021 (pp. 6816–6826). IEEE.\n",
      "Babu, A., Wang, C., Tjandra, A., Lakhotia, K., Xu, Q., Goyal, N., Singh, K., von Platen, P., Saraf, Y., Pino, J., Baevski, A., Conneau, A.,\n",
      "& Auli, M. (2022). XLS-R: self-supervised cross-lingual speech representation learning at scale. In H. Ko, & J. H. L. Hansen (Eds.),\n",
      "Interspeech, 23rd Annual Conference of the International Speech Communication Association, Incheon, Korea, 18-22 September (pp.\n",
      "2278–2282). ISCA.\n",
      "\n",
      "\n",
      "## Page 49\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Baevski, A., Schneider, S., & Auli, M. (2020a). vq-wav2vec: Self-supervised learning of discrete speech representations. In 8th Interna-\n",
      "tional Conference on Learning Representations, ICLR, Addis Ababa, Ethiopia, April 26-30. OpenReview.net.\n",
      "Baevski, A., Zhou, Y., Mohamed, A., & Auli, M. (2020b). wav2vec 2.0: A framework for self-supervised learning of speech representations.\n",
      "In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, & H. Lin (Eds.), Advances in Neural Information Processing Systems 33: Annual\n",
      "Conference on Neural Information Processing Systems, NeurIPS, December 6-12, virtual.\n",
      "Bao, H., Dong, L., Piao, S., & Wei, F. (2022). Beit: BERT pre-training of image transformers. In The Tenth International Conference on\n",
      "Learning Representations, ICLR Virtual Event, April 25-29. OpenReview.net.\n",
      "Bogatinovski, J., Todorovski, L., Dzeroski, S., & Kocev, D. (2022). Comprehensive comparative study of multi-label classification methods.\n",
      "Expert Syst. Appl., 203, 117215.\n",
      "Brasoveanu, A. M. P., & Andonie, R. (2020). Visualizing transformers for NLP: A brief survey. In 24th International Conference on\n",
      "Information Visualisation, IV 2020, Melbourne, Australia, September 7-11, 2020 (pp. 270–279). IEEE.\n",
      "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal,\n",
      "S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler,\n",
      "E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., & Amodei, D. (2020). Language\n",
      "models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, & H. Lin (Eds.), Advances in Neural Information\n",
      "Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS, December 6-12, virtual.\n",
      "Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S. (2020). End-to-end object detection with transformers. In\n",
      "A. Vedaldi, H. Bischof, T. Brox, & J. Frahm (Eds.), Computer Vision - ECCV - 16th European Conference, Glasgow, UK, August 23-28,\n",
      "Proceedings, Part I (pp. 213–229). Springer volume 12346 of Lecture Notes in Computer Science.\n",
      "Casal, R., Persia, L. E. D., & Schlotthauer, G. (2022). Temporal convolutional networks and transformers for classifying the sleep stage in\n",
      "awake or asleep using pulse oximetry signals. J. Comput. Sci., 59, 101544.\n",
      "Che, C., Zhang, P., Zhu, M., Qu, Y., & Jin, B. (2021). Constrained transformer network for ECG signal processing and arrhythmia\n",
      "classification. BMC Medical Informatics Decis. Mak., 21, 184.\n",
      "Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., & Su, J. (2019a). This looks like that: Deep learning for interpretable image recognition.\n",
      "In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alch´e-Buc, E. B. Fox, & R. Garnett (Eds.), Advances in Neural Information\n",
      "Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,\n",
      "Vancouver, BC, Canada (pp. 8928–8939).\n",
      "Chen, J., Mao, Q., & Liu, D. (2020a). Dual-path transformer network: Direct context-aware modeling for end-to-end monaural speech\n",
      "separation. In H. Meng, B. Xu, & T. F. Zheng (Eds.), Interspeech, 21st Annual Conference of the International Speech Communication\n",
      "Association, Virtual Event, Shanghai, China, 25-29 October (pp. 2642–2646). ISCA.\n",
      "Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., & Mordatch, I. (2021). Decision transformer:\n",
      "Reinforcement learning via sequence modeling. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, & J. W. Vaughan (Eds.),\n",
      "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems, NeurIPS,\n",
      "December 6-14, virtual (pp. 15084–15097).\n",
      "Chen, M., Challita, U., Saad, W., Yin, C., & Debbah, M. (2019b). Artificial neural networks-based machine learning for wireless networks:\n",
      "A tutorial. IEEE Commun. Surv. Tutorials, 21, 3039–3071.\n",
      "Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., & Sutskever, I. (2020b). Generative pretraining from pixels. In Proceedings\n",
      "of the 37th International Conference on Machine Learning, ICML, 13-18 July, Virtual Event (pp. 1691–1703). PMLR volume 119 of\n",
      "Proceedings of Machine Learning Research.\n",
      "Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., Wu, J., Zhou, L., Ren, S., Qian, Y., Qian,\n",
      "Y., Wu, J., Zeng, M., Yu, X., & Wei, F. (2022a). Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE\n",
      "J. Sel. Top. Signal Process., 16, 1505–1518.\n",
      "Chen, S., Wu, Y., Wang, C., Chen, Z., Chen, Z., Liu, S., Wu, J., Qian, Y., Wei, F., Li, J., & Yu, X. (2022b). Unispeech-sat: Universal speech\n",
      "representation learning with speaker aware pre-training. In IEEE International Conference on Acoustics, Speech and Signal Processing,\n",
      "ICASSP, Virtual and Singapore, 23-27 May (pp. 6152–6156). IEEE.\n",
      "Chen, X., Liu, Y., Yang, B., Zhu, J., Yuan, S., Xie, X., Liu, Y., Dai, J., & Men, K. (2022c). A more effective ct synthesizer using transformers\n",
      "for cone-beam ct-guided adaptive radiotherapy. Frontiers in Oncology, 12.\n",
      "Chen, Y., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z., Cheng, Y., & Liu, J. (2020c). UNITER: universal image-text representation\n",
      "learning. In Computer Vision - ECCV - 16th European Conference, Glasgow, UK, August 23-28 (pp. 104–120). Springer volume 12375\n",
      "of Lecture Notes in Computer Science.\n",
      "Chowdhary, K., & Chowdhary, K. (2020). Natural language processing. Fundamentals of artificial intelligence, (pp. 603–649).\n",
      "Clark, K., Luong, M., Le, Q. V., & Manning, C. D. (2020a). ELECTRA: pre-training text encoders as discriminators rather than generators.\n",
      "In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30. OpenReview.net.\n",
      "\n",
      "\n",
      "## Page 50\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Clark, P., Tafjord, O., & Richardson, K. (2020b). Transformers as soft reasoners over language. In C. Bessiere (Ed.), Proceedings of the\n",
      "Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI (pp. 3882–3890). ijcai.org.\n",
      "Clerckx, B., Huang, K., Varshney, L. R., Ulukus, S., & Alouini, M. (2021). Wireless power transfer for future networks: Signal processing,\n",
      "machine learning, computing, and sensing. IEEE J. Sel. Top. Signal Process., 15, 1060–1094.\n",
      "Conneau, A., Baevski, A., Collobert, R., Mohamed, A., & Auli, M. (2021). Unsupervised cross-lingual representation learning for speech\n",
      "recognition. In H. Hermansky, H. Cernock´y, L. Burget, L. Lamel, O. Scharenborg, & P. Motl´ıcek (Eds.), Interspeech, 22nd Annual\n",
      "Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September (pp. 2426–2430). ISCA.\n",
      "Conneau, A., & Lample, G. (2019). Cross-lingual language model pretraining. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alch´e-\n",
      "Buc, E. B. Fox, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information\n",
      "Processing Systems 2019, NeurIPS 2019, December 8-14, Vancouver, BC, Canada (pp. 7057–7067).\n",
      "d’Ascoli, S., Touvron, H., Leavitt, M. L., Morcos, A. S., Biroli, G., & Sagun, L. (2021). Convit: Improving vision transformers with soft\n",
      "convolutional inductive biases. In M. Meila, & T. Zhang (Eds.), Proceedings of the 38th International Conference on Machine Learning,\n",
      "ICML, 18-24 July, Virtual Event (pp. 2286–2296). PMLR volume 139 of Proceedings of Machine Learning Research.\n",
      "Deng, L., Hinton, G. E., & Kingsbury, B. (2013). New types of deep neural network learning for speech recognition and related applications:\n",
      "an overview. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP, Vancouver, BC, Canada, May\n",
      "26-31 (pp. 8599–8603). IEEE.\n",
      "Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2019). BERT: pre-training of deep bidirectional transformers for language understanding.\n",
      "In J. Burstein, C. Doran, & T. Solorio (Eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, Volume 1 (Long and\n",
      "Short Papers) (pp. 4171–4186). Association for Computational Linguistics.\n",
      "Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., & Tang, J. (2021). Cogview: Mastering\n",
      "text-to-image generation via transformers. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, & J. W. Vaughan (Eds.), Advances\n",
      "in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems, NeurIPS, December 6-14,\n",
      "virtual (pp. 19822–19835).\n",
      "Dong, L., Xu, S., & Xu, B. (2018). Speech-transformer: A no-recurrence sequence-to-sequence model for speech recognition. In 2018\n",
      "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP, Calgary, AB, Canada, April 15-20 (pp. 5884–\n",
      "5888). IEEE.\n",
      "Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,\n",
      "Uszkoreit, J., & Houlsby, N. (2021). An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International\n",
      "Conference on Learning Representations, ICLR, Virtual Event, Austria, May 3-7. OpenReview.net.\n",
      "Fedus, W., Zoph, B., & Shazeer, N. (2021). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.\n",
      "CoRR, abs/2101.03961. URL: https://arxiv.org/abs/2101.03961. arXiv:2101.03961.\n",
      "Fournier, Q., Caron, G. M., & Aloise, D. (2021). A practical survey on faster and lighter transformers. CoRR, abs/2103.14636. URL:\n",
      "https://arxiv.org/abs/2103.14636. arXiv:2103.14636.\n",
      "Fu, T., Li, L., Gan, Z., Lin, K., Wang, W. Y., Wang, L., & Liu, Z. (2021). VIOLET : End-to-end video-language transformers with masked\n",
      "visual-token modeling. CoRR, abs/2111.12681. URL: https://arxiv.org/abs/2111.12681. arXiv:2111.12681.\n",
      "Galanos, T., Liapis, A., & Yannakakis, G. N. (2021). Affectgan: Affect-based generative art driven by semantics. In 9th International\n",
      "Conference on Affective Computing and Intelligent Interaction, ACII - Workshops and Demos, Nara, Japan, September 28 - Oct. 1 (pp.\n",
      "1–7). IEEE.\n",
      "Giles, C. L., Chen, D., Sun, G., Chen, H., Lee, Y., & Goudreau, M. W. (1995). Constructive learning of recurrent neural networks:\n",
      "limitations of recurrent cascade correlation and a simple solution. IEEE Trans. Neural Networks, 6, 829–836.\n",
      "Gong, Y., Chung, Y., & Glass, J. R. (2021). AST: audio spectrogram transformer. In H. Hermansky, H. Cernock´y, L. Burget, L. Lamel,\n",
      "O. Scharenborg, & P. Motl´ıcek (Eds.), Interspeech, 22nd Annual Conference of the International Speech Communication Association,\n",
      "Brno, Czechia, 30 August - 3 September (pp. 571–575). ISCA.\n",
      "Graves, A., & Schmidhuber, J. (2005). Framewise phoneme classification with bidirectional LSTM and other neural network architectures.\n",
      "Neural Networks, 18, 602–610.\n",
      "Gruetzemacher, R., & Paradice, D. B. (2022). Deep transfer learning & beyond: Transformer language models in information systems\n",
      "research. ACM Comput. Surv., 54, 204:1–204:35.\n",
      "Gu, H., Wang, H., Qin, P., & Wang, J. (2022). Chest l-transformer: local features with position attention for weakly supervised chest\n",
      "radiograph segmentation and classification. Frontiers in Medicine, (p. 1619).\n",
      "Gu, Y., Li, X., Chen, S., Zhang, J., & Marsic, I. (2017). Speech intention classification with multimodal deep learning. In M. Mouhoub,\n",
      "& P. Langlais (Eds.), Advances in Artificial Intelligence - 30th Canadian Conference on Artificial Intelligence, Canadian AI, Edmonton,\n",
      "AB, Canada, May 16-19, Proceedings (pp. 260–271). volume 10233 of Lecture Notes in Computer Science.\n",
      "\n",
      "\n",
      "## Page 51\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Gulati, A., Qin, J., Chiu, C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., & Pang, R. (2020). Conformer:\n",
      "Convolution-augmented transformer for speech recognition. In H. Meng, B. Xu, & T. F. Zheng (Eds.), Interspeech, 21st Annual Confer-\n",
      "ence of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October (pp. 5036–5040). ISCA.\n",
      "Guo, J., Han, K., Wu, H., Tang, Y., Chen, X., Wang, Y., & Xu, C. (2022a). CMT: convolutional neural networks meet vision transformers.\n",
      "In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, New Orleans, LA, USA, June 18-24 (pp. 12165–12175).\n",
      "IEEE.\n",
      "Guo, J., Xiao, N., Li, H., He, L., Li, Q., Wu, T., He, X., Chen, P., Chen, D., Xiang, J. et al. (2022b). Transformer-based high-frequency\n",
      "oscillation signal detection on magnetoencephalography from epileptic patients. Frontiers in Molecular Biosciences, 9.\n",
      "Hahn, C., Schmitt, F., Kreber, J. U., Rabe, M. N., & Finkbeiner, B. (2021). Teaching temporal logics to neural networks. In 9th International\n",
      "Conference on Learning Representations, ICLR, Virtual Event, Austria, May 3-7. OpenReview.net.\n",
      "Hajiakhondi-Meybodi, Z., Mohammadi, A., Rahimian, E., Heidarian, S., Abouei, J., & Plataniotis, K. N. (2022).\n",
      "Tedge-caching:\n",
      "Transformer-based edge caching towards 6g networks.\n",
      "In IEEE International Conference on Communications, ICC Seoul, Korea,\n",
      "May 16-20 (pp. 613–618). IEEE.\n",
      "Hamidi-Rad, S., & Jain, S. (2021). Mcformer: A transformer based deep neural network for automatic modulation classification. In IEEE\n",
      "Global Communications Conference, GLOBECOM, Madrid, Spain, December 7-11 (pp. 1–6). IEEE.\n",
      "Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y., Xiao, A., Xu, C., Xu, Y., Yang, Z., Zhang, Y., & Tao, D. (2023). A survey\n",
      "on vision transformer. IEEE Trans. Pattern Anal. Mach. Intell., 45, 87–110.\n",
      "Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., & Wang, Y. (2021). Transformer in transformer. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin,\n",
      "P. Liang, & J. W. Vaughan (Eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information\n",
      "Processing Systems, NeurIPS, December 6-14, virtual (pp. 15908–15919).\n",
      "Haralick, R. M., & Shapiro, L. G. (1985). Image segmentation techniques. Comput. Vis. Graph. Image Process., 29, 100–132.\n",
      "He, H., Bai, Y., Garcia, E. A., & Li, S. (2008). ADASYN: adaptive synthetic sampling approach for imbalanced learning. In Proceedings of\n",
      "the International Joint Conference on Neural Networks, IJCNN 2008, part of the IEEE World Congress on Computational Intelligence,\n",
      "WCCI, Hong Kong, China, June 1-6 (pp. 1322–1328). IEEE.\n",
      "He, X., Tan, E., Bi, H., Zhang, X., Zhao, S., & Lei, B. (2022). Fully transformer network for skin lesion analysis. Medical Image Anal.,\n",
      "77, 102357.\n",
      "H´enaff, O. J. (2020). Data-efficient image recognition with contrastive predictive coding. In Proceedings of the 37th International Confer-\n",
      "ence on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event (pp. 4182–4192). PMLR volume 119 of Proceedings of Machine\n",
      "Learning Research.\n",
      "Hirschberg, J., & Manning, C. D. (2015). Advances in natural language processing. Science, 349, 261–266.\n",
      "Hirschman, L., & Gaizauskas, R. J. (2001). Natural language question answering: the view from here. Nat. Lang. Eng., 7, 275–300.\n",
      "Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Comput., 9, 1735–1780.\n",
      "Hossain, M. Z., Sohel, F., Shiratuddin, M. F., & Laga, H. (2019). A comprehensive survey of deep learning for image captioning. ACM\n",
      "Comput. Surv., 51, 118:1–118:36.\n",
      "Hsu, W., Bolte, B., Tsai, Y. H., Lakhotia, K., Salakhutdinov, R., & Mohamed, A. (2021). Hubert: Self-supervised speech representation\n",
      "learning by masked prediction of hidden units. IEEE ACM Trans. Audio Speech Lang. Process., 29, 3451–3460.\n",
      "Hu, R., Rohrbach, M., & Darrell, T. (2016). Segmentation from natural language expressions. In B. Leibe, J. Matas, N. Sebe, & M. Welling\n",
      "(Eds.), Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings,\n",
      "Part I (pp. 108–124). Springer volume 9905 of Lecture Notes in Computer Science.\n",
      "Huang, P., Kim, M., Hasegawa-Johnson, M., & Smaragdis, P. (2014). Deep learning for monaural speech separation. In IEEE International\n",
      "Conference on Acoustics, Speech and Signal Processing, ICASSP, Florence, Italy, May 4-9 (pp. 1562–1566). IEEE.\n",
      "Huang, Z., Zeng, Z., Liu, B., Fu, D., & Fu, J. (2020). Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. CoRR,\n",
      "abs/2004.00849. URL: https://arxiv.org/abs/2004.00849. arXiv:2004.00849.\n",
      "Islam, M. R., Nahiduzzaman, M., Goni, M. O. F., Sayeed, A., Anower, M. S., Ahsan, M., & Haider, J. (2022). Explainable transformer-\n",
      "based deep learning model for the detection of malaria parasites from blood cell images. Sensors, 22, 4358.\n",
      "Janner, M., Li, Q., & Levine, S. (2021). Offline reinforcement learning as one big sequence modeling problem. In M. Ranzato, A. Beygelz-\n",
      "imer, Y. N. Dauphin, P. Liang, & J. W. Vaughan (Eds.), Advances in Neural Information Processing Systems 34: Annual Conference on\n",
      "Neural Information Processing Systems, NeurIPS, December 6-14, virtual (pp. 1273–1286).\n",
      "Jauro, F., Chiroma, H., Gital, A. Y., Almutairi, M., Abdulhamid, S. M., & Abawajy, J. H. (2020). Deep learning architectures in emerging\n",
      "cloud computing architectures: Recent development, challenges and next research trend. Appl. Soft Comput., 96, 106582.\n",
      "\n",
      "\n",
      "## Page 52\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Jia, C., Yang, Y., Xia, Y., Chen, Y., Parekh, Z., Pham, H., Le, Q. V., Sung, Y., Li, Z., & Duerig, T. (2021). Scaling up visual and vision-\n",
      "language representation learning with noisy text supervision. In M. Meila, & T. Zhang (Eds.), Proceedings of the 38th International\n",
      "Conference on Machine Learning, ICML, 18-24 July, Virtual Event (pp. 4904–4916). PMLR volume 139 of Proceedings of Machine\n",
      "Learning Research.\n",
      "Jiang, Y., Liang, J., Cheng, T., Lin, X., Zhang, Y., & Dong, J. (2022a). Mtpa unet: Multi-scale transformer-position attention retinal vessel\n",
      "segmentation network joint transformer and CNN. Sensors, 22, 4592.\n",
      "Jiang, Y., Zhang, Y., Lin, X., Dong, J., Cheng, T., & Liang, J. (2022b). Swinbts: A method for 3d multimodal brain tumor segmentation\n",
      "using swin transformer. Brain Sciences, 12, 797.\n",
      "Jiao, L., Zhang, F., Liu, F., Yang, S., Li, L., Feng, Z., & Qu, R. (2019). A survey of deep learning-based object detection. IEEE Access, 7,\n",
      "128837–128868.\n",
      "Jin, C., & Chen, X. (2021). An end-to-end framework combining time-frequency expert knowledge and modified transformer networks for\n",
      "vibration signal classification. Expert Syst. Appl., 171, 114570.\n",
      "Jungiewicz, M., Jastrz´ebski, P., Wawryka, P., Przystalski, K., Sabatowski, K., & Bartu´s, S. (2023). Vision transformer in stenosis detection\n",
      "of coronary arteries. Expert Syst. Appl., 228, 120234.\n",
      "Kaliyar, R. K. (2020). A multi-layer bidirectional transformer encoder for pre-trained word embedding: A survey of bert. 2020 10th\n",
      "International Conference on Cloud Computing, Data Science & Engineering, .\n",
      "Keskar, N. S., McCann, B., Varshney, L. R., Xiong, C., & Socher, R. (2019). CTRL: A conditional transformer language model for\n",
      "controllable generation. CoRR, abs/1909.05858. URL: http://arxiv.org/abs/1909.05858. arXiv:1909.05858.\n",
      "Khan, A., & Lee, B. (2023). DeepGene transformer: Transformer for the gene expression-based classification of cancer subtypes. Expert\n",
      "Syst. Appl., 226, 120047.\n",
      "Khan, S. H., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., & Shah, M. (2022). Transformers in vision: A survey. ACM Comput. Surv.,\n",
      "54, 200:1–200:41.\n",
      "Kim, B., Lee, J., Kang, J., Kim, E., & Kim, H. J. (2021a). HOTR: end-to-end human-object interaction detection with transformers. In\n",
      "IEEE Conference on Computer Vision and Pattern Recognition, CVPR, virtual, June 19-25 (pp. 74–83). Computer Vision Foundation /\n",
      "IEEE.\n",
      "Kim, W., Son, B., & Kim, I. (2021b). Vilt: Vision-and-language transformer without convolution or region supervision. In M. Meila, &\n",
      "T. Zhang (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML, 18-24 July, Virtual Event (pp. 5583–\n",
      "5594). PMLR volume 139 of Proceedings of Machine Learning Research.\n",
      "Kuhn, T. (2014). A survey and classification of controlled natural languages. Comput. Linguistics, 40, 121–170.\n",
      "Kurin, V., Godil, S., Whiteson, S., & Catanzaro, B. (2020). Can q-learning with graph networks learn a generalizable branching heuristic\n",
      "for a SAT solver? In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, & H. Lin (Eds.), Advances in Neural Information Processing\n",
      "Systems 33: Annual Conference on Neural Information Processing Systems, NeurIPS, December 6-12, virtual.\n",
      "Lajk´o, M., Csuvik, V., & Vid´acs, L. (2022). Towards javascript program repair with generative pre-trained transformer (GPT-2). In 3rd\n",
      "IEEE/ACM International Workshop on Automated Program Repair, APR@ICSE, Pittsburgh, PA, USA, May 19 (pp. 61–68). IEEE.\n",
      "Le, H., Vial, L., Frej, J., Segonne, V., Coavoux, M., Lecouteux, B., Allauzen, A., Crabb´e, B., Besacier, L., & Schwab, D. (2020). Flaubert:\n",
      "Unsupervised language model pre-training for french. In Proceedings of The 12th Language Resources and Evaluation Conference,\n",
      "LREC 2020, Marseille, France, May 11-16, 2020 (pp. 2479–2490). European Language Resources Association.\n",
      "Leng, B., Leng, M., Ge, M., & Dong, W. (2022). Knowledge distillation-based deep learning classification network for peripheral blood\n",
      "leukocytes. Biomed. Signal Process. Control., 75, 103590.\n",
      "Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., & Zettlemoyer, L. (2020). BART: denoising\n",
      "sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In D. Jurafsky, J. Chai, N. Schluter,\n",
      "& J. R. Tetreault (Eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online,\n",
      "July 5-10, 2020 (pp. 7871–7880). Association for Computational Linguistics.\n",
      "Li, G., Duan, N., Fang, Y., Gong, M., & Jiang, D. (2020a). Unicoder-vl: A universal encoder for vision and language by cross-modal pre-\n",
      "training. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI, New York, NY, USA, February 7-12 (pp. 11336–11344).\n",
      "AAAI Press.\n",
      "Li, J., Chen, J., Tang, Y., Wang, C., Landman, B. A., & Zhou, S. K. (2023). Transforming medical imaging with transformers? a comparative\n",
      "review of key properties, current progresses, and future perspectives. Medical image analysis, (p. 102762).\n",
      "Li, J., Li, D., Xiong, C., & Hoi, S. C. H. (2022). BLIP: bootstrapping language-image pre-training for unified vision-language understanding\n",
      "and generation. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesv´ari, G. Niu, & S. Sabato (Eds.), International Conference on Machine\n",
      "Learning, ICML, 17-23 July, Baltimore, Maryland, USA (pp. 12888–12900). PMLR volume 162 of Proceedings of Machine Learning\n",
      "Research.\n",
      "\n",
      "\n",
      "## Page 53\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Li, P., Fu, T., & Ma, W. (2020b). Why attention? analyze bilstm deficiency and its remedies in the case of NER. In The Thirty-Fourth AAAI\n",
      "Conference on Artificial Intelligence, New York, NY, USA, February 7-12 (pp. 8236–8244). AAAI Press.\n",
      "Li, P., Li, J., Huang, Z., Li, T., Gao, C., Yiu, S., & Chen, K. (2017). Multi-key privacy-preserving deep learning in cloud computing. Future\n",
      "Gener. Comput. Syst., 74, 76–85.\n",
      "Li, S., & Hoefler, T. (2021). Chimera: efficiently training large-scale neural networks with bidirectional pipelines. In B. R. de Supinski,\n",
      "M. W. Hall, & T. Gamblin (Eds.), International Conference for High Performance Computing, Networking, Storage and Analysis, SC,\n",
      "St. Louis, Missouri, USA, November 14-19 (p. 27). ACM.\n",
      "Liang, J., Yang, C., Zeng, M., & Wang, X. (2022). Transconver: transformer and convolution parallel network for developing automatic\n",
      "brain tumor segmentation in mri images. Quantitative Imaging in Medicine and Surgery, 12, 2397.\n",
      "Lin, T., Wang, Y., Liu, X., & Qiu, X. (2022). A survey of transformers. AI Open, 3, 111–132.\n",
      "Liu, A. T., Yang, S., Chi, P., Hsu, P., & Lee, H. (2020). Mockingjay: Unsupervised speech representation learning with deep bidirectional\n",
      "transformer encoders. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP, Barcelona, Spain, May\n",
      "4-8 (pp. 6419–6423). IEEE.\n",
      "Liu, J., Li, Z., Fan, X., Hu, X., Yan, J., Li, B., Xia, Q., Zhu, J., & Wu, Y. (2022a). Crt-net: A generalized and scalable framework for the\n",
      "computer-aided diagnosis of electrocardiogram signals. Appl. Soft Comput., 128, 109481.\n",
      "Liu, J., Wang, T., Li, Y., Li, C., Wang, Y., & Shen, Y. (2022b). A transformer-based signal denoising network for aoa estimation in nlos\n",
      "environments. IEEE Commun. Lett., 26, 2336–2339.\n",
      "Liu, M., Breuel, T. M., & Kautz, J. (2017). Unsupervised image-to-image translation networks. In I. Guyon, U. von Luxburg, S. Bengio,\n",
      "H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 30: Annual\n",
      "Conference on Neural Information Processing Systems 2017, December 4-9, Long Beach, CA, USA (pp. 700–708).\n",
      "Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019).\n",
      "RoBERTa:\n",
      "A robustly optimized BERT pretraining approach.\n",
      "CoRR, abs/1907.11692. URL: http://arxiv.org/abs/1907.11692.\n",
      "arXiv:1907.11692.\n",
      "Liu, Y., Qiao, L., Yin, D., Jiang, Z., Jiang, X., Jiang, D., & Ren, B. (2022c). OS-MSL: one stage multimodal sequential link framework for\n",
      "scene segmentation and classification. In J. Magalh˜aes, A. D. Bimbo, S. Satoh, N. Sebe, X. Alameda-Pineda, Q. Jin, V. Oria, & L. Toni\n",
      "(Eds.), MM: The 30th ACM International Conference on Multimedia, Lisboa, Portugal, October 10 - 14 (pp. 6269–6277). ACM.\n",
      "Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using\n",
      "shifted windows. In 2021 IEEE/CVF International Conference on Computer Vision ICCV, Montreal, QC, Canada, October 10-17 (pp.\n",
      "9992–10002). IEEE.\n",
      "Livezey, J. A., Bouchard, K. E., & Chang, E. F. (2019). Deep learning as a tool for neural data analysis: Speech classification and\n",
      "cross-frequency coupling in human sensorimotor cortex. PLoS Comput. Biol., 15.\n",
      "L´opez-Linares, K., Garc´ıa Oca˜na, M. I., Lete Urzelai, N., Gonz´alez Ballester, M. ´A., & Mac´ıa Oliver, I. (2020). Medical image segmentation\n",
      "using deep learning. Deep Learning in Healthcare: Paradigms and Applications, (pp. 17–31).\n",
      "Lu, D., & Weng, Q. (2007). A survey of image classification methods and techniques for improving classification performance. Interna-\n",
      "tional journal of Remote sensing, 28, 823–870.\n",
      "Lu, J., Batra, D., Parikh, D., & Lee, S. (2019). Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language\n",
      "tasks. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alch´e-Buc, E. B. Fox, & R. Garnett (Eds.), Advances in Neural Information\n",
      "Processing Systems 32: Annual Conference on Neural Information Processing Systems, NeurIPS, December 8-14, Vancouver, BC,\n",
      "Canada (pp. 13–23).\n",
      "Ma, M., Xu, Y., Song, L., & Liu, G. (2022). Symmetric transformer-based network for unsupervised image registration. Knowl. Based\n",
      "Syst., 257, 109959.\n",
      "Mahesh, K. M., & Renjit, J. A. (2020). Deepjoint segmentation for the classification of severity-levels of glioma tumour using multimodal\n",
      "MRI images. IET Image Process., 14, 2541–2552.\n",
      "Mark A Musen, J. V. d. L. (1988). Of brittleness and bottlenecks: Challenges in the creation of pattern-recognition and expert-system\n",
      "models. In Machine Intelligence and Pattern Recognition,, 7, 335–352.\n",
      "Menze, B. H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J. S., Burren, Y., Porz, N., Slotboom, J., Wiest, R., Lanczi,\n",
      "L., Gerstner, E. R., Weber, M., Arbel, T., Avants, B. B., Ayache, N., Buendia, P., Collins, D. L., Cordier, N., Corso, J. J., Criminisi, A.,\n",
      "Das, T., Delingette, H., Demiralp, C¸ ., Durst, C. R., Dojat, M., Doyle, S., Festa, J., Forbes, F., Geremia, E., Glocker, B., Golland, P., Guo,\n",
      "X., Hamamci, A., Iftekharuddin, K. M., Jena, R., John, N. M., Konukoglu, E., Lashkari, D., Mariz, J. A., Meier, R., Pereira, S., Precup,\n",
      "D., Price, S. J., Raviv, T. R., Reza, S. M. S., Ryan, M. T., Sarikaya, D., Schwartz, L. H., Shin, H., Shotton, J., Silva, C. A., Sousa, N. J.,\n",
      "Subbanna, N. K., Sz´ekely, G., Taylor, T. J., Thomas, O. M., Tustison, N. J., ¨Unal, G. B., Vasseur, F., Wintermark, M., Ye, D. H., Zhao,\n",
      "L., Zhao, B., Zikic, D., Prastawa, M., Reyes, M., & Leemput, K. V. (2015). The multimodal brain tumor image segmentation benchmark\n",
      "(BRATS). IEEE Trans. Medical Imaging, 34, 1993–2024.\n",
      "\n",
      "\n",
      "## Page 54\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Mikolov, T., Karafi´at, M., Burget, L., Cernock´y, J., & Khudanpur, S. (2010).\n",
      "Recurrent neural network based language model.\n",
      "In\n",
      "T. Kobayashi, K. Hirose, & S. Nakamura (Eds.), INTERSPEECH 2010, 11th Annual Conference of the International Speech Com-\n",
      "munication Association, Makuhari, Chiba, Japan, September 26-30, 2010 (pp. 1045–1048). ISCA.\n",
      "Minaee, S., Boykov, Y., Porikli, F., Plaza, A., Kehtarnavaz, N., & Terzopoulos, D. (2022). Image segmentation using deep learning: A\n",
      "survey. IEEE Trans. Pattern Anal. Mach. Intell., 44, 3523–3542.\n",
      "Monroe, D. (2017). Deep learning takes on translation. Commun. ACM, 60, 12–14.\n",
      "Murtagh, F. (1990). Multilayer perceptrons for classification and regression. Neurocomputing, 2, 183–197.\n",
      "Nagrani, A., Yang, S., Arnab, A., Jansen, A., Schmid, C., & Sun, C. (2021). Attention bottlenecks for multimodal fusion. In M. Ranzato,\n",
      "A. Beygelzimer, Y. N. Dauphin, P. Liang, & J. W. Vaughan (Eds.), Advances in Neural Information Processing Systems 34: Annual\n",
      "Conference on Neural Information Processing Systems, NeurIPS, December 6-14, virtual (pp. 14200–14213).\n",
      "Nassif, A. B., Shahin, I., Attili, I. B., Azzeh, M., & Shaalan, K. (2019). Speech recognition using deep neural networks: A systematic\n",
      "review. IEEE Access, 7, 19143–19165.\n",
      "Nichol, A. Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., & Chen, M. (2022).\n",
      "GLIDE: towards\n",
      "photorealistic image generation and editing with text-guided diffusion models. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesv´ari,\n",
      "G. Niu, & S. Sabato (Eds.), International Conference on Machine Learning, ICML, 17-23 July, Baltimore, Maryland, USA (pp. 16784–\n",
      "16804). PMLR volume 162 of Proceedings of Machine Learning Research.\n",
      "Niu, Z., Zhong, G., & Yu, H. (2021). A review on the attention mechanism of deep learning. Neurocomputing, 452, 48–62.\n",
      "van den Oord, A., Kalchbrenner, N., Espeholt, L., Kavukcuoglu, K., Vinyals, O., & Graves, A. (2016). Conditional image generation\n",
      "with pixelcnn decoders. In D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, & R. Garnett (Eds.), Advances in Neural Information\n",
      "Processing Systems 29: Annual Conference on Neural Information Processing Systems, December 5-10, Barcelona, Spain (pp. 4790–\n",
      "4798).\n",
      "O’Shea, K., & Nash, R. (2015). An introduction to convolutional neural networks. CoRR, abs/1511.08458. URL: http://arxiv.org/\n",
      "abs/1511.08458. arXiv:1511.08458.\n",
      "Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton,\n",
      "J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., & Lowe, R. (2022). Training language models to\n",
      "follow instructions with human feedback. CoRR, abs/2203.02155. URL: https://doi.org/10.48550/arXiv.2203.02155.\n",
      "doi:10.48550/arXiv.2203.02155. arXiv:2203.02155.\n",
      "Pang, Y., Lin, J., Qin, T., & Chen, Z. (2022). Image-to-image translation: Methods and applications. IEEE Trans. Multim., 24, 3859–3881.\n",
      "Parisotto, E., Song, H. F., Rae, J. W., Pascanu, R., G¨ulc¸ehre, C¸ ., Jayakumar, S. M., Jaderberg, M., Kaufman, R. L., Clark, A., Noury, S.,\n",
      "Botvinick, M. M., Heess, N., & Hadsell, R. (2020). Stabilizing transformers for reinforcement learning. In Proceedings of the 37th\n",
      "International Conference on Machine Learning, ICML, 13-18 July, Virtual Event (pp. 7487–7498). PMLR volume 119 of Proceedings\n",
      "of Machine Learning Research.\n",
      "Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., & Tran, D. (2018). Image transformer. In J. G. Dy, & A. Krause\n",
      "(Eds.), Proceedings of the 35th International Conference on Machine Learning, ICML, Stockholmsm¨assan, Stockholm, Sweden, July\n",
      "10-15 (pp. 4052–4061). PMLR volume 80 of Proceedings of Machine Learning Research.\n",
      "Peng, Z., Huang, W., Gu, S., Xie, L., Wang, Y., Jiao, J., & Ye, Q. (2021). Conformer: Local features coupling global representations\n",
      "for visual recognition. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October\n",
      "10-17, 2021 (pp. 357–366). IEEE.\n",
      "Picco, G., Hoang, T. L., Sbodio, M. L., & L´opez, V. (2021). Neural unification for logic reasoning over natural language. In M. Moens,\n",
      "X. Huang, L. Specia, & S. W. Yih (Eds.), Findings of the Association for Computational Linguistics: EMNLP, Virtual Event / Punta\n",
      "Cana, Dominican Republic, 16-20 November (pp. 3939–3950). Association for Computational Linguistics.\n",
      "Pnueli, A. (1977). The temporal logic of programs. In 18th Annual Symposium on Foundations of Computer Science, Providence, Rhode\n",
      "Island, USA, 31 October - 1 November (pp. 46–57). IEEE Computer Society.\n",
      "Polu, S., & Sutskever, I. (2020). Generative language modeling for automated theorem proving. CoRR, abs/2009.03393. URL: https:\n",
      "//arxiv.org/abs/2009.03393. arXiv:2009.03393.\n",
      "Qi, W., Yan, Y., Gong, Y., Liu, D., Duan, N., Chen, J., Zhang, R., & Zhou, M. (2020). Prophetnet: Predicting future n-gram for sequence-\n",
      "to-sequence pre-training. In T. Cohn, Y. He, & Y. Liu (Eds.), Findings of the Association for Computational Linguistics: EMNLP 2020,\n",
      "Online Event, 16-20 November (pp. 2401–2410). Association for Computational Linguistics volume EMNLP 2020 of Findings of ACL.\n",
      "Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G.,\n",
      "& Sutskever, I. (2021). Learning transferable visual models from natural language supervision. In M. Meila, & T. Zhang (Eds.),\n",
      "Proceedings of the 38th International Conference on Machine Learning, ICML, 18-24 July, Virtual Event (pp. 8748–8763). PMLR\n",
      "volume 139 of Proceedings of Machine Learning Research.\n",
      "\n",
      "\n",
      "## Page 55\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2022). Robust speech recognition via large-scale weak\n",
      "supervision.\n",
      "CoRR, abs/2212.04356. URL: https://doi.org/10.48550/arXiv.2212.04356. doi:10.48550/arXiv.\n",
      "2212.04356. arXiv:2212.04356.\n",
      "Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding with unsupervised learning. Tech-\n",
      "nical Report OpenAI.\n",
      "Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I. et al. (2019). Language models are unsupervised multitask learners.\n",
      "OpenAI blog, 1, 9.\n",
      "Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2020). Exploring the limits of transfer\n",
      "learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21, 140:1–140:67.\n",
      "Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., & Sutskever, I. (2021). Zero-shot text-to-image generation. In\n",
      "M. Meila, & T. Zhang (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML, 18-24 July, Virtual Event\n",
      "(pp. 8821–8831). PMLR volume 139 of Proceedings of Machine Learning Research.\n",
      "Reiter, E., & Dale, R. (1997). Building applied natural language generation systems. Nat. Lang. Eng., 3, 57–87.\n",
      "Ren, Q., Li, Y., & Liu, Y. (2023). Transformer-enhanced periodic temporal convolution network for long short-term traffic flow forecasting.\n",
      "Expert Syst. Appl., 227, 120203.\n",
      "Ren, Z., Cheng, N., Sun, R., Wang, X., Lu, N., & Xu, W. (2022). Sigt: An efficient end-to-end MIMO-OFDM receiver framework based\n",
      "on transformer. In 5th International Conference on Communications, Signal Processing, and their Applications, ICCSPA, Cairo, Egypt,\n",
      "December 27-29 (pp. 1–6). IEEE.\n",
      "Reza, S., Ferreira, M. C., Machado, J. J. M., & Tavares, J. M. R. S. (2022). A multi-head attention-based transformer model for traffic flow\n",
      "forecasting with a comparative analysis to recurrent neural networks. Expert Syst. Appl., 202, 117275.\n",
      "Richardson, K., & Sabharwal, A. (2022). Pushing the limits of rule reasoning in transformers through natural language satisfiability. In\n",
      "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI, Virtual Event, February 22 - March 1 (pp. 11209–11219). AAAI Press.\n",
      "Rjoub, G., Bentahar, J., Abdel Wahab, O., & Saleh Bataineh, A. (2021). Deep and reinforcement learning for automated task scheduling in\n",
      "large-scale cloud computing systems. Concurrency and Computation: Practice and Experience, 33, e5919.\n",
      "Rjoub, G., Bentahar, J., Wahab, O. A., & Bataineh, A. (2019). Deep smart scheduling: A deep learning approach for automated big data\n",
      "scheduling over the cloud. In 2019 7th International Conference on Future Internet of Things and Cloud (FiCloud) (pp. 189–196). IEEE.\n",
      "Rjoub, G., Wahab, O. A., Bentahar, J., & Bataineh, A. (2022). Trust-driven reinforcement selection strategy for federated learning on IoT\n",
      "devices. Computing, (pp. 1–23).\n",
      "Ruan, L., & Jin, Q. (2022). Survey: Transformer based video-language pre-training. AI Open, 3, 1–13.\n",
      "Saha, S., Ghosh, S., Srivastava, S., & Bansal, M. (2020). Prover: Proof generation for interpretable reasoning over rules. In B. Webber,\n",
      "T. Cohn, Y. He, & Y. Liu (Eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP\n",
      "2020, Online, November 16-20, 2020 (pp. 122–136). Association for Computational Linguistics.\n",
      "Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. CoRR,\n",
      "abs/1910.01108. URL: http://arxiv.org/abs/1910.01108. arXiv:1910.01108.\n",
      "Selsam, D., Lamm, M., B¨unz, B., Liang, P., de Moura, L., & Dill, D. L. (2019). Learning a SAT solver from single-bit supervision. In 7th\n",
      "International Conference on Learning Representations, ICLR, New Orleans, LA, USA, May 6-9. OpenReview.net.\n",
      "Selva, J., Johansen, A. S., Escalera, S., Nasrollahi, K., Moeslund, T. B., & Clap´es, A. (2023). Video transformers: A survey. IEEE\n",
      "Transactions on Pattern Analysis and Machine Intelligence, .\n",
      "Shamshad, F., Khan, S., Zamir, S. W., Khan, M. H., Hayat, M., Khan, F. S., & Fu, H. (2023). Transformers in medical imaging: A survey.\n",
      "Medical Image Analysis, (p. 102802).\n",
      "Shen, H., Zhou, X., Wang, Z., & Wang, J. (2022a). State of charge estimation for lithium-ion battery using transformer with immersion and\n",
      "invariance adaptive observer. Journal of Energy Storage, 45, 103768.\n",
      "Shen, X., Wang, L., Zhao, Y., Liu, R., Qian, W., & Ma, H. (2022b). Dilated transformer: residual axial attention for breast ultrasound image\n",
      "segmentation. Quantitative Imaging in Medicine and Surgery, 12, 4512.\n",
      "Shi, C., Xiao, Y., & Chen, Z. (2022a). Dual-domain sparse-view ct reconstruction with transformers. Physica Medica, 101, 1–7.\n",
      "Shi, F., Lee, C., Bashar, M. K., Shukla, N., Zhu, S., & Narayanan, V. (2021). Transformer-based machine learning for fast SAT solvers and\n",
      "logic synthesis. CoRR, abs/2107.07116. URL: https://arxiv.org/abs/2107.07116. arXiv:2107.07116.\n",
      "Shi, Z., Li, M., Khan, S., Zhen, H., Yuan, M., & Xu, Q. (2022b). Satformer: Transformers for SAT solving. CoRR, abs/2209.00953. URL:\n",
      "https://doi.org/10.48550/arXiv.2209.00953. doi:10.48550/arXiv.2209.00953. arXiv:2209.00953.\n",
      "\n",
      "\n",
      "## Page 56\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Shih, K. J., Singh, S., & Hoiem, D. (2016). Where to look: Focus regions for visual question answering. In 2016 IEEE Conference on\n",
      "Computer Vision and Pattern Recognition, CVPR, Las Vegas, NV, USA, June 27-30 (pp. 4613–4621). IEEE Computer Society.\n",
      "Shin, A., Ishii, M., & Narihira, T. (2022). Perspectives and prospects on transformer architecture for cross-modal tasks with language and\n",
      "vision. Int. J. Comput. Vis., 130, 435–454.\n",
      "Sinha, K., Sodhani, S., Dong, J., Pineau, J., & Hamilton, W. L. (2019). CLUTRR: A diagnostic benchmark for inductive reasoning from text.\n",
      "In K. Inui, J. Jiang, V. Ng, & X. Wan (Eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\n",
      "and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP, Hong Kong, China, November 3-7 (pp.\n",
      "4505–4514). Association for Computational Linguistics.\n",
      "Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., & Dai, J. (2020). VL-BERT: pre-training of generic visual-linguistic representations. In\n",
      "8th International Conference on Learning Representations, ICLR, Addis Ababa, Ethiopia, April 26-30. OpenReview.net.\n",
      "Subakan, C., Ravanelli, M., Cornell, S., Bronzi, M., & Zhong, J. (2021).\n",
      "Attention is all you need in speech separation.\n",
      "In IEEE\n",
      "International Conference on Acoustics, Speech and Signal Processing, ICASSP, Toronto, ON, Canada, June 6-11 (pp. 21–25). IEEE.\n",
      "Subramanyam, K., Rajasekharan, A., & Sangeetha, S. (2021a). Ammu: A survey of transformer-based biomedical pretrained language\n",
      "models. arXiv e-prints, (pp. arXiv–2105).\n",
      "Subramanyam, K., Rajasekharan, A., & Sangeetha, S. (2021b). AMMUS : A survey of transformer-based pretrained models in natural\n",
      "language processing. CoRR, abs/2108.05542. URL: https://arxiv.org/abs/2108.05542. arXiv:2108.05542.\n",
      "Sun, H., Chen, X., Shi, Q., Hong, M., Fu, X., & Sidiropoulos, N. D. (2017). Learning to optimize: Training deep neural networks for\n",
      "wireless resource management. In 18th IEEE International Workshop on Signal Processing Advances in Wireless Communications,\n",
      "SPAWC, Sapporo, Japan, July 3-6 (pp. 1–6). IEEE.\n",
      "Sun, J., Shen, Z., Wang, Y., Bao, H., & Zhou, X. (2021a). Loftr: Detector-free local feature matching with transformers. In IEEE Conference\n",
      "on Computer Vision and Pattern Recognition, CVPR virtual, June 19-25 (pp. 8922–8931). Computer Vision Foundation / IEEE.\n",
      "Sun, Q., Fang, N., Liu, Z., Zhao, L., Wen, Y., Lin, H. et al. (2021b). Hybridctrm: Bridging cnn and transformer for multimodal brain image\n",
      "segmentation. Journal of Healthcare Engineering, 2021.\n",
      "Suzuki, M., & Matsuo, Y. (2022). A survey of multimodal deep generative models. Adv. Robotics, 36, 261–278.\n",
      "Szummer, M., & Picard, R. W. (1998). Indoor-outdoor image classification. In 1998 International Workshop on Content-Based Access of\n",
      "Image and Video Databases, CAIVD 1998, Bombay, India, January 3, 1998 (pp. 42–51). IEEE Computer Society.\n",
      "Tan, H., & Bansal, M. (2019). LXMERT: learning cross-modality encoder representations from transformers. In K. Inui, J. Jiang, V. Ng,\n",
      "& X. Wan (Eds.), Proceedings of the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint\n",
      "Conference on Natural Language Processing, EMNLP-IJCNLP, Hong Kong, China, November 3-7 (pp. 5099–5110). Association for\n",
      "Computational Linguistics.\n",
      "Tas, O., & Kiyani, F. (2007). A survey automatic text summarization. PressAcademia Procedia, 5, 205–213.\n",
      "Tay, Y., Dehghani, M., Bahri, D., & Metzler, D. (2023). Efficient transformers: A survey. ACM Comput. Surv., 55, 109:1–109:28.\n",
      "Tay, Y., Tran, V. Q., Ruder, S., Gupta, J. P., Chung, H. W., Bahri, D., Qin, Z., Baumgartner, S., Yu, C., & Metzler, D. (2022). Charformer:\n",
      "Fast character transformers via gradient-based subword tokenization. In The Tenth International Conference on Learning Representa-\n",
      "tions, ICLR 2022, Virtual Event, April 25-29. OpenReview.net.\n",
      "Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., & J´egou, H. (2021). Training data-efficient image transformers & distillation\n",
      "through attention. In M. Meila, & T. Zhang (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML, 18-24\n",
      "July, Virtual Event (pp. 10347–10357). PMLR volume 139 of Proceedings of Machine Learning Research.\n",
      "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention is all you need.\n",
      "In I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, & R. Garnett (Eds.), Advances in Neural\n",
      "Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, December 4-9, Long Beach, CA,\n",
      "USA (pp. 5998–6008).\n",
      "Vig, J., Madani, A., Varshney, L. R., Xiong, C., Socher, R., & Rajani, N. F. (2021). Bertology meets biology: Interpreting attention\n",
      "in protein language models. In 9th International Conference on Learning Representations, ICLR, Virtual Event, Austria, May 3-7.\n",
      "OpenReview.net.\n",
      "Wang, D., & Chen, J. (2018). Supervised speech separation based on deep learning: An overview. IEEE ACM Trans. Audio Speech Lang.\n",
      "Process., 26, 1702–1726.\n",
      "Wang, G., Smetannikov, I., & Man, T. (2020a). Survey on automatic text summarization and transformer models applicability. In CCRIS:\n",
      "International Conference on Control, Robotics and Intelligent System, Xiamen, China, October 27-29 (pp. 176–184). ACM.\n",
      "\n",
      "\n",
      "## Page 57\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Wang, J., Yang, Z., Hu, X., Li, L., Lin, K., Gan, Z., Liu, Z., Liu, C., & Wang, L. (2022a). GIT: A generative image-to-text transformer\n",
      "for vision and language. CoRR, abs/2205.14100. URL: https://doi.org/10.48550/arXiv.2205.14100. doi:10.48550/\n",
      "arXiv.2205.14100. arXiv:2205.14100.\n",
      "Wang, P., Cheng, Y., & Dong, B. (2021a). Augmented convolutional neural networks with transformer for wireless interference identifica-\n",
      "tion. In IEEE Global Communications Conference, GLOBECOM, Madrid, Spain, December 7-11 (pp. 1–6). IEEE.\n",
      "Wang, S., Bi, S., & Zhang, Y.-J. A. (2022b). Deep reinforcement learning with communication transformer for adaptive live streaming in\n",
      "wireless edge networks. IEEE Journal on Selected Areas in Communications, 40, 308–322.\n",
      "Wang, T., Lai, Z., & Kong, H. (2021b). Tfnet: Transformer fusion network for ultrasound image segmentation. In C. Wallraven, Q. Liu,\n",
      "& H. Nagahara (Eds.), Pattern Recognition - 6th Asian Conference, ACPR, Jeju Island, South Korea, November 9-12, Revised Selected\n",
      "Papers, Part I (pp. 314–325). Springer volume 13188 of Lecture Notes in Computer Science.\n",
      "Wang, T., Lan, J., Han, Z., Hu, Z., Huang, Y., Deng, Y., Zhang, H., Wang, J., Chen, M., Jiang, H. et al. (2022c). O-net: a novel framework\n",
      "with deep fusion of cnn and transformer for simultaneous segmentation and classification. Frontiers in Neuroscience, 16.\n",
      "Wang, W., Liang, D., Chen, Q., Iwamoto, Y., Han, X.-H., Zhang, Q., Hu, H., Lin, L., & Chen, Y.-W. (2020b). Medical image classification\n",
      "using deep learning. Deep learning in healthcare: paradigms and applications, (pp. 33–51).\n",
      "Wang, Z., Ma, Y., Liu, Z., & Tang, J. (2019). R-transformer: Recurrent neural network enhanced transformer. CoRR, abs/1907.05572.\n",
      "URL: http://arxiv.org/abs/1907.05572. arXiv:1907.05572.\n",
      "Wang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., & Cao, Y. (2022d). Simvlm: Simple visual language model pretraining with weak\n",
      "supervision. In The Tenth International Conference on Learning Representations, ICLR, Virtual Event, April 25-29. OpenReview.net.\n",
      "Wu, D., Pigou, L., Kindermans, P., Le, N. D., Shao, L., Dambre, J., & Odobez, J. (2016). Deep dynamic neural networks for multimodal\n",
      "gesture segmentation and recognition. IEEE Trans. Pattern Anal. Mach. Intell., 38, 1583–1597.\n",
      "Wu, Y., Wang, G., Wang, Z., Wang, H., & Li, Y. (2022). Di-unet: Dimensional interaction self-attention for medical image segmentation.\n",
      "Biomed. Signal Process. Control., 78, 103896.\n",
      "Xie, W., Zou, J., Xiao, J., Li, M., & Peng, X. (2022). Quan-transformer based channel feedback for ris-aided wireless communication\n",
      "systems. IEEE Commun. Lett., 26, 2631–2635.\n",
      "Xing, Y., Shi, Z., Meng, Z., Lakemeyer, G., Ma, Y., & Wattenhofer, R. (2021). KM-BART: knowledge enhanced multimodal BART\n",
      "for visual commonsense generation. In C. Zong, F. Xia, W. Li, & R. Navigli (Eds.), Proceedings of the 59th Annual Meeting of the\n",
      "Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP,\n",
      "(Volume 1: Long Papers), Virtual Event, August 1-6 (pp. 525–535). Association for Computational Linguistics.\n",
      "Xu, Y., Wei, H., Lin, M., Deng, Y., Sheng, K., Zhang, M., Tang, F., Dong, W., Huang, F., & Xu, C. (2022). Transformers in computational\n",
      "visual media: A survey. Computational Visual Media, 8, 33–62.\n",
      "Xu, Y., & Zhao, J. (2022). Actor-critic with transformer for cloud computing resource three stage job scheduling. In 7th International\n",
      "Conference on Cloud Computing and Big Data Analytics (ICCCBDA), Chengdu, China, 22-24 April (pp. 33–37).\n",
      "Yan, J., Li, J., Xu, H., Yu, Y., & Xu, T. (2022a). Seizure prediction based on transformer using scalp electroencephalogram. Applied\n",
      "Sciences, 12, 4158.\n",
      "Yan, S., Wang, C., Chen, W., & Lyu, J. (2022b). Swin transformer-based GAN for multi-modal medical image translation. Frontiers in\n",
      "Oncology, 12.\n",
      "Yan, W., Zhang, Y., Abbeel, P., & Srinivas, A. (2021).\n",
      "Videogpt: Video generation using VQ-VAE and transformers.\n",
      "CoRR,\n",
      "abs/2104.10157. URL: https://arxiv.org/abs/2104.10157. arXiv:2104.10157.\n",
      "Yang, H., & Yang, D. (2023). Cswin-pnet: A cnn-swin transformer combined pyramid network for breast lesion segmentation in ultrasound\n",
      "images. Expert Syst. Appl., Volume 213, Part B, 119024.\n",
      "Yang, M., Lee, D., & Park, S. (2022). Automated diagnosis of atrial fibrillation using ECG component-aware transformer. Comput. Biol.\n",
      "Medicine, 150, 106115.\n",
      "Yeh, C., Mahadeokar, J., Kalgaonkar, K., Wang, Y., Le, D., Jain, M., Schubert, K., Fuegen, C., & Seltzer, M. L. (2019). Transformer-\n",
      "transducer: End-to-end speech recognition with self-attention. CoRR, abs/1910.12977. URL: http://arxiv.org/abs/1910.\n",
      "12977. arXiv:1910.12977.\n",
      "Yu, D., & Deng, L. (2016). Automatic speech recognition volume 1. Springer.\n",
      "Yu, J., Li, J., Yu, Z., & Huang, Q. (2020). Multimodal transformer with multi-view visual representation for image captioning. IEEE Trans.\n",
      "Circuits Syst. Video Technol., 30, 4467–4480.\n",
      "\n",
      "\n",
      "## Page 58\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "Yu, S., Wang, X., & Langar, R. (2017). Computation offloading for mobile edge computing: A deep learning approach. In 28th IEEE\n",
      "Annual International Symposium on Personal, Indoor, and Mobile Radio Communications, PIMRC, Montreal, QC, Canada, October\n",
      "8-13 (pp. 1–6). IEEE.\n",
      "Yuan, L., Chen, D., Chen, Y., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B., Li, C., Liu, C., Liu, M., Liu, Z., Lu, Y., Shi, Y., Wang,\n",
      "L., Wang, J., Xiao, B., Xiao, Z., Yang, J., Zeng, M., Zhou, L., & Zhang, P. (2021). Florence: A new foundation model for computer\n",
      "vision. CoRR, abs/2111.11432. URL: https://arxiv.org/abs/2111.11432. arXiv:2111.11432.\n",
      "Yun, S., Jeong, M., Kim, R., Kang, J., & Kim, H. J. (2019). Graph transformer networks. In H. M. Wallach, H. Larochelle, A. Beygelzimer,\n",
      "F. d’Alch´e-Buc, E. B. Fox, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural\n",
      "Information Processing Systems, NeurIPS, December 8-14, Vancouver, BC, Canada (pp. 11960–11970).\n",
      "Zellers, R., Bisk, Y., Farhadi, A., & Choi, Y. (2019). From recognition to cognition: Visual commonsense reasoning. In IEEE Conference\n",
      "on Computer Vision and Pattern Recognition, CVPR, Long Beach, CA, USA, June 16-20 (pp. 6720–6731). Computer Vision Foundation\n",
      "/ IEEE.\n",
      "Zhang, C., Patras, P., & Haddadi, H. (2019). Deep learning in mobile and wireless networking: A survey. IEEE Commun. Surv. Tutorials,\n",
      "21, 2224–2287.\n",
      "Zhang, J., Zhao, Y., Saleh, M., & Liu, P. J. (2020a). PEGASUS: pre-training with extracted gap-sentences for abstractive summarization.\n",
      "In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July, Virtual Event (pp. 11328–11339).\n",
      "PMLR volume 119 of Proceedings of Machine Learning Research.\n",
      "Zhang, Q., Lu, H., Sak, H., Tripathi, A., McDermott, E., Koo, S., & Kumar, S. (2020b). Transformer transducer: A streamable speech\n",
      "recognition model with transformer encoders and RNN-T loss. In IEEE International Conference on Acoustics, Speech and Signal\n",
      "Processing, ICASSP, Barcelona, Spain, May 4-8 (pp. 7829–7833). IEEE.\n",
      "Zhang, Y., Park, D. S., Han, W., Qin, J., Gulati, A., Shor, J., Jansen, A., Xu, Y., Huang, Y., Wang, S., Zhou, Z., Li, B., Ma, M., Chan, W.,\n",
      "Yu, J., Wang, Y., Cao, L., Sim, K. C., Ramabhadran, B., Sainath, T. N., Beaufays, F., Chen, Z., Le, Q. V., Chiu, C., Pang, R., & Wu, Y.\n",
      "(2022). Bigssl: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition. IEEE J. Sel. Top. Signal\n",
      "Process., 16, 1519–1532.\n",
      "Zhao, Z., Zheng, P., Xu, S., & Wu, X. (2019). Object detection with deep learning: A review. IEEE Trans. Neural Networks Learn. Syst.,\n",
      "30, 3212–3232.\n",
      "Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T., Torr, P. H. S., & Zhang, L. (2021). Rethinking\n",
      "semantic segmentation from a sequence-to-sequence perspective with transformers. In IEEE Conference on Computer Vision and Pattern\n",
      "Recognition, CVPR, virtual, June 19-25 (pp. 6881–6890). Computer Vision Foundation / IEEE.\n",
      "Zheng, Y., Li, X., Xie, F., & Lu, L. (2020). Improving end-to-end speech synthesis with local recurrent neural network enhanced trans-\n",
      "former. In 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8,\n",
      "2020 (pp. 6734–6738). IEEE.\n",
      "Zhou, H., Guo, J., Zhang, Y., Yu, L., Wang, L., & Yu, Y. (2021a). nnformer: Interleaved transformer for volumetric segmentation. CoRR,\n",
      "abs/2109.03201. URL: https://arxiv.org/abs/2109.03201. arXiv:2109.03201.\n",
      "Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A. L., & Kong, T. (2021b). iBOT: Image BERT pre-training with online tokenizer.\n",
      "CoRR, abs/2111.07832. URL: https://arxiv.org/abs/2111.07832. arXiv:2111.07832.\n",
      "Zhou, S., Li, J., Zhang, K., Wen, M., & Guan, Q. (2020). An accurate ensemble forecasting approach for highly dynamic cloud workload\n",
      "with VMD and r-transformer. IEEE Access, 8, 115992–116003.\n",
      "Zhu, X., Hu, H., Wang, H., Yao, J., Li, W., Ou, D., & Xu, D. (2022). Region aware transformer for automatic breast ultrasound tumor\n",
      "segmentation. In E. Konukoglu, B. H. Menze, A. Venkataraman, C. F. Baumgartner, Q. Dou, & S. Albarqouni (Eds.), International\n",
      "Conference on Medical Imaging with Deep Learning, MIDL, 6-8 July, Zurich, Switzerland (pp. 1523–1537). PMLR volume 172 of\n",
      "Proceedings of Machine Learning Research.\n",
      "Zidan, U., Gaber, M. M., & Abdelsamea, M. M. (2023). Swincup: Cascaded swin transformer for histopathological structures segmentation\n",
      "in colorectal cancer. Expert Syst. Appl., 216, 119452.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53221992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"output.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c60ca432",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transformers.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc257ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "        Processes a PDF, extracts text, images (gets captions), and tables,\n",
    "        and returns a Markdown string.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    print(f\"Processing PDF: {pdf_path} with {len(doc)} pages.\")\n",
    "    c = 0\n",
    "    final_doc = []\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        \n",
    "        page_content = []\n",
    "        page_content.append(f\"\\n## Page {page_num + 1}\\n\")\n",
    "        \n",
    "        # Extracting Text\n",
    "        text = page.get_text(\"text\")\n",
    "        if text.strip():\n",
    "            page_content.append(\"### Text\\n\")\n",
    "            page_content.append(text.strip())\n",
    "            page_content.append(\"\\n\")\n",
    "            \n",
    "        \n",
    "        # Extracting Images and getting caption\n",
    "        image_list = page.get_images(full=True)\n",
    "        print(f\"Page: {page_num}\")\n",
    "        if image_list:\n",
    "            # print(f\"YESS: {page_num}\")\n",
    "            page_content.append(\"### Images\\n\")\n",
    "            \n",
    "            for img in image_list:\n",
    "                \n",
    "                # get the XREF of the image\n",
    "                xref = img[0]\n",
    "\n",
    "                base_image = doc.extract_image(xref)\n",
    "                # base_image is a dictionary with lot of info\n",
    "                \n",
    "                # this is the bytes of the image\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                \n",
    "                # converting it to base 64 to make it easy to use with Together AI\n",
    "                base64_image = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "                \n",
    "                # get the image extension(useful for saving the img)\n",
    "                # image_ext = base_image[\"ext\"]\n",
    "                \n",
    "                # Caption the image and add it to our page_content\n",
    "                caption = get_image_caption(base64_image)\n",
    "                page_content.append(caption)\n",
    "                page_content.append(\"\\n\")\n",
    "\n",
    "        \n",
    "        # Extracting tables\n",
    "        tables = page.find_tables()\n",
    "        if tables.tables:\n",
    "            page_content.append(\"### Table\\n\")\n",
    "            for table in tables.tables:\n",
    "                data = table.extract() # List[List[str]]\n",
    "                md_table = process_tables(data)\n",
    "                page_content.append(md_table)\n",
    "                page_content.append(\"\\n\")\n",
    "\n",
    "        final_doc.extend(page_content)\n",
    "    return final_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "901e9fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF: Data/Applications of Transformers.pdf with 58 pages.\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "process_pdf(pdf_path=PDF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cf4e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path: str) -> list:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    print(f\"Processing PDF: {pdf_path} with {len(doc)} pages.\")\n",
    "    \n",
    "    all_tables = [] \n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        tables = page.find_tables()\n",
    "        \n",
    "        if tables.tables:\n",
    "            for table in tables.tables:\n",
    "                data = table.extract()  # List[List[str]]\n",
    "                all_tables.append(data)\n",
    "    \n",
    "    print(f\"Extracted {len(all_tables)} tables.\")\n",
    "    return all_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "051796b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF: Data/Applications of Transformers.pdf with 58 pages.\n",
      "Extracted 43 tables.\n"
     ]
    }
   ],
   "source": [
    "tables = process_pdf(PDF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "060bc187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Fields of\\nApplication',\n",
       "  'Keywords for Paper Search',\n",
       "  'Tasks Of Application',\n",
       "  None,\n",
       "  'Number of papers',\n",
       "  None],\n",
       " [None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  'Relevant models\\nusing keywords',\n",
       "  'Selected models\\nfor Taxonomy'],\n",
       " ['Natural Language\\nProcessing',\n",
       "  '“Natural Language Processing”,\\n“NLP”,“Text”,“Text Processing”,\\n“Transformer”, “Attention”,\\n“Self-attention”, “multi-head\\nattention”, “Language model”.',\n",
       "  'Language Translation',\n",
       "  None,\n",
       "  '257',\n",
       "  '25'],\n",
       " [None, None, 'Text Classification & Segmentation', None, None, None],\n",
       " [None, None, 'Question Answering', None, None, None],\n",
       " [None, None, 'Text Summarization', None, None, None],\n",
       " [None, None, 'Text Generation', None, None, None],\n",
       " [None, None, 'Natural Language Reasoning', None, None, None],\n",
       " [None, None, 'Automated Symbolic\\nReasoning', None, None, None],\n",
       " ['Computer Vision',\n",
       "  '“Transformer”,“Attention”,\\n“Self-attention”,“Image”,\\n“Natural image”,“medical\\nimage”,“Biomedical”,\\n“health”,“Image processing”,\\n“Computer vision”,“Vision”.',\n",
       "  'Natural Image\\nProcessing',\n",
       "  'Image\\nClassification',\n",
       "  '197',\n",
       "  '27'],\n",
       " [None, None, None, 'Recognition &\\nObject Detection', None, None],\n",
       " [None, None, None, 'Image\\nSegmentation', None, None],\n",
       " [None, None, None, 'Image Generation', None, None],\n",
       " [None, None, 'Medical Image\\nProcessing', 'Image\\nSegmentation', None, None],\n",
       " [None, None, None, 'Image\\nClassification', None, None],\n",
       " [None, None, None, 'Image\\nTranslation', None, None],\n",
       " ['Multi-modal',\n",
       "  '“Transformer”,“Attention”,\\n“Self-attention”,“multi-head\\nattention”,“multimodal”,\\n“multi-modality”,“text-image”,\\n“image-text”,“ video-audio-\\ntext, “text-audio”,“audio-text”,\\n“vision-language”,\\n“language-vision”.',\n",
       "  'Classification &\\nSegmentation',\n",
       "  None,\n",
       "  '94',\n",
       "  '20'],\n",
       " ['Continued on next page', None, None, None, None, None]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65812740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "raw_table = tables[0]\n",
    "\n",
    "# Serialize it as a JSON-like string (so LLM can parse it easily)\n",
    "table_str = json.dumps(raw_table, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9060cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [ {\"role\": \"system\", \"content\": \"You are a Markdown formatting assistant.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "I have extracted a table from a PDF using OCR. It is in the form of a nested list of rows (some cells are `null` meaning continuation of above cell). Please convert this into a clean, readable markdown table.\n",
    "\n",
    "If some cells are meant to span multiple rows, fill in the blanks based on context. Properly handle newlines inside cells too.\n",
    "\n",
    "Here's the table:\n",
    "    {table_str}\n",
    "    Now return the cleaned markdown version of this table.\n",
    "\"\"\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c831f57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Cleaned Markdown Table\n",
      "| Fields of Application | Keywords for Paper Search | Tasks Of Application | Number of papers | Selected models for Taxonomy |\n",
      "| --- | --- | --- | --- | --- |\n",
      "| Natural Language Processing | \"Natural Language Processing\", \"NLP\", \"Text\", \"Text Processing\", \"Transformer\", \"Attention\", \"Self-attention\", \"multi-head attention\", \"Language model\" | Language Translation | 257 | 25 |\n",
      "|  |  | Text Classification & Segmentation |  |  |\n",
      "|  |  | Question Answering |  |  |\n",
      "|  |  | Text Summarization |  |  |\n",
      "|  |  | Text Generation |  |  |\n",
      "|  |  | Natural Language Reasoning |  |  |\n",
      "|  |  | Automated Symbolic Reasoning |  |  |\n",
      "| Computer Vision | \"Transformer\", \"Attention\", \"Self-attention\", \"Image\", \"Natural image\", \"medical image\", \"Biomedical\", \"health\", \"Image processing\", \"Computer vision\", \"Vision\" | Natural Image Processing | 197 | 27 |\n",
      "|  |  |  | Image Classification |  |  |\n",
      "|  |  |  | Recognition & Object Detection |  |  |\n",
      "|  |  |  | Image Segmentation |  |  |\n",
      "|  |  |  | Image Generation |  |  |\n",
      "|  |  | Medical Image Processing | Image Segmentation |  |  |\n",
      "|  |  |  | Image Classification |  |  |\n",
      "|  |  |  | Image Translation |  |  |\n",
      "| Multi-modal | \"Transformer\", \"Attention\", \"Self-attention\", \"multi-head attention\", \"multimodal\", \"multi-modality\", \"text-image\", \"image-text\", \"video-audio-text\", \"text-audio\", \"audio-text\", \"vision-language\", \"language-vision\" | Classification & Segmentation | 94 | 20 |\n",
      "\n",
      "Note: The \"Relevant models using keywords\" column has been removed as it seemed to be a header for the \"Selected models for Taxonomy\" column. The \"Continued on next page\" row has also been removed as it's not part of the actual table data. \n",
      "\n",
      "Also, note that the table has been formatted to make the rows with null values span the cells above them, as per the context. Newlines inside cells have been preserved.\n"
     ]
    }
   ],
   "source": [
    "client = Together() \n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "    messages=messages\n",
    ")\n",
    "md_table = response.choices[0].message.content\n",
    "print(md_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "644dfed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_table(table):\n",
    "    table_str = json.dumps(raw_table, indent=2)\n",
    "    client = Together()\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "    messages=messages\n",
    "    )\n",
    "    md_table = response.choices[0].message.content\n",
    "    return md_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7e8f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"table_trial.md\", \"w\") as f:\n",
    "    f.write(md_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3215de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
