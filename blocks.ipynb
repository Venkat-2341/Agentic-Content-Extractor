{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be7d82bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a1a3f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_caption(image_bytes: bytes) -> str:\n",
    "    \"\"\"VLM captioning. Replace with actual VLM call.\"\"\"\n",
    "    return \"This is a placeholder caption for an image.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d458073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_markdown(tabele_data: list) -> str:\n",
    "    \"\"\"Converts a table (list of lists) to a Markdown format\"\"\"\n",
    "    return \"This is a placeholder for converted markdown of a table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8ac6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "        Processes a PDF, extracts text, images (gets captions), and tables,\n",
    "        and returns a Markdown string.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    print(f\"Processing PDF: {pdf_path} with {len(doc)} pages.\")\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        \n",
    "        page_content = []\n",
    "        page_content.append(f\"\\n## Page {page_num + 1}\\n\")\n",
    "        \n",
    "        # Extracting Text\n",
    "        text = page.get_text(\"text\")\n",
    "        if text.strip():\n",
    "            page_content.append(\"### Text\\n\")\n",
    "            page_content.append(text.strip())\n",
    "            page_content.append(\"\\n\")\n",
    "            \n",
    "        \n",
    "        # Extracting Images and getting caption\n",
    "        image_list = page.get_images(full = True)\n",
    "        if image_list:\n",
    "            page_content.append(\"### Images\\n\")\n",
    "            # Write logic here to caption the images\n",
    "            \n",
    "        \n",
    "        # Extracting tables\n",
    "        # PyMuPDF's table extraction is heuristic.\n",
    "        # For complex tables, check pdfplumber or camelot-py.\n",
    "        tables = page.find_tables()\n",
    "        if tables.tables:\n",
    "            page_content.append(\"### Table\\n\")\n",
    "            # Write logic here to convert table into plain text\n",
    "                    \n",
    "        print(page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be6e3e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = 'Data/Luong Attention.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d973d83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF: Data/Luong Attention.pdf with 11 pages.\n",
      "['\\n## Page 1\\n', '### Text\\n', 'arXiv:1508.04025v5  [cs.CL]  20 Sep 2015\\nEffective Approaches to Attention-based Neural Machine Translation\\nMinh-Thang Luong\\nHieu Pham\\nChristopher D. Manning\\nComputer Science Department, Stanford University, Stanford, CA 94305\\n{lmthang,hyhieu,manning}@stanford.edu\\nAbstract\\nAn attentional mechanism has lately been\\nused to improve neural machine transla-\\ntion (NMT) by selectively focusing on\\nparts of the source sentence during trans-\\nlation.\\nHowever, there has been little\\nwork exploring useful architectures for\\nattention-based NMT. This paper exam-\\nines two simple and effective classes of at-\\ntentional mechanism: a global approach\\nwhich always attends to all source words\\nand a local one that only looks at a subset\\nof source words at a time. We demonstrate\\nthe effectiveness of both approaches on the\\nWMT translation tasks between English\\nand German in both directions. With local\\nattention, we achieve a signiﬁcant gain of\\n5.0 BLEU points over non-attentional sys-\\ntems that already incorporate known tech-\\nniques such as dropout.\\nOur ensemble\\nmodel using different attention architec-\\ntures yields a new state-of-the-art result in\\nthe WMT’15 English to German transla-\\ntion task with 25.9 BLEU points, an im-\\nprovement of 1.0 BLEU points over the\\nexisting best system backed by NMT and\\nan n-gram reranker.1\\n1\\nIntroduction\\nNeural Machine Translation (NMT) achieved\\nstate-of-the-art performances in large-scale trans-\\nlation tasks such as from English to French\\n(Luong et al., 2015)\\nand\\nEnglish\\nto\\nGerman\\n(Jean et al., 2015). NMT is appealing since it re-\\nquires minimal domain knowledge and is concep-\\ntually simple. The model by Luong et al. (2015)\\nreads through all the source words until the end-of-\\nsentence symbol <eos> is reached. It then starts\\n1All our code and models are publicly available at\\nhttp://nlp.stanford.edu/projects/nmt.\\nB\\nC\\nD\\n<eos>\\nX\\nY\\nZ\\nX\\nY\\nZ\\n<eos>\\nA\\nFigure 1: Neural machine translation – a stack-\\ning recurrent architecture for translating a source\\nsequence A B C D into a target sequence X Y\\nZ. Here, <eos> marks the end of a sentence.\\nemitting one target word at a time, as illustrated in\\nFigure 1. NMT is often a large neural network that\\nis trained in an end-to-end fashion and has the abil-\\nity to generalize well to very long word sequences.\\nThis means the model does not have to explicitly\\nstore gigantic phrase tables and language models\\nas in the case of standard MT; hence, NMT has\\na small memory footprint. Lastly, implementing\\nNMT decoders is easy unlike the highly intricate\\ndecoders in standard MT (Koehn et al., 2003).\\nIn parallel, the concept of “attention” has\\ngained popularity recently in training neural net-\\nworks, allowing models to learn alignments be-\\ntween different modalities, e.g., between image\\nobjects and agent actions in the dynamic con-\\ntrol problem (Mnih et al., 2014), between speech\\nframes and text in the speech recognition task\\n(?), or between visual features of a picture and\\nits text description in the image caption gener-\\nation task (Xu et al., 2015).\\nIn the context of\\nNMT, Bahdanau et al. (2015) has successfully ap-\\nplied such attentional mechanism to jointly trans-\\nlate and align words. To the best of our knowl-\\nedge, there has not been any other work exploring\\nthe use of attention-based architectures for NMT.\\nIn this work, we design, with simplicity and ef-', '\\n']\n",
      "['\\n## Page 2\\n', '### Text\\n', 'fectiveness in mind, two novel types of attention-\\nbased models: a global approach in which all\\nsource words are attended and a local one whereby\\nonly a subset of source words are considered at a\\ntime. The former approach resembles the model\\nof (Bahdanau et al., 2015) but is simpler architec-\\nturally. The latter can be viewed as an interesting\\nblend between the hard and soft attention models\\nproposed in (Xu et al., 2015): it is computation-\\nally less expensive than the global model or the\\nsoft attention; at the same time, unlike the hard at-\\ntention, the local attention is differentiable almost\\neverywhere, making it easier to implement and\\ntrain.2\\nBesides, we also examine various align-\\nment functions for our attention-based models.\\nExperimentally, we demonstrate that both of\\nour approaches are effective in the WMT trans-\\nlation tasks between English and German in both\\ndirections. Our attentional models yield a boost\\nof up to 5.0 BLEU over non-attentional systems\\nwhich already incorporate known techniques such\\nas dropout.\\nFor English to German translation,\\nwe achieve new state-of-the-art (SOTA) results\\nfor both WMT’14 and WMT’15, outperforming\\nprevious SOTA systems, backed by NMT mod-\\nels and n-gram LM rerankers, by more than 1.0\\nBLEU. We conduct extensive analysis to evaluate\\nour models in terms of learning, the ability to han-\\ndle long sentences, choices of attentional architec-\\ntures, alignment quality, and translation outputs.\\n2\\nNeural Machine Translation\\nA neural machine translation system is a neural\\nnetwork that directly models the conditional prob-\\nability p(y|x) of translating a source sentence,\\nx1, . . . , xn, to a target sentence, y1, . . . , ym.3 A\\nbasic form of NMT consists of two components:\\n(a) an encoder which computes a representation s\\nfor each source sentence and (b) a decoder which\\ngenerates one target word at a time and hence de-\\ncomposes the conditional probability as:\\nlog p(y|x) =\\nXm\\nj=1 log p (yj|y<j, s)\\n(1)\\nA\\nnatural\\nchoice\\nto\\nmodel\\nsuch\\na\\nde-\\ncomposition\\nin\\nthe\\ndecoder\\nis\\nto\\nuse\\na\\n2There is a recent work by Gregor et al. (2015), which is\\nvery similar to our local attention and applied to the image\\ngeneration task. However, as we detail later, our model is\\nmuch simpler and can achieve good performance for NMT.\\n3All sentences are assumed to terminate with a special\\n“end-of-sentence” token <eos>.\\nrecurrent\\nneural\\nnetwork\\n(RNN)\\narchitec-\\nture,\\nwhich most of the recent NMT work\\nsuch\\nas\\n(Kalchbrenner and Blunsom, 2013;\\nSutskever et al., 2014;\\nCho et al., 2014;\\nBahdanau et al., 2015;\\nLuong et al., 2015;\\nJean et al., 2015) have in common.\\nThey, how-\\never, differ in terms of which RNN architectures\\nare used for the decoder and how the encoder\\ncomputes the source sentence representation s.\\nKalchbrenner and Blunsom (2013)\\nused\\nan\\nRNN with the standard hidden unit for the\\ndecoder and a convolutional neural network for\\nencoding the source sentence representation. On\\nthe other hand, both Sutskever et al. (2014) and\\nLuong et al. (2015) stacked multiple layers of an\\nRNN with a Long Short-Term Memory (LSTM)\\nhidden unit for both the encoder and the decoder.\\nCho et al. (2014),\\nBahdanau et al. (2015),\\nand\\nJean et al. (2015) all adopted a different version of\\nthe RNN with an LSTM-inspired hidden unit, the\\ngated recurrent unit (GRU), for both components.4\\nIn more detail, one can parameterize the proba-\\nbility of decoding each word yj as:\\np (yj|y<j, s) = softmax (g (hj))\\n(2)\\nwith g being the transformation function that out-\\nputs a vocabulary-sized vector.5 Here, hj is the\\nRNN hidden unit, abstractly computed as:\\nhj = f(hj−1, s),\\n(3)\\nwhere f\\ncomputes the current hidden state\\ngiven the previous hidden state and can be\\neither a vanilla RNN unit, a GRU, or an LSTM\\nunit.\\nIn\\n(Kalchbrenner and Blunsom, 2013;\\nSutskever et al., 2014;\\nCho et al., 2014;\\nLuong et al., 2015),\\nthe\\nsource\\nrepresenta-\\ntion s is only used once to initialize\\nthe\\ndecoder hidden state.\\nOn the other hand, in\\n(Bahdanau et al., 2015;\\nJean et al., 2015)\\nand\\nthis work, s, in fact, implies a set of source\\nhidden states which are consulted throughout the\\nentire course of the translation process. Such an\\napproach is referred to as an attention mechanism,\\nwhich we will discuss next.\\nIn this work, following (Sutskever et al., 2014;\\nLuong et al., 2015), we use the stacking LSTM\\narchitecture for our NMT systems, as illustrated\\n4They all used a single RNN layer except for the latter two\\nworks which utilized a bidirectional RNN for the encoder.\\n5One can provide g with other inputs such as the currently\\npredicted word yj as in (Bahdanau et al., 2015).', '\\n']\n",
      "['\\n## Page 3\\n', '### Text\\n', 'in Figure 1. We use the LSTM unit deﬁned in\\n(Zaremba et al., 2015). Our training objective is\\nformulated as follows:\\nJt =\\nX\\n(x,y)∈D −log p(y|x)\\n(4)\\nwith D being our parallel training corpus.\\n3\\nAttention-based Models\\nOur various attention-based models are classifed\\ninto two broad categories, global and local. These\\nclasses differ in terms of whether the “attention”\\nis placed on all source positions or on only a few\\nsource positions. We illustrate these two model\\ntypes in Figure 2 and 3 respectively.\\nCommon to these two types of models is the fact\\nthat at each time step t in the decoding phase, both\\napproaches ﬁrst take as input the hidden state ht\\nat the top layer of a stacking LSTM. The goal is\\nthen to derive a context vector ct that captures rel-\\nevant source-side information to help predict the\\ncurrent target word yt. While these models differ\\nin how the context vector ct is derived, they share\\nthe same subsequent steps.\\nSpeciﬁcally, given the target hidden state ht and\\nthe source-side context vector ct, we employ a\\nsimple concatenation layer to combine the infor-\\nmation from both vectors to produce an attentional\\nhidden state as follows:\\n˜ht = tanh(Wc[ct; ht])\\n(5)\\nThe attentional vector ˜ht is then fed through the\\nsoftmax layer to produce the predictive distribu-\\ntion formulated as:\\np(yt|y<t, x) = softmax(Ws˜ht)\\n(6)\\nWe now detail how each model type computes\\nthe source-side context vector ct.\\n3.1\\nGlobal Attention\\nThe idea of a global attentional model is to con-\\nsider all the hidden states of the encoder when de-\\nriving the context vector ct. In this model type,\\na variable-length alignment vector at, whose size\\nequals the number of time steps on the source side,\\nis derived by comparing the current target hidden\\nstate ht with each source hidden state ¯hs:\\nat(s) = align(ht, ¯hs)\\n(7)\\n=\\nexp\\n\\x00score(ht, ¯hs)\\n\\x01\\nP\\ns′ exp\\n\\x00score(ht, ¯hs′)\\n\\x01\\nyt\\n˜ht\\nct\\nat\\nht\\n¯hs\\nGlobal align weights\\nAttention Layer\\nContext vector\\nFigure 2: Global attentional model – at each time\\nstep t, the model infers a variable-length align-\\nment weight vector at based on the current target\\nstate ht and all source states ¯hs. A global context\\nvector ct is then computed as the weighted aver-\\nage, according to at, over all the source states.\\nHere, score is referred as a content-based function\\nfor which we consider three different alternatives:\\nscore(ht, ¯hs)=\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\nh⊤\\nt ¯hs\\ndot\\nh⊤\\nt Wa¯hs\\ngeneral\\nv⊤\\na tanh\\n\\x00Wa[ht; ¯hs]\\n\\x01\\nconcat\\nBesides, in our early attempts to build attention-\\nbased models, we use a location-based function\\nin which the alignment scores are computed from\\nsolely the target hidden state ht as follows:\\nat = softmax(Waht)\\nlocation\\n(8)\\nGiven the alignment vector as weights, the context\\nvector ct is computed as the weighted average over\\nall the source hidden states.6\\nComparison to (Bahdanau et al., 2015) – While\\nour global attention approach is similar in spirit\\nto the model proposed by Bahdanau et al. (2015),\\nthere are several key differences which reﬂect how\\nwe have both simpliﬁed and generalized from\\nthe original model.\\nFirst, we simply use hid-\\nden states at the top LSTM layers in both the\\nencoder and decoder as illustrated in Figure 2.\\nBahdanau et al. (2015), on the other hand, use\\nthe concatenation of the forward and backward\\nsource hidden states in the bi-directional encoder\\n6Eq. (8) implies that all alignment vectors at are of the\\nsame length. For short sentences, we only use the top part of\\nat and for long sentences, we ignore words near the end.', '\\n', '### Table\\n']\n",
      "['\\n## Page 4\\n', '### Text\\n', 'yt\\n˜ht\\nct\\nat\\nht\\npt\\n¯hs\\nAttention Layer\\nContext vector\\nLocal weights\\nAligned position\\nFigure 3: Local attention model – the model ﬁrst\\npredicts a single aligned position pt for the current\\ntarget word. A window centered around the source\\nposition pt is then used to compute a context vec-\\ntor ct, a weighted average of the source hidden\\nstates in the window. The weights at are inferred\\nfrom the current target state ht and those source\\nstates ¯hs in the window.\\nand target hidden states in their non-stacking uni-\\ndirectional decoder. Second, our computation path\\nis simpler; we go from ht →at →ct →˜ht\\nthen make a prediction as detailed in Eq. (5),\\nEq. (6), and Figure 2.\\nOn the other hand, at\\nany time t, Bahdanau et al. (2015) build from the\\nprevious hidden state ht−1\\n→at →ct →\\nht, which, in turn, goes through a deep-output\\nand a maxout layer before making predictions.7\\nLastly, Bahdanau et al. (2015) only experimented\\nwith one alignment function, the concat product;\\nwhereas we show later that the other alternatives\\nare better.\\n3.2\\nLocal Attention\\nThe global attention has a drawback that it has to\\nattend to all words on the source side for each tar-\\nget word, which is expensive and can potentially\\nrender it impractical to translate longer sequences,\\ne.g., paragraphs or documents.\\nTo address this\\ndeﬁciency, we propose a local attentional mech-\\nanism that chooses to focus only on a small subset\\nof the source positions per target word.\\nThis model takes inspiration from the tradeoff\\nbetween the soft and hard attentional models pro-\\nposed by Xu et al. (2015) to tackle the image cap-\\ntion generation task. In their work, soft attention\\n7We will refer to this difference again in Section 3.3.\\nrefers to the global attention approach in which\\nweights are placed “softly” over all patches in the\\nsource image.\\nThe hard attention, on the other\\nhand, selects one patch of the image to attend to at\\na time. While less expensive at inference time, the\\nhard attention model is non-differentiable and re-\\nquires more complicated techniques such as vari-\\nance reduction or reinforcement learning to train.\\nOur local attention mechanism selectively fo-\\ncuses on a small window of context and is differ-\\nentiable. This approach has an advantage of avoid-\\ning the expensive computation incurred in the soft\\nattention and at the same time, is easier to train\\nthan the hard attention approach. In concrete de-\\ntails, the model ﬁrst generates an aligned position\\npt for each target word at time t. The context vec-\\ntor ct is then derived as a weighted average over\\nthe set of source hidden states within the window\\n[pt−D, pt+D]; D is empirically selected.8 Unlike\\nthe global approach, the local alignment vector at\\nis now ﬁxed-dimensional, i.e., ∈R2D+1. We con-\\nsider two variants of the model as below.\\nMonotonic alignment (local-m) – we simply set\\npt = t assuming that source and target sequences\\nare roughly monotonically aligned. The alignment\\nvector at is deﬁned according to Eq. (7).9\\nPredictive alignment (local-p) – instead of as-\\nsuming monotonic alignments, our model predicts\\nan aligned position as follows:\\npt = S · sigmoid(v⊤\\np tanh(Wpht)),\\n(9)\\nWp and vp are the model parameters which will\\nbe learned to predict positions. S is the source sen-\\ntence length. As a result of sigmoid, pt ∈[0, S].\\nTo favor alignment points near pt, we place a\\nGaussian distribution centered around pt . Specif-\\nically, our alignment weights are now deﬁned as:\\nat(s) = align(ht, ¯hs) exp\\n\\x12\\n−(s −pt)2\\n2σ2\\n\\x13\\n(10)\\nWe use the same align function as in Eq. (7) and\\nthe standard deviation is empirically set as σ = D\\n2 .\\nNote that pt is a real nummber; whereas s is an\\ninteger within the window centered at pt.10\\n8If the window crosses the sentence boundaries, we sim-\\nply ignore the outside part and consider words in the window.\\n9local-m is the same as the global model except that the\\nvector at is ﬁxed-length and shorter.\\n10local-p is similar to the local-m model except that we dy-\\nnamically compute pt and use a truncated Gaussian distribu-\\ntion to modify the original alignment weights align(ht, ¯hs)\\nas shown in Eq. (10). By utilizing pt to derive at, we can\\ncompute backprop gradients for Wp and vp. This model is\\ndifferentiable almost everywhere.', '\\n', '### Table\\n']\n",
      "['\\n## Page 5\\n', '### Text\\n', '˜ht\\nAttention Layer\\nB\\nC\\nD\\n<eos>\\nX\\nY\\nZ\\nX\\nY\\nZ\\n<eos>\\nA\\nFigure 4: Input-feeding approach – Attentional\\nvectors ˜ht are fed as inputs to the next time steps to\\ninform the model about past alignment decisions.\\nComparison to (Gregor et al., 2015) – have pro-\\nposed a selective attention mechanism, very simi-\\nlar to our local attention, for the image generation\\ntask. Their approach allows the model to select an\\nimage patch of varying location and zoom. We,\\ninstead, use the same “zoom” for all target posi-\\ntions, which greatly simpliﬁes the formulation and\\nstill achieves good performance.\\n3.3\\nInput-feeding Approach\\nIn our proposed global and local approaches,\\nthe attentional decisions are made independently,\\nwhich is suboptimal. Whereas, in standard MT,\\na coverage set is often maintained during the\\ntranslation process to keep track of which source\\nwords have been translated. Likewise, in atten-\\ntional NMTs, alignment decisions should be made\\njointly taking into account past alignment infor-\\nmation.\\nTo address that, we propose an input-\\nfeeding approach in which attentional vectors ˜ht\\nare concatenated with inputs at the next time steps\\nas illustrated in Figure 4.11 The effects of hav-\\ning such connections are two-fold: (a) we hope\\nto make the model fully aware of previous align-\\nment choices and (b) we create a very deep net-\\nwork spanning both horizontally and vertically.\\nComparison\\nto\\nother\\nwork\\n–\\nBahdanau et al. (2015)\\nuse\\ncontext\\nvectors,\\nsimilar to our ct, in building subsequent hidden\\nstates, which can also achieve the “coverage”\\neffect. However, there has not been any analysis\\nof whether such connections are useful as done\\n11If n is the number of LSTM cells, the input size of the\\nﬁrst LSTM layer is 2n; those of subsequent layers are n.\\nin this work. Also, our approach is more general;\\nas illustrated in Figure 4, it can be applied to\\ngeneral stacking recurrent architectures, including\\nnon-attentional models.\\nXu et al. (2015) propose a doubly attentional\\napproach with an additional constraint added to\\nthe training objective to make sure the model pays\\nequal attention to all parts of the image during the\\ncaption generation process. Such a constraint can\\nalso be useful to capture the coverage set effect\\nin NMT that we mentioned earlier. However, we\\nchose to use the input-feeding approach since it\\nprovides ﬂexibility for the model to decide on any\\nattentional constraints it deems suitable.\\n4\\nExperiments\\nWe evaluate the effectiveness of our models\\non the WMT translation tasks between En-\\nglish and German in both directions.\\nnew-\\nstest2013 (3000 sentences) is used as a develop-\\nment set to select our hyperparameters. Transla-\\ntion performances are reported in case-sensitive\\nBLEU (Papineni et al., 2002) on newstest2014\\n(2737 sentences) and newstest2015 (2169 sen-\\ntences). Following (Luong et al., 2015), we report\\ntranslation quality using two types of BLEU: (a)\\ntokenized12 BLEU to be comparable with existing\\nNMT work and (b) NIST13 BLEU to be compara-\\nble with WMT results.\\n4.1\\nTraining Details\\nAll our models are trained on the WMT’14 train-\\ning data consisting of 4.5M sentences pairs (116M\\nEnglish words, 110M German words).\\nSimilar\\nto (Jean et al., 2015), we limit our vocabularies to\\nbe the top 50K most frequent words for both lan-\\nguages. Words not in these shortlisted vocabular-\\nies are converted into a universal token <unk>.\\nWhen training our NMT systems, following\\n(Bahdanau et al., 2015; Jean et al., 2015), we ﬁl-\\nter out sentence pairs whose lengths exceed\\n50 words and shufﬂe mini-batches as we pro-\\nceed.\\nOur stacking LSTM models have 4 lay-\\ners, each with 1000 cells, and 1000-dimensional\\nembeddings.\\nWe follow (Sutskever et al., 2014;\\nLuong et al., 2015) in training NMT with similar\\nsettings: (a) our parameters are uniformly initial-\\nized in [−0.1, 0.1], (b) we train for 10 epochs us-\\n12All texts are tokenized with tokenizer.perl and\\nBLEU scores are computed with multi-bleu.perl.\\n13With the mteval-v13a script as per WMT guideline.', '\\n', '### Table\\n']\n",
      "['\\n## Page 6\\n', '### Text\\n', 'System\\nPpl\\nBLEU\\nWinning WMT’14 system – phrase-based + large LM (Buck et al., 2014)\\n20.7\\nExisting NMT systems\\nRNNsearch (Jean et al., 2015)\\n16.5\\nRNNsearch + unk replace (Jean et al., 2015)\\n19.0\\nRNNsearch + unk replace + large vocab + ensemble 8 models (Jean et al., 2015)\\n21.6\\nOur NMT systems\\nBase\\n10.6\\n11.3\\nBase + reverse\\n9.9\\n12.6 (+1.3)\\nBase + reverse + dropout\\n8.1\\n14.0 (+1.4)\\nBase + reverse + dropout + global attention (location)\\n7.3\\n16.8 (+2.8)\\nBase + reverse + dropout + global attention (location) + feed input\\n6.4\\n18.1 (+1.3)\\nBase + reverse + dropout + local-p attention (general) + feed input\\n5.9\\n19.0 (+0.9)\\nBase + reverse + dropout + local-p attention (general) + feed input + unk replace\\n20.9 (+1.9)\\nEnsemble 8 models + unk replace\\n23.0 (+2.1)\\nTable 1: WMT’14 English-German results – shown are the perplexities (ppl) and the tokenized BLEU\\nscores of various systems on newstest2014. We highlight the best system in bold and give progressive\\nimprovements in italic between consecutive systems. local-p referes to the local attention with predictive\\nalignments. We indicate for each attention model the alignment score function used in pararentheses.\\ning plain SGD, (c) a simple learning rate sched-\\nule is employed – we start with a learning rate of\\n1; after 5 epochs, we begin to halve the learning\\nrate every epoch, (d) our mini-batch size is 128,\\nand (e) the normalized gradient is rescaled when-\\never its norm exceeds 5.\\nAdditionally, we also\\nuse dropout with probability 0.2 for our LSTMs as\\nsuggested by (Zaremba et al., 2015). For dropout\\nmodels, we train for 12 epochs and start halving\\nthe learning rate after 8 epochs. For local atten-\\ntion models, we empirically set the window size\\nD = 10.\\nOur code is implemented in MATLAB. When\\nrunning on a single GPU device Tesla K40, we\\nachieve a speed of 1K target words per second.\\nIt takes 7–10 days to completely train a model.\\n4.2\\nEnglish-German Results\\nWe compare our NMT systems in the English-\\nGerman task with various other systems. These\\ninclude\\nthe\\nwinning\\nsystem\\nin\\nWMT’14\\n(Buck et al., 2014),\\na\\nphrase-based\\nsystem\\nwhose language models were trained on a huge\\nmonolingual text, the Common Crawl corpus.\\nFor end-to-end NMT systems, to the best of\\nour knowledge, (Jean et al., 2015) is the only\\nwork experimenting with this language pair and\\ncurrently the SOTA system.\\nWe only present\\nresults for some of our attention models and will\\nlater analyze the rest in Section 5.\\nAs shown\\nin Table 1,\\nwe achieve\\npro-\\ngressive improvements when (a) reversing the\\nsource sentence, +1.3 BLEU, as proposed in\\n(Sutskever et al., 2014) and (b) using dropout,\\n+1.4 BLEU. On top of that, (c) the global atten-\\ntion approach gives a signiﬁcant boost of +2.8\\nBLEU, making our model slightly better than the\\nbase attentional system of Bahdanau et al. (2015)\\n(row RNNSearch).\\nWhen (d) using the input-\\nfeeding approach, we seize another notable gain\\nof +1.3 BLEU and outperform their system. The\\nlocal attention model with predictive alignments\\n(row local-p) proves to be even better, giving\\nus a further improvement of +0.9 BLEU on top\\nof the global attention model.\\nIt is interest-\\ning to observe the trend previously reported in\\n(Luong et al., 2015) that perplexity strongly corre-\\nlates with translation quality. In total, we achieve\\na signiﬁcant gain of 5.0 BLEU points over the\\nnon-attentional baseline, which already includes\\nknown techniques such as source reversing and\\ndropout.\\nThe unknown replacement technique proposed\\nin (Luong et al., 2015; Jean et al., 2015) yields an-\\nother nice gain of +1.9 BLEU, demonstrating that\\nour attentional models do learn useful alignments\\nfor unknown works.\\nFinally, by ensembling 8\\ndifferent models of various settings, e.g., using\\ndifferent attention approaches, with and without\\ndropout etc., we were able to achieve a new SOTA\\nresult of 23.0 BLEU, outperforming the existing', '\\n']\n",
      "['\\n## Page 7\\n', '### Text\\n', 'best system (Jean et al., 2015) by +1.4 BLEU.\\nSystem\\nBLEU\\nTop – NMT + 5-gram rerank (Montreal)\\n24.9\\nOur ensemble 8 models + unk replace\\n25.9\\nTable 2: WMT’15 English-German results –\\nNIST BLEU scores of the winning entry in\\nWMT’15 and our best one on newstest2015.\\nLatest results in WMT’15 – despite the fact that\\nour models were trained on WMT’14 with slightly\\nless data, we test them on newstest2015 to demon-\\nstrate that they can generalize well to different test\\nsets. As shown in Table 2, our best system es-\\ntablishes a new SOTA performance of 25.9 BLEU,\\noutperforming the existing best system backed by\\nNMT and a 5-gram LM reranker by +1.0 BLEU.\\n4.3\\nGerman-English Results\\nWe carry out a similar set of experiments for the\\nWMT’15 translation task from German to En-\\nglish. While our systems have not yet matched\\nthe performance of the SOTA system, we never-\\ntheless show the effectiveness of our approaches\\nwith large and progressive gains in terms of BLEU\\nas illustrated in Table 3. The attentional mech-\\nanism gives us +2.2 BLEU gain and on top of\\nthat, we obtain another boost of up to +1.0 BLEU\\nfrom the input-feeding approach. Using a better\\nalignment function, the content-based dot product\\none, together with dropout yields another gain of\\n+2.7 BLEU. Lastly, when applying the unknown\\nword replacement technique, we seize an addi-\\ntional +2.1 BLEU, demonstrating the usefulness\\nof attention in aligning rare words.\\n5\\nAnalysis\\nWe conduct extensive analysis to better understand\\nour models in terms of learning, the ability to han-\\ndle long sentences, choices of attentional architec-\\ntures, and alignment quality. All results reported\\nhere are on English-German newstest2014.\\n5.1\\nLearning curves\\nWe compare models built on top of one another as\\nlisted in Table 1. It is pleasant to observe in Fig-\\nure 5 a clear separation between non-attentional\\nand attentional models.\\nThe input-feeding ap-\\nproach and the local attention model also demon-\\nstrate their abilities in driving the test costs lower.\\nThe non-attentional model with dropout (the blue\\nSystem\\nPpl.\\nBLEU\\nWMT’15 systems\\nSOTA – phrase-based (Edinburgh)\\n29.2\\nNMT + 5-gram rerank (MILA)\\n27.6\\nOur NMT systems\\nBase (reverse)\\n14.3\\n16.9\\n+ global (location)\\n12.7\\n19.1 (+2.2)\\n+ global (location) + feed\\n10.9\\n20.1 (+1.0)\\n+ global (dot) + drop + feed\\n9.7\\n22.8 (+2.7)\\n+ global (dot) + drop + feed + unk\\n24.9 (+2.1)\\nTable 3: WMT’15 German-English results –\\nperformances of various systems (similar to Ta-\\nble 1). The base system already includes source\\nreversing on which we add global attention,\\ndropout, input feeding, and unk replacement.\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n1.2\\n1.4\\n1.6\\n1.8\\nx 10\\n5\\n2\\n3\\n4\\n5\\n6\\nMini−batches\\nTest cost\\n \\n \\nbasic\\nbasic+reverse\\nbasic+reverse+dropout\\nbasic+reverse+dropout+globalAttn\\nbasic+reverse+dropout+globalAttn+feedInput\\nbasic+reverse+dropout+pLocalAttn+feedInput\\nFigure 5: Learning curves – test cost (ln perplex-\\nity) on newstest2014 for English-German NMTs\\nas training progresses.\\n+ curve) learns slower than other non-dropout\\nmodels, but as time goes by, it becomes more ro-\\nbust in terms of minimizing test errors.\\n5.2\\nEffects of Translating Long Sentences\\nWe follow (Bahdanau et al., 2015) to group sen-\\ntences of similar lengths together and compute\\na BLEU score per group.\\nFigure 6 shows that\\nour attentional models are more effective than the\\nnon-attentional one in handling long sentences:\\nthe quality does not degrade as sentences become\\nlonger. Our best model (the blue + curve) outper-\\nforms all other systems in all length buckets.\\n5.3\\nChoices of Attentional Architectures\\nWe examine different attention models (global,\\nlocal-m, local-p) and different alignment func-\\ntions (location, dot, general, concat) as described\\nin Section 3. Due to limited resources, we can-\\nnot run all the possible combinations. However,\\nresults in Table 4 do give us some idea about dif-\\nferent choices. The location-based function does', '\\n']\n",
      "['\\n## Page 8\\n', '### Text\\n', '10\\n20\\n30\\n40\\n50\\n60\\n70\\n10\\n15\\n20\\n25\\nSent Lengths\\nBLEU\\t\\t\\t\\t\\t\\n \\n \\nours, no attn (BLEU 13.9)\\nours, local−p attn (BLEU 20.9)\\nours, best system (BLEU 23.0)\\nWMT’14 best (BLEU 20.7)\\nJeans et al., 2015 (BLEU 21.6)\\nFigure 6: Length Analysis – translation qualities\\nof different systems as sentences become longer.\\nSystem\\nPpl\\nBLEU\\nBefore\\nAfter unk\\nglobal (location)\\n6.4\\n18.1\\n19.3 (+1.2)\\nglobal (dot)\\n6.1\\n18.6\\n20.5 (+1.9)\\nglobal (general)\\n6.1\\n17.3\\n19.1 (+1.8)\\nlocal-m (dot)\\n>7.0\\nx\\nx\\nlocal-m (general)\\n6.2\\n18.6\\n20.4 (+1.8)\\nlocal-p (dot)\\n6.6\\n18.0\\n19.6 (+1.9)\\nlocal-p (general)\\n5.9\\n19\\n20.9 (+1.9)\\nTable 4:\\nAttentional Architectures – perfor-\\nmances of different attentional models. We trained\\ntwo local-m (dot) models; both have ppl > 7.0.\\nnot learn good alignments: the global (location)\\nmodel can only obtain a small gain when per-\\nforming unknown word replacement compared to\\nusing other alignment functions.14\\nFor content-\\nbased functions, our implementation concat does\\nnot yield good performances and more analysis\\nshould be done to understand the reason.15 It is\\ninteresting to observe that dot works well for the\\nglobal attention and general is better for the local\\nattention. Among the different models, the local\\nattention model with predictive alignments (local-\\np) is best, both in terms of perplexities and BLEU.\\n5.4\\nAlignment Quality\\nA by-product of attentional models are word align-\\nments. While (Bahdanau et al., 2015) visualized\\n14There is a subtle difference in how we retrieve align-\\nments for the different alignment functions. At time step t in\\nwhich we receive yt−1 as input and then compute ht, at, ct,\\nand ˜ht before predicting yt, the alignment vector at is used\\nas alignment weights for (a) the predicted word yt in the\\nlocation-based alignment functions and (b) the input word\\nyt−1 in the content-based functions.\\n15With concat, the perplexities achieved by different mod-\\nels are 6.7 (global), 7.1 (local-m), and 7.1 (local-p). Such\\nhigh perplexities could be due to the fact that we simplify the\\nmatrix Wa to set the part that corresponds to ¯hs to identity.\\nMethod\\nAER\\nglobal (location)\\n0.39\\nlocal-m (general)\\n0.34\\nlocal-p (general)\\n0.36\\nensemble\\n0.34\\nBerkeley Aligner\\n0.32\\nTable 6: AER scores – results of various models\\non the RWTH English-German alignment data.\\nalignments for some sample sentences and ob-\\nserved gains in translation quality as an indica-\\ntion of a working attention model, no work has as-\\nsessed the alignments learned as a whole. In con-\\ntrast, we set out to evaluate the alignment quality\\nusing the alignment error rate (AER) metric.\\nGiven the gold alignment data provided by\\nRWTH for 508 English-German Europarl sen-\\ntences, we “force” decode our attentional models\\nto produce translations that match the references.\\nWe extract only one-to-one alignments by select-\\ning the source word with the highest alignment\\nweight per target word. Nevertheless, as shown in\\nTable 6, we were able to achieve AER scores com-\\nparable to the one-to-many alignments obtained\\nby the Berkeley aligner (Liang et al., 2006).16\\nWe also found that the alignments produced by\\nlocal attention models achieve lower AERs than\\nthose of the global one. The AER obtained by the\\nensemble, while good, is not better than the local-\\nm AER, suggesting the well-known observation\\nthat AER and translation scores are not well cor-\\nrelated (Fraser and Marcu, 2007). We show some\\nalignment visualizations in Appendix A.\\n5.5\\nSample Translations\\nWe show in Table 5 sample translations in both\\ndirections.\\nIt it appealing to observe the ef-\\nfect of attentional models in correctly translating\\nnames such as “Miranda Kerr” and “Roger Dow”.\\nNon-attentional models, while producing sensi-\\nble names from a language model perspective,\\nlack the direct connections from the source side\\nto make correct translations.\\nWe also observed\\nan interesting case in the second example, which\\nrequires translating the doubly-negated phrase,\\n“not incompatible”.\\nThe attentional model cor-\\nrectly produces “nicht . . . unvereinbar”; whereas\\nthe non-attentional model generates “nicht verein-\\n16We concatenate the 508 sentence pairs with 1M sentence\\npairs from WMT and run the Berkeley aligner.', '\\n']\n",
      "['\\n## Page 9\\n', '### Text\\n', 'English-German translations\\nsrc\\nOrlando Bloom and Miranda Kerr still love each other\\nref\\nOrlando Bloom und Miranda Kerr lieben sich noch immer\\nbest\\nOrlando Bloom und Miranda Kerr lieben einander noch immer .\\nbase\\nOrlando Bloom und Lucas Miranda lieben einander noch immer .\\nsrc\\n′′ We ′ re pleased the FAA recognizes that an enjoyable passenger experience is not incompatible\\nwith safety and security , ′′ said Roger Dow , CEO of the U.S. Travel Association .\\nref\\n“ Wir freuen uns , dass die FAA erkennt , dass ein angenehmes Passagiererlebnis nicht im Wider-\\nspruch zur Sicherheit steht ” , sagte Roger Dow , CEO der U.S. Travel Association .\\nbest\\n′′ Wir freuen uns , dass die FAA anerkennt , dass ein angenehmes ist nicht mit Sicherheit und\\nSicherheit unvereinbar ist ′′ , sagte Roger Dow , CEO der US - die .\\nbase\\n′′ Wir freuen uns ¨uber die <unk> , dass ein <unk> <unk> mit Sicherheit nicht vereinbar ist mit\\nSicherheit und Sicherheit ′′ , sagte Roger Cameron , CEO der US - <unk> .\\nGerman-English translations\\nsrc\\nIn einem Interview sagte Bloom jedoch , dass er und Kerr sich noch immer lieben .\\nref\\nHowever , in an interview , Bloom has said that he and Kerr still love each other .\\nbest\\nIn an interview , however , Bloom said that he and Kerr still love .\\nbase\\nHowever , in an interview , Bloom said that he and Tina were still <unk> .\\nsrc\\nWegen der von Berlin und der Europ¨aischen Zentralbank verh¨angten strengen Sparpolitik in\\nVerbindung mit der Zwangsjacke , in die die jeweilige nationale Wirtschaft durch das Festhal-\\nten an der gemeinsamen W¨ahrung gen¨otigt wird , sind viele Menschen der Ansicht , das Projekt\\nEuropa sei zu weit gegangen\\nref\\nThe austerity imposed by Berlin and the European Central Bank , coupled with the straitjacket\\nimposed on national economies through adherence to the common currency , has led many people\\nto think Project Europe has gone too far .\\nbest\\nBecause of the strict austerity measures imposed by Berlin and the European Central Bank in\\nconnection with the straitjacket in which the respective national economy is forced to adhere to\\nthe common currency , many people believe that the European project has gone too far .\\nbase\\nBecause of the pressure imposed by the European Central Bank and the Federal Central Bank\\nwith the strict austerity imposed on the national economy in the face of the single currency ,\\nmany people believe that the European project has gone too far .\\nTable 5: Sample translations – for each example, we show the source (src), the human translation (ref),\\nthe translation from our best model (best), and the translation of a non-attentional model (base). We\\nitalicize some correct translation segments and highlight a few wrong ones in bold.\\nbar”, meaning “not compatible”.17 The attentional\\nmodel also demonstrates its superiority in translat-\\ning long sentences as in the last example.\\n6\\nConclusion\\nIn this paper, we propose two simple and effective\\nattentional mechanisms for neural machine trans-\\nlation: the global approach which always looks\\nat all source positions and the local one that only\\nattends to a subset of source positions at a time.\\nWe test the effectiveness of our models in the\\nWMT translation tasks between English and Ger-\\nman in both directions. Our local attention yields\\nlarge gains of up to 5.0 BLEU over non-attentional\\n17The reference uses a more fancy translation of “incom-\\npatible”, which is “im Widerspruch zu etwas stehen”. Both\\nmodels, however, failed to translate “passenger experience”.\\nmodels which already incorporate known tech-\\nniques such as dropout. For the English to Ger-\\nman translation direction, our ensemble model has\\nestablished new state-of-the-art results for both\\nWMT’14 and WMT’15, outperforming existing\\nbest systems, backed by NMT models and n-gram\\nLM rerankers, by more than 1.0 BLEU.\\nWe have compared various alignment functions\\nand shed light on which functions are best for\\nwhich attentional models. Our analysis shows that\\nattention-based NMT models are superior to non-\\nattentional ones in many cases, for example in\\ntranslating names and handling long sentences.\\nAcknowledgment\\nWe gratefully acknowledge support from a gift\\nfrom Bloomberg L.P. and the support of NVIDIA', '\\n']\n",
      "['\\n## Page 10\\n', '### Text\\n', 'Corporation with the donation of Tesla K40 GPUs.\\nWe thank Andrew Ng and his group as well as\\nthe Stanford Research Computing for letting us\\nuse their computing resources.\\nWe thank Rus-\\nsell Stewart for helpful discussions on the models.\\nLastly, we thank Quoc Le, Ilya Sutskever, Oriol\\nVinyals, Richard Socher, Michael Kayser, Jiwei\\nLi, Panupong Pasupat, Kelvin Guu, members of\\nthe Stanford NLP Group and the annonymous re-\\nviewers for their valuable comments and feedback.\\nReferences\\n[Bahdanau et al.2015] D.\\nBahdanau,\\nK.\\nCho,\\nand\\nY. Bengio.\\n2015.\\nNeural machine translation by\\njointly learning to align and translate. In ICLR.\\n[Buck et al.2014] Christian Buck, Kenneth Heaﬁeld,\\nand Bas van Ooyen. 2014. N-gram counts and lan-\\nguage models from the common crawl. In LREC.\\n[Cho et al.2014] Kyunghyun Cho, Bart van Merrien-\\nboer, Caglar Gulcehre, Fethi Bougares, Holger\\nSchwenk, and Yoshua Bengio.\\n2014.\\nLearning\\nphrase representations using RNN encoder-decoder\\nfor statistical machine translation. In EMNLP.\\n[Fraser and Marcu2007] Alexander Fraser and Daniel\\nMarcu. 2007. Measuring word alignment quality\\nfor statistical machine translation.\\nComputational\\nLinguistics, 33(3):293–303.\\n[Gregor et al.2015] Karol Gregor, Ivo Danihelka, Alex\\nGraves, Danilo Jimenez Rezende, and Daan Wier-\\nstra. 2015. DRAW: A recurrent neural network for\\nimage generation. In ICML.\\n[Jean et al.2015] S´ebastien\\nJean,\\nKyunghyun\\nCho,\\nRoland Memisevic, and Yoshua Bengio. 2015. On\\nusing very large target vocabulary for neural ma-\\nchine translation. In ACL.\\n[Kalchbrenner and Blunsom2013] N. Kalchbrenner and\\nP. Blunsom. 2013. Recurrent continuous translation\\nmodels. In EMNLP.\\n[Koehn et al.2003] Philipp Koehn, Franz Josef Och,\\nand Daniel Marcu. 2003. Statistical phrase-based\\ntranslation. In NAACL.\\n[Liang et al.2006] P. Liang, B. Taskar, and D. Klein.\\n2006. Alignment by agreement. In NAACL.\\n[Luong et al.2015] M.-T. Luong, I. Sutskever, Q. V. Le,\\nO. Vinyals, and W. Zaremba. 2015. Addressing the\\nrare word problem in neural machine translation. In\\nACL.\\n[Mnih et al.2014] Volodymyr Mnih,\\nNicolas Heess,\\nAlex Graves, and Koray Kavukcuoglu. 2014. Re-\\ncurrent models of visual attention. In NIPS.\\n[Papineni et al.2002] Kishore Papineni, Salim Roukos,\\nTodd Ward, and Wei jing Zhu.\\n2002.\\nBleu: a\\nmethod for automatic evaluation of machine trans-\\nlation. In ACL.\\n[Sutskever et al.2014] I. Sutskever, O. Vinyals, and\\nQ. V. Le. 2014. Sequence to sequence learning with\\nneural networks. In NIPS.\\n[Xu et al.2015] Kelvin Xu, Jimmy Ba, Ryan Kiros,\\nKyunghyun Cho,\\nAaron C. Courville,\\nRuslan\\nSalakhutdinov, Richard S. Zemel, and Yoshua Ben-\\ngio. 2015. Show, attend and tell: Neural image cap-\\ntion generation with visual attention. In ICML.\\n[Zaremba et al.2015] Wojciech\\nZaremba,\\nIlya\\nSutskever, and Oriol Vinyals.\\n2015.\\nRecurrent\\nneural network regularization. In ICLR.\\nA\\nAlignment Visualization\\nWe visualize the alignment weights produced by\\nour different attention models in Figure 7. The vi-\\nsualization of the local attention model is much\\nsharper than that of the global one. This contrast\\nmatches our expectation that local attention is de-\\nsigned to only focus on a subset of words each\\ntime. Also, since we translate from English to Ger-\\nman and reverse the source English sentence, the\\nwhite strides at the words “reality” and “.” in the\\nglobal attention model reveals an interesting ac-\\ncess pattern: it tends to refer back to the beginning\\nof the source sequence.\\nCompared to the alignment visualizations in\\n(Bahdanau et al., 2015), our alignment patterns\\nare not as sharp as theirs. Such difference could\\npossibly be due to the fact that translating from\\nEnglish to German is harder than translating into\\nFrench as done in (Bahdanau et al., 2015), which\\nis an interesting point to examine in future work.', '\\n']\n",
      "['\\n## Page 11\\n', '### Text\\n', 'They\\ndo\\nnot\\nunderstand\\nwhy\\nEurope\\nexists\\nin\\ntheory\\nbut\\nnot\\nin\\nreality\\n.\\nSie\\nverstehen\\nnicht\\n,\\nwarum\\nEuropa\\ntheoretisch\\nzwar\\nexistiert\\n,\\naber\\nnicht\\nin\\nWirklichkeit\\n.\\nThey\\ndo\\nnot\\nunderstand\\nwhy\\nEurope\\nexists\\nin\\ntheory\\nbut\\nnot\\nin\\nreality\\n.\\nSie\\nverstehen\\nnicht\\n,\\nwarum\\nEuropa\\ntheoretisch\\nzwar\\nexistiert\\n,\\naber\\nnicht\\nin\\nWirklichkeit\\n.\\nThey\\ndo\\nnot\\nunderstand\\nwhy\\nEurope\\nexists\\nin\\ntheory\\nbut\\nnot\\nin\\nreality\\n.\\nSie\\nverstehen\\nnicht\\n,\\nwarum\\nEuropa\\ntheoretisch\\nzwar\\nexistiert\\n,\\naber\\nnicht\\nin\\nWirklichkeit\\n.\\nThey\\ndo\\nnot\\nunderstand\\nwhy\\nEurope\\nexists\\nin\\ntheory\\nbut\\nnot\\nin\\nreality\\n.\\nSie\\nverstehen\\nnicht\\n,\\nwarum\\nEuropa\\ntheoretisch\\nzwar\\nexistiert\\n,\\naber\\nnicht\\nin\\nWirklichkeit\\n.\\nFigure 7: Alignment visualizations – shown are images of the attention weights learned by various\\nmodels: (top left) global, (top right) local-m, and (bottom left) local-p. The gold alignments are displayed\\nat the bottom right corner.', '\\n', '### Images\\n', '### Table\\n']\n"
     ]
    }
   ],
   "source": [
    "process_pdf(PDF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f7d695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
